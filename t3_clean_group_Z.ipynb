{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction  \n",
    "### The following code was tested on:  \n",
    "Python 3+ running on...  \n",
    "\n",
    "ULB Cluster - user epb123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note - the code runs slowly due to the number of .count() we use for summary purposes. To arrive at a clean dataset quicker, all .count() in string outputs can be commented out. I would also recommend leaving it running overnight.\n",
    "\n",
    "Additionally, our yellow data file is large - if you find yourself getting memory errors, it may be worth running FHV, FHVHV, and Green in one notebook, and Yellow separately in another. I have increased spark's memory allocation below, but on a personal computer it may throw an exception if the allocated memory is greater than the physical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "Now that the files in each sub-dataset conform to the same schema it becomes possible to check\n",
    "the files for errors. To this end, for each sub-dataset:  \n",
    "- First analyze what the valid values for each column are, based on the data dictionary available at the TLC website. (E.g., trip_distance should be a floating point value, and cannot be negative). Be sure to discuss your conclusions in your report.\n",
    "- Write code that can be used to check all the validity constraints for this sub-dataset.\n",
    "- For each file in the sub-dataset, detect the dirty records by running your validity constraints.\n",
    "- Inspect the dirty records. Are there records that can be repaired ? Repairing is possible, for example, when the record has data of the incorrect type in the column, but it is clear how to convert that data to the desired type.\n",
    "- Split the file into two: one containing the clean and repaired records, and one containing the dirty (and unrepairable) records. During the analysis of Section 2.4, we only use the clean and repaired records, the others are discarded.\n",
    "- In your report, include a discussion that summarizes, for each file, the kinds of errors found (with statistics), how you repaired those errors (if applicable), and how many records are discarded (because they are dirty and unrepairable).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import various packages to read and analyse our cleaned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various imports\n",
    "import os \n",
    "import glob\n",
    "import math\n",
    "import pickle\n",
    "import ntpath\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover directories and filepaths\n",
    "\n",
    "v_direc = pickle.load(open(\"v_direc\",'rb'))\n",
    "data_directory = pickle.load(open(v_direc + \"data_folder\",'rb'))\n",
    "save_directory = pickle.load(open(v_direc + \"save_directory\",'rb'))\n",
    "\n",
    "# This should not be set to true - see comment below\n",
    "cluster = False\n",
    "\n",
    "if cluster == True:\n",
    "    cluster = False\n",
    "    # Just in case - wouldn't want an error immediately, but also would like the future functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the read/write permissions issues in HDFS on the cluster, there is no way to run 2.3 in cluster mode without re-calculating the integrated datasets in this notebook. We therefore start spark in local mode, which will also work on the cluster (and is much faster than my laptop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will start Spark in local mode, using all available CPU cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are setting spark.driver.memory to 16g here - the Yellow datasets consume a lot of memory with our memory inefficient code.\n",
    "If you find that you are getting spark errors, you may have <16g memory on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.4\n"
     ]
    }
   ],
   "source": [
    "# # -------------------------------\n",
    "# # Start Spark in LOCAL mode\n",
    "# # -------------------------------\n",
    "\n",
    "if cluster == False:\n",
    "\n",
    "    #This is needed to start a Spark session from the notebook\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=16g  pyspark-shell\"\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # Stop any previously running spark session\n",
    "    try: \n",
    "        spark\n",
    "        print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "        spark.stop()\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "    # Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"H600Project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    #When dealing with RDDs, we work the sparkContext object. \n",
    "    #See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "    sc=spark.sparkContext\n",
    "\n",
    "    from pyspark.sql.types import *\n",
    "    from pyspark.sql.functions import col,udf,lit,struct\n",
    "    from pyspark import SQLContext\n",
    "\n",
    "    sql_sc = SQLContext(sc)\n",
    "\n",
    "    print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will start Spark on the ULB cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# Start Spark on the ULB cluster ,with YARN as resource manager\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "if cluster == True:\n",
    "\n",
    "    #This is needed to start a Spark session from the notebook\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=16g  pyspark-shell\"\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # We need to set the following environment variable, so that Spark knows where YARN runs\n",
    "    os.environ['HADOOP_CONF_DIR']=\"/etc/hadoop/conf\"\n",
    "\n",
    "    # Since we are accessing spark through it's python API, we need to make sure that all executor\n",
    "    # instances run the same version of python. \n",
    "    # (and we want Anaconda to be used, so we have access to numpy, pandas, and so forth)\n",
    "    # You will likely need to adjust this path if your run on a different cluster\n",
    "    os.environ['PYSPARK_PYTHON']=\"/usr/local/anaconda3/bin/python\"\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON']=\"/usr/local/anaconda3/bin/python\"\n",
    "\n",
    "    #The following lines are just there to allow this cell to be re-executed multiple times:\n",
    "    #if a spark session was already started, we stop it before starting a new one\n",
    "    #(there can be only one spark context per jupyter notebook)\n",
    "    try: \n",
    "        spark\n",
    "        print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "        spark.stop()\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\") \\\n",
    "        .config(\"spark.executor.instances\",\"4\") \\\n",
    "        .appName(\"H600Project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    #When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "    sc=spark.sparkContext\n",
    "\n",
    "    from pyspark.sql.types import *\n",
    "    from pyspark.sql.functions import col,udf,lit,struct\n",
    "    from pyspark.sql import SQLContext\n",
    "    sql_sc = SQLContext(sc)\n",
    "\n",
    "    print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '44289'),\n",
       " ('spark.app.id', 'local-1611630047402'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'H600Project'),\n",
       " ('spark.driver.host', 'publiclogin.hpda.ulb.ac.be'),\n",
       " ('spark.driver.memory', '16g'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that spark context is working, print its configuration\n",
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve stored variables from memory\n",
    "\n",
    "fhv_integratedPaths = pickle.load(open(v_direc + \"fhv_integratedPaths\",'rb'))\n",
    "fhv_folderPath = pickle.load(open(v_direc + \"fhv_folderPath\",'rb'))\n",
    "\n",
    "fhvhv_integratedPaths = pickle.load(open(v_direc + \"fhvhv_integratedPaths\",'rb'))\n",
    "fhvhv_folderPath = pickle.load(open(v_direc + \"fhvhv_folderPath\",'rb'))\n",
    "\n",
    "green_integratedPaths = pickle.load(open(v_direc + \"green_integratedPaths\",'rb'))\n",
    "green_folderPath = pickle.load(open(v_direc + \"green_folderPath\",'rb'))\n",
    "\n",
    "yellow_integratedPaths = pickle.load(open(v_direc + \"yellow_integratedPaths\",'rb'))\n",
    "yellow_folderPath = pickle.load(open(v_direc + \"yellow_folderPath\",'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General auxiliary functions\n",
    "Auxiliary code to help in the data cleaning process goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(address):\n",
    "# Takes csv from address given and creats a dataframe\n",
    "# Returns given csv as a dataframe\n",
    "# Schema of this csv will be in string format!!!!!\n",
    "    # Reading a csv from address while taking headers as a schema\n",
    "    # To infer schema type (i.e. int, float), add option inferSchema = True\n",
    "        \n",
    "    # When running in YARN mode, spark expects its input to come from HDFS by default\n",
    "    # Copying files to local Hadoop would prevent a 'Path does not exist' error on the cluster\n",
    "    # but this is not allowed\n",
    "    # We can therefore only run the below in local mode, but there is a workaround through pandas\n",
    "    \n",
    "    # When opening these through pandas, we have to add the \"error_bad_lines = False\" argument\n",
    "    # This is because PySpark seemed to drop rows with an extra column, but Pandas did not\n",
    "    \n",
    "    #Note - opening as Pandas appears to add a recordID in an unnamed first column\n",
    "    \n",
    "    # Code which does not work on cluster in HDFS mode:\n",
    "    # Possibly solved by substituting HDFS file location above\n",
    "    addressDF = spark.read.format('csv').option(\"header\", True).load(address)\n",
    "    \n",
    "    print(\"opened\", address)\n",
    "\n",
    "    # Because we had to save as Pandas in 2.2, there is a new column _c0 in some dataframes which will interfere with operations\n",
    "    # We need to detect, and drop, this column\n",
    "    \n",
    "    if '_c0' in addressDF.columns:\n",
    "        addressDF = addressDF.drop(col('_c0'))\n",
    "    \n",
    "    \n",
    "    # pandas_DF = pd.read_csv(address, warn_bad_lines = True, error_bad_lines = False)\n",
    "    # addressDF = sql_sc.createDataFrame(pandas_DF.astype(str))\n",
    "    \n",
    "    return addressDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(saveDF, saveName, savePath):\n",
    "# Function takes a dataframe, filename, and filepath and saves the dataframe\n",
    "# csv will be saved inside a folder with the given saveName, along with a text file _SUCCESS\n",
    "# Want to use this as it is quicker than converting to PandasDF\n",
    "\n",
    "# WARNING: WILL OVERWRITE PREVIOUS VERSIONS\n",
    "    \n",
    "    # I have not been given write permissions on the cluster and cannot directly save this through HDFS\n",
    "    # Another workaround is to just move via pandas again\n",
    "    # If working locally, the first line of code below can be uncommented and used instead\n",
    "    \n",
    "# NOTE: coalesce(1) can be used due to small data size - otherwise, must be written as multiple files due to memory restrictions \n",
    "    \n",
    "    # Code which does not work on HDFS:\n",
    "    saveDF.coalesce(1).write.options(header = True).mode(\"overwrite\").csv(os.path.join(savePath, saveName))\n",
    "    #saveDF.toPandas().to_csv(os.path.join(savePath, saveName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(newFolderName, newFolderPath):\n",
    "# Function takes a folder name and a path and creates a new folder/directory\n",
    "# Function returns folder path\n",
    "    \n",
    "    fullFolderPath = os.path.join(newFolderPath, newFolderName)\n",
    "    \n",
    "    # Folder might already have been created\n",
    "    try:\n",
    "        os.mkdir(fullFolderPath)\n",
    "        print(\"new folder created at \"+ fullFolderPath)\n",
    "    except FileExistsError:\n",
    "        print(\"folder already exists at \" + fullFolderPath)\n",
    "    \n",
    "    return fullFolderPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_fromSchema(schema):\n",
    "# Takes a schema and creates an empty dataframe with that schema in string format\n",
    "# Returns dataframe with schema and no values\n",
    "#     fields = []\n",
    "    \n",
    "#     for i in schema:\n",
    "#         fields.append(StructField(i, StringType(), True))\n",
    "    \n",
    "#     dfSchema = StructType(fields)\n",
    "#     emptyDF = spark.createDataFrame([],dfSchema)\n",
    "    emptyDF = spark.createDataFrame([],schema)\n",
    "    \n",
    "    return emptyDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_dupes(dupeData):\n",
    "# function accepts a spark dataframe with duplicates\n",
    "# function returns that dataframe with duplicates removed as [0]\n",
    "# function returns the number of duplicates as [1]\n",
    "# function returns the final length of the dataframe as [2]\n",
    "    # we want to drop duplicates and track how many we catch - the native PySpark dataframe method drop_duplicates works()\n",
    "    # it will drop based on all columns by default\n",
    "    \n",
    "    # NOTE: Pandas has added a _c0 to our dataframes upon saving - this disables functionality for drop_duplicates()\n",
    "    # We need to drop _c0 before calling this function\n",
    "    \n",
    "    initialLen = dupeData.count()\n",
    "    dupeData = dupeData.dropDuplicates()\n",
    "    finalLen = dupeData.count()\n",
    "    dupeCount = initialLen - finalLen\n",
    "    print(str(dupeCount) + ' duplicate entries removed')\n",
    "    \n",
    "    return dupeData, dupeCount, finalLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(folderpath):\n",
    "# Function which takes the folderpath for integrated data\n",
    "# Function moves all CSV's to one folder and renames them properly\n",
    "# Function returns the filepath of the new csvs\n",
    "# DO NOT CLEAN MORE THAN ONCE, or your pickled variables will not work\n",
    "\n",
    "    newPath = create_folder(\"all CSV\", folderpath)\n",
    "    testFilePath = []\n",
    "\n",
    "    for (root,direc,files) in os.walk(folderpath):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                if os.path.basename(root) != os.path.basename(newPath):\n",
    "                    os.rename(os.path.join(root,file),os.path.join(newPath,os.path.basename(root)))\n",
    "                    testFilePath.append(os.path.join(newPath,os.path.basename(root)))\n",
    "\n",
    "    testFilePath.sort()\n",
    "    return testFilePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions for cleanup\n",
    "Auxiliary code to help in the data cleaning process goes here\n",
    "Please read the comments for a detailed description of validation criteria. We are using UDFs here, as the validation criteria are fairly complex. As mentioned though, their performance hit is minimal compared to the .count() calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def datetime_validation(pudt,dodt):\n",
    "# Function to validate datetime values\n",
    "# Function returns 0 if datetimes together are valid, 1 if they are invalid, 2 if they do not exist\n",
    "# This function also checks that the timedelta is >=0\n",
    "# We appear to need both for all queries with distance, so validation flags will be set together\n",
    "    \n",
    "    if pudt == \"\" or pudt == None or pudt =='nan' or dodt == \"\" or dodt == None or dodt =='nan':\n",
    "        return 2\n",
    "    \n",
    "    # Convert to datetime to see if input follows the correct format\n",
    "    try:\n",
    "        pudt_DT = dt.strptime(pudt, '%Y-%m-%d %H:%M:%S')\n",
    "        dodt_DT = dt.strptime(dodt, '%Y-%m-%d %H:%M:%S')\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    timeDelta = (dodt_DT - pudt_DT).total_seconds()\n",
    "    # if the time difference is less than or equal to 0, we can return a 1\n",
    "    if timeDelta <= 0:\n",
    "        return 1\n",
    "    \n",
    "    # if timeDelta is above 0 and the formats are valid, we return a 0\n",
    "    return 0\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udfdatetime_validation = udf(datetime_validation, IntegerType())\n",
    "\n",
    "print(\"Date testing function stored as UDF\")\n",
    "\n",
    "def add_datetime_validation(dfDTval):\n",
    "# Adds the datetime validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfDTval = dfDTval.withColumn('pickup_datetime_val', udfdatetime_validation('pickup_datetime',\\\n",
    "                                                                                'dropoff_datetime'))\n",
    "    dfDTval = dfDTval.withColumn('dropoff_datetime_val', col('pickup_datetime_val'))\n",
    "\n",
    "    return dfDTval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LocationID testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def location_validation(locID):\n",
    "# Function to validate locationID values\n",
    "# Function returns 0 if locationID is valid, 1 if it is invalid, 2 if it is None\n",
    "# Function essentially checks that it is an integer and has a valid location code (above 0 and below 263)\n",
    "\n",
    "    if locID == \"\" or locID == None or locID == 'nan':\n",
    "        return 2\n",
    "\n",
    "    # want to catch non-integer errors\n",
    "    try:\n",
    "        locID = int(locID)\n",
    "    \n",
    "        # Checking location code\n",
    "        if locID <= 263 and locID >= 1:\n",
    "            return 0\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    return 1\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udflocation_validation = udf(location_validation, IntegerType())\n",
    "\n",
    "print(\"LocationID testing function stored as UDF\")\n",
    "\n",
    "def add_location_validation(dfLCval):\n",
    "# Adds the location validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfLCval = dfLCval.withColumn('pulocationid_val', udflocation_validation('pulocationid'))\n",
    "    dfLCval = dfLCval.withColumn('dolocationid_val', udflocation_validation('dolocationid'))\n",
    "\n",
    "    return dfLCval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sr_flag testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def srflag_validation(srflag):\n",
    "# Function to validate sr_flag values\n",
    "# Function returns 0 if sr_flag is valid, 1 if it is invalid, 2 if it is None\n",
    "# SR Flag has to be either 1 or Null, which makes this simple\n",
    "\n",
    "    # Checking sr_flag value - int(srflag) must be at the end or errors will appear\n",
    "    if srflag == None or srflag == \"\" or srflag == 'nan':\n",
    "        return 0\n",
    "    if int(srflag) == 1:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udfsr_validation = udf(srflag_validation, IntegerType())\n",
    "\n",
    "print(\"sr_flag testing function stored as UDF\")\n",
    "\n",
    "def add_sr_validation(dfSRval):\n",
    "# Adds the SR Flag validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfSRval = dfSRval.withColumn('sr_flag_val', udfsr_validation('sr_flag'))\n",
    "\n",
    "    return dfSRval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispatching_base_num testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def basenum_validation(basenum):\n",
    "# Function to validate dispatching_base_num values\n",
    "# Function returns 0 if dispatching_base_num is valid, 1 if it is invalid, 2 if it is None\n",
    "# Basenumbers seem to require format B00000 - the function checks for the length and the initial B\n",
    "    \n",
    "    if basenum == None or basenum == \"\" or basenum == 'nan':\n",
    "        return 2\n",
    "    \n",
    "    # Checking overall length of base_num\n",
    "    if len(basenum) == 6 and (basenum[0] == \"B\" or basenum[0] ==\"b\"):\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "# Define function as a UDF for pyspark usage\n",
    "udfbnum_validation = udf(basenum_validation, IntegerType())\n",
    "\n",
    "print(\"dispatching_base_num testing function stored as UDF\")\n",
    "\n",
    "def add_basenum_validation(dfBNval):\n",
    "# Adds the base number validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfBNval = dfBNval.withColumn('dispatching_base_num_val', udfbnum_validation('dispatching_base_num'))\n",
    "\n",
    "    return dfBNval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvfhs license number testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def hvfhs_license_validation(licensenum):\n",
    "# Function to validate hvfhs license number values\n",
    "# Function returns 0 if the hvfhs number is valid, 1 if it is invalid, 2 if it is None\n",
    "# Similarly to the basenumber, we can test for the four specific given values\n",
    "    \n",
    "    if licensenum == None or licensenum == \"\" or licensenum == 'nan':\n",
    "        return 2\n",
    "    \n",
    "    if len(licensenum) == 6 and (licensenum == \"HV0002\" or licensenum == \"HV0003\"\\\n",
    "                                 or licensenum == \"HV0004\" or licensenum == \"HV0005\"):\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "# Define function as a UDF for pyspark usage\n",
    "udflicensenum_validation = udf(hvfhs_license_validation, IntegerType())\n",
    "\n",
    "print(\"hvfhs license number testing function stored as UDF\")\n",
    "\n",
    "def add_license_validation(dfLNval):\n",
    "# Adds the hvfhs license number validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfLNval = dfLNval.withColumn('hvfhs_license_num_val', udflicensenum_validation('hvfhs_license_num'))\n",
    "\n",
    "    return dfLNval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FHV & FHVHV row testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def FHV_FHVHV_valid_row(pudt_val,dodt_val,puloc_val,doloc_val):\n",
    "# takes input columns\n",
    "# returns 1 if any of the columns contain 1\n",
    "# returns 2 if any of the columns contain 2\n",
    "# FHV and FHVHV have no amount columns - for the queries we are only interested in datetime and location\n",
    "\n",
    "    if pudt_val == 1 or dodt_val == 1 or puloc_val == 1 or doloc_val == 1:\n",
    "        return 1\n",
    "    if pudt_val == 2 or dodt_val == 2 or puloc_val == 2 or doloc_val == 2:\n",
    "        return 2\n",
    "\n",
    "    return 0\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udfFHV_FHVHV_valid_row = udf(FHV_FHVHV_valid_row, IntegerType())\n",
    "\n",
    "print(\"FHV & FHVHV row testing function stored as UDF\")\n",
    "\n",
    "def add_FHV_HV_row_validation(dfFHV_HVval):\n",
    "# Adds the row validation as a new dataframe column, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "\n",
    "    dfFHV_HVval = dfFHV_HVval.withColumn('row_val', udfFHV_FHVHV_valid_row('pickup_datetime_val',\\\n",
    "                                                                     'dropoff_datetime_val',\\\n",
    "                                                                     'pulocationid_val',\\\n",
    "                                                                     'dolocationid_val'))\n",
    "\n",
    "    return dfFHV_HVval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the missing and dirty values\n",
    "\n",
    "The obvious first step is identifying what information we actually need in our cleaned datasets. If a record is dirty in a dimension unnecessary for our analysis, there is no reason to drop the record.  \n",
    "\n",
    "Our approach is simple: we want to minimze the loss of information but be able to compute each query.\n",
    "Hence, at the begining of the cleaning of each sub-type of datasets (FHV,FHVHV,Green,Yellow), we state what\n",
    "disqualifies a record from being part of the last type of files (what is referenced as \"clean files\" in the assignment pdf).\n",
    "\n",
    "Here, our goal is to have usable datasets for our queries. Hence, if the data, after cleaning is such that no query can be computed, the record is discarded. Additionally, to maintain consistency, we will keep only data on which all queries can be run - we would not want to change the total number of trips across queries.\n",
    "\n",
    "Info requried for the queries:\n",
    "\n",
    "1. The number of trips: \n",
    "    - can be computed on the raw integrated files.\n",
    "    - No impact on this part.\n",
    "<br>\n",
    "2. The number of trips in Manhatan and Brooklyn:\n",
    "    - It is computable on each dataset (FHV, FHVHV, Green and Yellow).\n",
    "    - Drop off and pick up location IDs required.\n",
    "<br>\n",
    "3. Monthly total receipt (exclude tips) and \n",
    "    - Computable only on the Green and Yellow dataset.\n",
    "    - Requires All the monetary variables (or total and tip and possibly extra).\n",
    "<br>\n",
    "4. Average trip receipt (also exclude tips):\n",
    "    - Computable only on the Green and Yellow dataset.\n",
    "    - Requires All the monetary variables (or total and tip and possibly extra).\n",
    "<br>\n",
    "5. Average cost per in-progress minute:\n",
    "    - Computable only on the Green and Yellow dataset.\n",
    "    - Requires the tolls, surchages, fares, taxes and time (i.e. the difference between pick up and drop off datetime).\n",
    "<br>\n",
    "6. Average tip per trip:\n",
    "    - Computable only on the Green and Yellow dataset.\n",
    "    - Requires tip data.\n",
    "<br>\n",
    "7. Median monthly average speed per borough \n",
    "    - Computable only on the Green and Yellow dataset.\n",
    "    - Requires the pudatetime and dodatetime, the distance ,the drop off and pick up locaiton IDs and the location IDs of each borough.\n",
    "<br>\n",
    "8. How long does it take to go to the airport:\n",
    "    - Computable on each dataset(FHV,FHVHV,Green and Yellow).\n",
    "    - Requires the drop off and pick up dates times as well as pu/do location IDs.\n",
    "<br>\n",
    "\n",
    "This gives us the following list of critical variables:\n",
    "\n",
    "**FHV/FHVHV variables**\n",
    "- pickup_datetime\n",
    "- dropoff_datetime\n",
    "- pulocationid\n",
    "- dolocationid\n",
    "\n",
    "**Green/Yellow variables**\n",
    "- l/tpep_pickup_datetime\n",
    "- l/tpep_dropoff_datetime\n",
    "- trip_distance\n",
    "- pulocationid \n",
    "- dolocationid  \n",
    "- tip_amount\n",
    "- total_amount\n",
    "- payment_type\n",
    "\n",
    "note: these following two do not appear in the data dictionary, but are unnecessary regardless.\n",
    "- congestion_surcharge\n",
    "- ehail fee\n",
    "\n",
    "\n",
    "For the cost related queries, it is specified that Cost should exclude tips, but include fares, surcharges, taxes and tolls. The total_amount less the tip amount, as specified by the dictionary, will satisfy this condition, and so we will prioritise taking that variable in a complete form, and potentially repairing it with the others if it is missing. This has the added benefit that the total amount is already the most likely to be accurate, as it would need to be correct for customer billing purposes. For payment_type, we only need to filter out cash tips for one of our queries. There is, in this variable, also a line item for voided_trips and one for trips with no charge - we will remove all of these records before cleaning, as free trips should not be counted as a regular taxi journey. \n",
    "\n",
    "Accordingly, in the analysis below, we will detect all dirty variable entries according to validity constraints (as specified in the assignment), however dirty variables which are unnecessary will not cause a record to be dirty, as that record will of course still be usable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We design testing functions that will flag any default for the relevant values. The data is supposed to respect specific formats according to https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "If a format is not respected for query-relevant data, we try to provide a solution by repairing the default.\n",
    "If there is no solution to repair the record, we take steps depending on the overall picture.  \n",
    "\n",
    "The procedure we generally try to follow is to work through the validation criteria, but in reverse. We count the number of each type of error found, and where positive we apply corrective action. Due to the need for counts, we have artificially slowed our code significantly. Otherwise, we generally will use PySpark UDFs for validation criteria, but PySpark native functions for identification and repair.\n",
    "\n",
    "For each subset, we open each month's file and extract the broken data for repairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning the FHV dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values and validity constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "dispatching_base_num: TLC base license number - string, normally needs 6 characters, first character must be B or b (from observing data, not from data dictionary)  \n",
    "pickup_datetime: 19 character string (needs 19 characters), should follow format yyyy-mm-dd hh:mm:ss  \n",
    "dropoff_datetime: 19 character string (needs 19 characters), should follow format yyyy-mm-dd hh:mm:ss  \n",
    "pulocationid: int 1 <= x <= 263  \n",
    "dolocationid: int 1 <= x <= 263  \n",
    "sr_flag: int 1 or null  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and splitting off dirty records\n",
    "\n",
    "Here, as in other taxi types, after first removing duplicates, we run a number of PySpark UDFs on each file to classify the clean and dirty variables. We will then split each file into a clean set and a dirty set, where the dirty set is placed in another dataframe for further analysis.  \n",
    "\n",
    "The PySpark UDFs are unfortunate, performance wise, but due to the complexity of each validation requirement, they are more practical than native spark functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will class a record (not a value) as being dirty if it is unusable for our purposes in 2.4 - the remaining clean records may have dirty values in unused variables, but are unnecessary for our final database.\n",
    "\n",
    "If a key variable itself is dirty, we will attempt to use the remaining variables to repair it, but otherwise we will only track errors caught by our constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the queries:  \n",
    "1. The monthly total number of trips\n",
    "2. Monthly total number of trips in Manhattan and Brooklyn\n",
    "3. The monthly total receipts - exclude tips, but include fares, surcharges, taxes and tolls\n",
    "4. Average trip receipt\n",
    "5. Average cost per in-progress minute\n",
    "6. Average tip per trip\n",
    "7. Median monthly average trip speed per borough\n",
    "8. How long does it take to go to the airport?\n",
    "\n",
    "We are unable to calculate 3, 4, 5, 6, and 7 by nature - the FHV data has no fare records. However, we can obviously calculate query 1, with pickup and dropoff intact we can calculate query 2, and with both pickup and dropoff times and location values we can calculate query 8.  \n",
    "\n",
    "We could therefore consider records containing both pickup and dropoff locations or both locations and both times to still be useful. Records containing only one of them are not useful and as we have no further data, these records cannot be repaired. If row_val == 0, all four of these variables are included (for query 8) and we do not have to worry. We therefore can just also filter for records containing a pair of location values (query 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/output/cleaned FHV\n",
      "folder already exists at /home/epb123/output/dirty FHV\n"
     ]
    }
   ],
   "source": [
    "clean_FHV_directory = create_folder(\"cleaned FHV\", save_directory)\n",
    "with open(v_direc + \"clean_FHV_directory\",'wb') as cleanedfhvdirec:\n",
    "    pickle.dump(clean_FHV_directory,cleanedfhvdirec)\n",
    "    \n",
    "dirty_FHV_directory = create_folder(\"dirty FHV\", save_directory)\n",
    "with open(v_direc + \"dirty_FHV_directory\",'wb') as dirtyfhvdirec:\n",
    "    pickle.dump(dirty_FHV_directory,dirtyfhvdirec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FHV_cleanup():\n",
    "# takes no arguments - splits FHV into clean and dirty data\n",
    "# returns a pyspark dataframe with dirty data and the clean filepaths\n",
    "\n",
    "    dropped_duplicates_count = 0\n",
    "    clean_count = 0\n",
    "    row_count = 0\n",
    "        \n",
    "    for i in fhv_integratedPaths:\n",
    "        dfi = create_df(i)\n",
    "        dfi_row_count = dfi.count()\n",
    "        row_count = row_count + dfi_row_count\n",
    "\n",
    "        # Adding validation columns\n",
    "        dfi = add_basenum_validation(dfi)\n",
    "        dfi = add_datetime_validation(dfi)\n",
    "        dfi = add_location_validation(dfi)\n",
    "        dfi = add_sr_validation(dfi)\n",
    "        dfi = add_FHV_HV_row_validation(dfi)\n",
    "\n",
    "        # Store records which are not dirty (this does not consider variables useless for 2.4 completion)\n",
    "        # We also want to keep records which we can use - namely those with a pair of locationID values\n",
    "        dfiClean = dfi.filter((dfi.row_val == 0))\n",
    "        \n",
    "        dfiClean = dfiClean.drop(*['dispatching_base_num_val','pickup_datetime_val','dropoff_datetime_val','pulocationid_val', \\\n",
    "                              'dolocationid_val','sr_flag_val','row_val'])\n",
    "\n",
    "        # We can drop duplicates in the clean dataset here - they will all have pickup and dropoff times or locations\n",
    "        # The times are stored to the millisecond and PySpark's drop_duplicates compares all columns\n",
    "        # If two records have pickup and dropoff times identical to the millisecond, we can safely drop them\n",
    "        # If two records have both location ids identical and identical dispatch numbers, we can also safely drop them\n",
    "        duplicates_result = drop_dupes(dfiClean)\n",
    "        dfiClean = duplicates_result[0]\n",
    "\n",
    "        dropped_duplicates_count = dropped_duplicates_count + duplicates_result[1]\n",
    "\n",
    "        dfiClean_row_count = duplicates_result[2]\n",
    "        clean_count = clean_count + dfiClean_row_count\n",
    "\n",
    "        # Save the cleaned dataframes - pyspark will not save an empty dataframe correctly - it will have no schema!\n",
    "        if dfiClean_row_count > 0:\n",
    "            save_df(dfiClean,ntpath.basename(i) + 'clean', clean_FHV_directory)\n",
    "        else:\n",
    "            print(\"no remaining records - CSV not saved\")\n",
    "\n",
    "        dirtyInI = dfi.filter((dfi.row_val == 1) | (dfi.row_val == 2))\\\n",
    "            .withColumn(\"filename\", lit(ntpath.basename(i)))\n",
    "        \n",
    "        # We then add the dirty records to a spark dataframe, with an additional column specifying file of origin\n",
    "        if i == fhv_integratedPaths[0]:\n",
    "            dfiDirty = dirtyInI\n",
    "\n",
    "        else:\n",
    "            dfiDirty = dfiDirty.unionByName(dirtyInI)\n",
    "        \n",
    "        save_df(dirtyInI,ntpath.basename(i) + 'dirty', dirty_FHV_directory)\n",
    "\n",
    "        print(str(dfiClean_row_count) + ' clean records preserved out of ' + str(dfi_row_count) + \\\n",
    "              ' original records - ' + str(dfiClean_row_count/dfi_row_count*100)[0:4] + \"%\")\n",
    "        print(str(dirtyInI.filter(dirtyInI.row_val == 1).count()) + \" invalid records have been found\")\n",
    "        print(str(dirtyInI.filter(dirtyInI.row_val == 2).count()) + \" records with missing data have been found\")\n",
    "        \n",
    "        # We don't need them, but we can check for dirty sr_flag and basenumber values\n",
    "        dfi = dfi.select([dfi.sr_flag_val, dfi.dispatching_base_num_val])\n",
    "        \n",
    "        print(\"For the variables useless for our 2.4 analysis, we have (among values not missing):\")\n",
    "        dfi.filter((dfi.sr_flag_val == 1)).\\\n",
    "                   agg(sql.sum(\"sr_flag_val\")).show()\n",
    "        dfi.filter((dfi.dispatching_base_num_val == 1)).\\\n",
    "                   agg(sql.sum(\"dispatching_base_num_val\")).show()\n",
    "        \n",
    "                   \n",
    "    print(\"Out of \" + str(row_count) + \" original records, \" + str(clean_count) + \" records were clean.\")\n",
    "    print(str(dropped_duplicates_count) + \" records were dropped as duplicates.\")\n",
    "    print(str(row_count - clean_count - dropped_duplicates_count) \\\n",
    "          + \" dirty records remain for resolution.\")\n",
    "    \n",
    "    return dfiDirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2015-01.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 5462 original records - 0.0%\n",
      "182 invalid records have been found\n",
      "5280 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           10|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2015-02.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 6220 original records - 0.0%\n",
      "190 invalid records have been found\n",
      "6030 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           10|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2015-03.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 6529 original records - 0.0%\n",
      "152 invalid records have been found\n",
      "6377 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2015-04.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 7807 original records - 0.0%\n",
      "152 invalid records have been found\n",
      "7655 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2015-05.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 8560 original records - 0.0%\n",
      "193 invalid records have been found\n",
      "8367 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2015-06.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 8476 original records - 0.0%\n",
      "87 invalid records have been found\n",
      "8389 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2015-07.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 8762 original records - 0.0%\n",
      "74 invalid records have been found\n",
      "8688 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2015-08.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 10526 original records - 0.0%\n",
      "110 invalid records have been found\n",
      "10416 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2015-09.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 13269 original records - 0.0%\n",
      "131 invalid records have been found\n",
      "13138 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                            1|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2015-10.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 17295 original records - 0.0%\n",
      "373 invalid records have been found\n",
      "16922 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                            1|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2015-11.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 16780 original records - 0.0%\n",
      "378 invalid records have been found\n",
      "16402 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                            9|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2015-12.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 17601 original records - 0.0%\n",
      "322 invalid records have been found\n",
      "17279 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                            7|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2016-01.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 17465 original records - 0.0%\n",
      "190 invalid records have been found\n",
      "17275 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           10|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2016-02.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 18947 original records - 0.0%\n",
      "221 invalid records have been found\n",
      "18726 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                            6|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2016-03.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 19402 original records - 0.0%\n",
      "191 invalid records have been found\n",
      "19211 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                            5|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2016-04.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 20445 original records - 0.0%\n",
      "272 invalid records have been found\n",
      "20173 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           12|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2016-05.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 21675 original records - 0.0%\n",
      "176 invalid records have been found\n",
      "21499 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                            7|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2016-06.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 21709 original records - 0.0%\n",
      "66 invalid records have been found\n",
      "21643 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                            6|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2016-07.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 22204 original records - 0.0%\n",
      "68 invalid records have been found\n",
      "22136 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                            8|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2016-08.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 22577 original records - 0.0%\n",
      "49 invalid records have been found\n",
      "22528 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           14|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2016-09.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 23546 original records - 0.0%\n",
      "56 invalid records have been found\n",
      "23490 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                            1|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2016-10.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 25399 original records - 0.0%\n",
      "63 invalid records have been found\n",
      "25336 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           26|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2016-11.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 24959 original records - 0.0%\n",
      "50 invalid records have been found\n",
      "24909 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                            5|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2016-12.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 27785 original records - 0.0%\n",
      "55 invalid records have been found\n",
      "27730 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                            4|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2017-01.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 27290 original records - 0.0%\n",
      "278 invalid records have been found\n",
      "27012 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                            8|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2017-02.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 26547 original records - 0.0%\n",
      "219 invalid records have been found\n",
      "26328 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           12|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2017-03.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 31372 original records - 0.0%\n",
      "314 invalid records have been found\n",
      "31058 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           26|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2017-04.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 29508 original records - 0.0%\n",
      "243 invalid records have been found\n",
      "29265 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           10|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2017-05.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 30766 original records - 0.0%\n",
      "552 invalid records have been found\n",
      "30214 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                            5|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2017-06.csv\n",
      "1 duplicate entries removed\n",
      "23052 clean records preserved out of 30672 original records - 75.1%\n",
      "1025 invalid records have been found\n",
      "6594 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           53|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2017-07.csv\n",
      "0 duplicate entries removed\n",
      "23995 clean records preserved out of 31320 original records - 76.6%\n",
      "1063 invalid records have been found\n",
      "6262 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           78|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2017-08.csv\n",
      "0 duplicate entries removed\n",
      "24477 clean records preserved out of 31782 original records - 77.0%\n",
      "3150 invalid records have been found\n",
      "4155 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           64|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2017-09.csv\n",
      "0 duplicate entries removed\n",
      "25525 clean records preserved out of 33210 original records - 76.8%\n",
      "4106 invalid records have been found\n",
      "3579 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           25|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2017-10.csv\n",
      "0 duplicate entries removed\n",
      "27628 clean records preserved out of 35749 original records - 77.2%\n",
      "4508 invalid records have been found\n",
      "3613 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           55|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2017-11.csv\n",
      "0 duplicate entries removed\n",
      "28238 clean records preserved out of 36069 original records - 78.2%\n",
      "4139 invalid records have been found\n",
      "3692 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           94|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2017-12.csv\n",
      "0 duplicate entries removed\n",
      "31202 clean records preserved out of 39900 original records - 78.2%\n",
      "4864 invalid records have been found\n",
      "3834 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                          131|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2018-01.csv\n",
      "0 duplicate entries removed\n",
      "31226 clean records preserved out of 39565 original records - 78.9%\n",
      "4354 invalid records have been found\n",
      "3985 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                          102|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2018-02.csv\n",
      "0 duplicate entries removed\n",
      "30725 clean records preserved out of 38647 original records - 79.5%\n",
      "4161 invalid records have been found\n",
      "3761 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           90|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2018-03.csv\n",
      "0 duplicate entries removed\n",
      "35555 clean records preserved out of 43945 original records - 80.9%\n",
      "4530 invalid records have been found\n",
      "3860 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                           70|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2018-04.csv\n",
      "0 duplicate entries removed\n",
      "34520 clean records preserved out of 42050 original records - 82.0%\n",
      "4452 invalid records have been found\n",
      "3078 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                          165|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2018-05.csv\n",
      "0 duplicate entries removed\n",
      "35398 clean records preserved out of 43087 original records - 82.1%\n",
      "4244 invalid records have been found\n",
      "3445 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                          180|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2018-06.csv\n",
      "0 duplicate entries removed\n",
      "34794 clean records preserved out of 42225 original records - 82.4%\n",
      "4324 invalid records have been found\n",
      "3107 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                          164|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2018-07.csv\n",
      "0 duplicate entries removed\n",
      "36446 clean records preserved out of 43158 original records - 84.4%\n",
      "3883 invalid records have been found\n",
      "2829 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                          168|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2018-08.csv\n",
      "0 duplicate entries removed\n",
      "36878 clean records preserved out of 44194 original records - 83.4%\n",
      "4249 invalid records have been found\n",
      "3067 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                          169|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2018-09.csv\n",
      "0 duplicate entries removed\n",
      "36960 clean records preserved out of 44261 original records - 83.5%\n",
      "4273 invalid records have been found\n",
      "3028 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                          110|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2018-10.csv\n",
      "0 duplicate entries removed\n",
      "39112 clean records preserved out of 46539 original records - 84.0%\n",
      "4266 invalid records have been found\n",
      "3161 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                          154|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2018-11.csv\n",
      "0 duplicate entries removed\n",
      "38630 clean records preserved out of 45705 original records - 84.5%\n",
      "4111 invalid records have been found\n",
      "2964 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                          128|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2018-12.csv\n",
      "0 duplicate entries removed\n",
      "40458 clean records preserved out of 47672 original records - 84.8%\n",
      "4158 invalid records have been found\n",
      "3056 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                          159|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2019-01.csv\n",
      "0 duplicate entries removed\n",
      "39618 clean records preserved out of 46215 original records - 85.7%\n",
      "5251 invalid records have been found\n",
      "1346 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            5993|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                          159|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2019-02.csv\n",
      "0 duplicate entries removed\n",
      "345 clean records preserved out of 3386 original records - 10.1%\n",
      "3041 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2019-03.csv\n",
      "0 duplicate entries removed\n",
      "203 clean records preserved out of 2638 original records - 7.69%\n",
      "2435 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2019-04.csv\n",
      "0 duplicate entries removed\n",
      "509 clean records preserved out of 3788 original records - 13.4%\n",
      "3279 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2019-05.csv\n",
      "0 duplicate entries removed\n",
      "474 clean records preserved out of 4053 original records - 11.6%\n",
      "3579 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2019-06.csv\n",
      "0 duplicate entries removed\n",
      "501 clean records preserved out of 3904 original records - 12.8%\n",
      "3403 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2019-07.csv\n",
      "0 duplicate entries removed\n",
      "530 clean records preserved out of 3793 original records - 13.9%\n",
      "3208 invalid records have been found\n",
      "55 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2019-08.csv\n",
      "0 duplicate entries removed\n",
      "617 clean records preserved out of 3639 original records - 16.9%\n",
      "2933 invalid records have been found\n",
      "89 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2019-09.csv\n",
      "0 duplicate entries removed\n",
      "544 clean records preserved out of 2075 original records - 26.2%\n",
      "1529 invalid records have been found\n",
      "2 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2019-10.csv\n",
      "0 duplicate entries removed\n",
      "649 clean records preserved out of 3604 original records - 18.0%\n",
      "2955 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2019-11.csv\n",
      "0 duplicate entries removed\n",
      "512 clean records preserved out of 3252 original records - 15.7%\n",
      "2740 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2020-01.csv\n",
      "0 duplicate entries removed\n",
      "697 clean records preserved out of 3917 original records - 17.7%\n",
      "3200 invalid records have been found\n",
      "20 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2020-03.csv\n",
      "0 duplicate entries removed\n",
      "455 clean records preserved out of 2502 original records - 18.1%\n",
      "1971 invalid records have been found\n",
      "76 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2020-04.csv\n",
      "0 duplicate entries removed\n",
      "116 clean records preserved out of 959 original records - 12.0%\n",
      "607 invalid records have been found\n",
      "236 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2020-05.csv\n",
      "0 duplicate entries removed\n",
      "1 clean records preserved out of 1411 original records - 0.07%\n",
      "38 invalid records have been found\n",
      "1372 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHV/fhv_tripdata_2020-06.csv\n",
      "0 duplicate entries removed\n",
      "no remaining records - CSV not saved\n",
      "0 clean records preserved out of 1839 original records - 0.0%\n",
      "0 invalid records have been found\n",
      "1839 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "Out of 1389608 original records, 660590 records were clean.\n",
      "1 records were dropped as duplicates.\n",
      "729017 dirty records remain for resolution.\n"
     ]
    }
   ],
   "source": [
    "dirtyFHV = FHV_cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up keeping 660,590 (with 1 duplicate) records out of the original 1,389,608. We do end up dropping many months worth of data (everything up until 2017-06!) as a part of this, but given that the queries will rely on dodatetime or dolocationid, we do not have much of a choice. Imputing may be possible, but the accuracy would be so little as to be negligible. I would prefer to compute the first query of 2.4 on the full dataset, and then use this clean version for the rest - the pure number of trips should be clean (given the 2 duplicates picked up), but location based queries will require location data.  \n",
    "\n",
    "This is unfortunate when comparing location and time based queries across months, but with no way of repairing due to the limited data in the FHV records, we must live with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, it looks like there are a small number of invalid dispatching numbers in each file, and almost 6000 broken sr_flag values in one file. As we are far more interested in the other variables (in fact, we do not care about these at all), we will not consider invalid values in dispatching_base_num and sr_flag as something worth classifying a record as dirty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 119436 damaged records\n",
      "There were 609581 missing records\n"
     ]
    }
   ],
   "source": [
    "# Let's look at our dirty record data\n",
    "FHVdamaged_rows = dirtyFHV.filter(dirtyFHV.row_val == 1).count()\n",
    "print(\"There were \"+str(FHVdamaged_rows)+\" damaged records\")\n",
    "      \n",
    "FHVmissing_rows = dirtyFHV.filter(dirtyFHV.row_val == 2).count()\n",
    "print(\"There were \"+str(FHVmissing_rows)+\" missing records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without any other data - we cannot reasonably repair missing values. We proceed immediately to repairable records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dispatching_base_num_val\n",
      "Checking pickup_datetime_val\n",
      "Checking dropoff_datetime_val\n",
      "Checking pulocationid_val\n",
      "Checking dolocationid_val\n",
      "Checking sr_flag_val\n",
      "Checking row_val\n",
      "{'dispatching_base_num_val': 199, 'pickup_datetime_val': 11, 'dropoff_datetime_val': 11, 'pulocationid_val': 42154, 'dolocationid_val': 104905, 'sr_flag_val': 12, 'row_val': 119436}\n"
     ]
    }
   ],
   "source": [
    "# we can check filter by row_val = 1 first --> by construction\n",
    "# any row with a 1 anywhere will have taken row_val = 1\n",
    "# these rows contain invalid, but not missing variable entries\n",
    "\n",
    "def FHV_dirty_count(FHV1):\n",
    "# Count corrupted records by variable type\n",
    "    dirty_records = {}\n",
    "\n",
    "    for i in range (6,len(FHV1.columns)-1):\n",
    "        print('Checking ' + str(FHV1.columns[i]))\n",
    "        tick = FHV1.filter(col(FHV1.columns[i]) == 1).count()\n",
    "              \n",
    "        if tick != 0:\n",
    "            dirty_records[FHV1.columns[i]] = tick\n",
    "\n",
    "    return dirty_records\n",
    "\n",
    "# Let's start by seeing where the majority of errors lie (if there are any)\n",
    "if FHVdamaged_rows != 0:\n",
    "    print(FHV_dirty_count(dirtyFHV.filter(dirtyFHV.row_val == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the dirty dataframe separately to avoid unfortunate errors\n",
    "# We can update our cleaned values as we go\n",
    "\n",
    "recleanedFHV = dirtyFHV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with analysing the **dolocationid** and **pulocationid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606462 missing dropoff values\n",
      "282286 missing pickup values\n"
     ]
    }
   ],
   "source": [
    "# Let's count our missing records first\n",
    "print(str(dirtyFHV.filter(dirtyFHV.dolocationid_val == 2).count())+\" missing dropoff values\")\n",
    "print(str(dirtyFHV.filter(dirtyFHV.pulocationid_val == 2).count())+\" missing pickup values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then start with **pulocationid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 0 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "# Let us now check to see if the values are incorrect strings\n",
    "# We know the locationids here are not null, thus we can do...\n",
    "\n",
    "dirtyFHVanalysis = dirtyFHV.filter(col('pulocationid_val') == 1)\n",
    "\n",
    "#CASTING AS INT HERE!!!!!!\n",
    "print(\"we have \" + str(dirtyFHVanalysis.select(\"pulocationid\",sql.col(\"pulocationid\").cast(\"int\").isNotNull().alias(\"strings\")).\\\n",
    "        filter(col('strings') == 'false').count()) + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These would result from a \"failed to match\" failure coming out of the 2.2 locationid calculations. Basically, if the coordinates were corrupt - i.e. something was stored at 5 deg. longitude and 36 deg latitude, the shapefile would not identify the location as being in New York. We have none of these errors, so we should move on.  \n",
    "\n",
    "We will have run the locationid testing code in 2.2, and the resulting errors must come from values 264, 265, and above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 42036 values above our threshold\n",
      "We have 118 zero values\n",
      "We have 0 values below our threshold\n"
     ]
    }
   ],
   "source": [
    "print(\"We have \"+str(recleanedFHV.filter(col('pulocationid') > 263).count())+\" values above our threshold\")\n",
    "print(\"We have \"+str(recleanedFHV.filter(col('pulocationid') == 0).count())+\" zero values\")\n",
    "print(\"We have \"+str(recleanedFHV.filter(col('pulocationid') < 0).count())+\" values below our threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot repair these - all results coming from our location conversion would have fallen within our boundaries. All remaining results must be from later schemas without conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "recleanedFHV = recleanedFHV.withColumn('pulocationid', sql.when(col('pulocationid') > 263,\\\n",
    "                                            None).otherwise(col('pulocationid')))\n",
    "recleanedFHV = recleanedFHV.withColumn('pulocationid', sql.when(col('pulocationid') == 0,\\\n",
    "                                            None).otherwise(col('pulocationid')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 excess values remain after cleaning\n",
      "0 zero values remain after cleaning\n"
     ]
    }
   ],
   "source": [
    "print(str(recleanedFHV.filter(col('pulocationid') > 263).count())+\" excess values remain after cleaning\")\n",
    "print(str(recleanedFHV.filter(col('pulocationid') == 0).count())+\" zero values remain after cleaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to **dolocationid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 0 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "# Let us now check to see if the values are incorrect strings\n",
    "# We know the locationids here are not null, thus we can do...\n",
    "\n",
    "dirtyFHVanalysis = dirtyFHV.filter(col('dolocationid_val') == 1)\n",
    "\n",
    "#CASTING AS INT HERE!!!!!!\n",
    "print(\"we have \" + str(dirtyFHVanalysis.filter(col(\"dolocationid\").cast(\"int\").isNull()).count()) + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 103539 values above our threshold\n",
      "We have 1366 zero values\n",
      "We have 0 values below our threshold\n"
     ]
    }
   ],
   "source": [
    "print(\"We have \"+str(recleanedFHV.filter(col('dolocationid') > 263).count())+\" values above our threshold\")\n",
    "print(\"We have \"+str(recleanedFHV.filter(col('dolocationid') == 0).count())+\" zero values\")\n",
    "print(\"We have \"+str(recleanedFHV.filter(col('dolocationid') < 0).count())+\" values below our threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "recleanedFHV = recleanedFHV.withColumn('dolocationid', sql.when(col('dolocationid') > 263,\\\n",
    "                                            None).otherwise(col('dolocationid')))\n",
    "recleanedFHV = recleanedFHV.withColumn('dolocationid', sql.when(col('dolocationid') == 0,\\\n",
    "                                            None).otherwise(col('dolocationid')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 excess values remain after cleaning\n",
      "0 zero values remain after cleaning\n"
     ]
    }
   ],
   "source": [
    "print(str(recleanedFHV.filter(col('dolocationid') > 263).count())+\" excess values remain after cleaning\")\n",
    "print(str(recleanedFHV.filter(col('dolocationid') == 0).count())+\" zero values remain after cleaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot do anything about the missing values with the data at hand, so let's move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not forget to examine the incorrect **pickup/dropoff_datetime** columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 11 dirty dropoff values and...\n",
      "538883 missing dropoff values\n",
      "We have 11 dirty pickup values and...\n",
      "538883 missing pickup values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where our dropped records came from\n",
    "print(\"We have \"+str(dirtyFHV.filter(dirtyFHV.dropoff_datetime_val == 1).count())+\" dirty dropoff values and...\")\n",
    "print(str(dirtyFHV.filter(dirtyFHV.dropoff_datetime_val == 2).count())+\" missing dropoff values\")\n",
    "print(\"We have \"+str(dirtyFHV.filter(dirtyFHV.pickup_datetime_val == 1).count())+\" dirty pickup values and...\")\n",
    "print(str(dirtyFHV.filter(dirtyFHV.pickup_datetime_val == 2).count())+\" missing pickup values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the dirty values must come from negative trip distances or invalid formats. Let us test for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FHV/FHVHV date testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "# We will here make the UDF such that it is applicable to FHV and FHVHV taxi types:\n",
    "# If we have moved past our exclusion criteria, we want to see if we should swap the columns to repair the records\n",
    "\n",
    "def datetime_switch_flag(pudt,dodt):\n",
    "# We have just set invalid timestamps to Null, so\n",
    "    if pudt == \"\" or pudt == None or pudt =='nan' or dodt == \"\" or dodt == None or dodt =='nan':\n",
    "    # If Null, we don't want to switch them anyway\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        pudt_DT = dt.strptime(pudt, '%Y-%m-%d %H:%M:%S')\n",
    "        dodt_DT = dt.strptime(dodt, '%Y-%m-%d %H:%M:%S')\n",
    "    except:\n",
    "        return 2\n",
    "    # if the datetime records are in an invalid format, we can set null with a 2 return.\n",
    "    \n",
    "    timeDelta = (dodt_DT - pudt_DT).total_seconds()\n",
    "    if timeDelta < 0:\n",
    "        return 1\n",
    "    # if the datetime records are 0, to the millisecond, we want to remove them as well!\n",
    "    elif timeDelta == 0:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udfdatetime_switch = udf(datetime_switch_flag, IntegerType())\n",
    "\n",
    "print(\"FHV/FHVHV date testing function stored as UDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will now drop 3 invalid values\n",
      "We will now repair 8 backwards values\n"
     ]
    }
   ],
   "source": [
    "# Make the switch Flag dummy\n",
    "recleanedFHV = recleanedFHV.withColumn('tsFlag',udfdatetime_switch('pickup_datetime','dropoff_datetime'))\n",
    "\n",
    "print(\"We will now drop \"+str(recleanedFHV.filter(col('tsFlag') == 2).count())+\" invalid values\")\n",
    "\n",
    "recleanedFHV = recleanedFHV.withColumn('dropoff_datetime',sql.when(col('tsFlag') == 2, None).\\\n",
    "                                           otherwise(col('dropoff_datetime')))\n",
    "recleanedFHV = recleanedFHV.withColumn('pickup_datetime',sql.when(col('tsFlag') == 2, None).\\\n",
    "                                           otherwise(col('pickup_datetime')))\n",
    "\n",
    "print(\"We will now repair \"+str(recleanedFHV.filter(col('tsFlag') == 1).count())+\" backwards values\")\n",
    "\n",
    "# Put dropoff values into a temporary column\n",
    "recleanedFHV = recleanedFHV.withColumn('dropoff_temp',col('dropoff_datetime'))\n",
    "\n",
    "# Move pickup values into dropoff when switch flag is 1\n",
    "recleanedFHV = recleanedFHV.withColumn('dropoff_datetime',sql.when(col('tsFlag') == 1,col('pickup_datetime')).\\\n",
    "                                           otherwise(col('dropoff_datetime')))\n",
    "# Move dropoff values from temporary column into pickup when switch flag is 1\n",
    "recleanedFHV = recleanedFHV.withColumn('pickup_datetime',sql.when(col('tsFlag') == 1,col('dropoff_temp')).\\\n",
    "                                           otherwise(col('pickup_datetime')))\n",
    "\n",
    "# Drop temporary columns\n",
    "recleanedFHV = recleanedFHV.drop(*['tsFlag','dropoff_temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 values which we could possibly use to compute query 2 for FHV records.\n"
     ]
    }
   ],
   "source": [
    "FHVpotential = dirtyFHV.filter((col('row_val') == 2) & (col('pulocationid_val') == 0) & (col('dolocationid_val') == 0)).count()\n",
    "print(\"There are \"+str(FHVpotential)+\" values which we could possibly use to compute query 2 for FHV records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from our cleaning, we have investigated re-adding entries with ID values, but no times, as these could be used in Query 2 of t4.\n",
    "\n",
    "Given the insignificance which re-adding these 8 rows would bring compared to the overall fully clean dataset (~700k) records, and the minor inconsistencies in the monthly number of fares resulting from their re-addition, we choose to not re-integrate this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/output/cleaned FHV/all CSV\n",
      "folder already exists at /home/epb123/output/dirty FHV/all CSV\n"
     ]
    }
   ],
   "source": [
    "# Run if saving as PySpark\n",
    "FHV_cleanCSV_folderPath = cleanup(clean_FHV_directory)\n",
    "with open(v_direc + \"FHV_cleanCSV_folderPath\",'wb') as cleanedfhvCSVdirec:\n",
    "    pickle.dump(FHV_cleanCSV_folderPath,cleanedfhvCSVdirec)\n",
    "\n",
    "FHV_dirtyCSV_folderPath = cleanup(dirty_FHV_directory)\n",
    "with open(v_direc + \"FHV_dirtyCSV_folderPath\",'wb') as dirtyfhvCSVdirec:\n",
    "    pickle.dump(FHV_dirtyCSV_folderPath,dirtyfhvCSVdirec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have managed to repair 1 complete records\n"
     ]
    }
   ],
   "source": [
    "# We've set irreparable values as Null, so...\n",
    "print(\"We have managed to repair \"+str(recleanedFHV.filter(\\\n",
    "                                                             (col('pickup_datetime').isNotNull()) &\\\n",
    "                                                             (col('dropoff_datetime').isNotNull()) &\\\n",
    "                                                             (col('dolocationid').isNotNull()) &\\\n",
    "                                                             (col('pulocationid').isNotNull())).count())+\" complete records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as if the massive number of unrepairable location errors means we cannot include even those repairable datetime values. We will add our repaired record to the relevant file, but we must discard 729,017 others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us filter for the values we want\n",
    "everdirtyFHV = recleanedFHV.filter(\\\n",
    "                                     (col('pickup_datetime').isNull()) |\\\n",
    "                                     (col('dropoff_datetime').isNull()) |\\\n",
    "                                     (col('dolocationid').isNull()) |\\\n",
    "                                     (col('pulocationid').isNull())\\\n",
    "                                     )\n",
    "\n",
    "recleanedFHV = recleanedFHV.filter(\\\n",
    "                                     (col('pickup_datetime').isNotNull()) &\\\n",
    "                                     (col('dropoff_datetime').isNotNull()) &\\\n",
    "                                     (col('dolocationid').isNotNull()) &\\\n",
    "                                     (col('pulocationid').isNotNull())\\\n",
    "                                     )\n",
    "\n",
    "# Before re-integration, we must remember that we have re-cast a lot of columns as floats and the rest of the data is StringType\n",
    "everdirtyFHV = everdirtyFHV.select([col(i).cast(\"string\") for i in everdirtyFHV.columns])\n",
    "recleanedFHV = recleanedFHV.select([col(i).cast(\"string\") for i in recleanedFHV.columns])\n",
    "\n",
    "# We can then drop the validation columns\n",
    "recleanedFHV = recleanedFHV.drop(*['dispatching_base_num_val','pickup_datetime_val','dropoff_datetime_val','pulocationid_val', \\\n",
    "                              'dolocationid_val','sr_flag_val','row_val'])\n",
    "\n",
    "# leaving us with just the filename variable as something which differentiates the schema from our clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FHV_combine(cleanedupFHVDF,alwaysdirtyFHVDF):\n",
    "    \n",
    "    reAdded = 0\n",
    "    for i in FHV_cleanCSV_folderPath:\n",
    "        dfi = create_df(i)\n",
    "\n",
    "        # We can here filter by filename to select the records which we would like to re-add\n",
    "        # We can also add this if to skip the files which don't have any repaired records\n",
    "        tempFilterDF = cleanedupFHVDF.filter(col('filename') == i.replace(clean_FHV_directory,\"\").\\\n",
    "                                                          replace('/all CSV/',\"\").\\\n",
    "                                                          replace('csvclean','csv'))\n",
    "        if tempFilterDF.count() > 0:\n",
    "            pre_existing = dfi.count()\n",
    "            print(str(pre_existing)+\" pre-existing clean records\")\n",
    "\n",
    "            dfi = dfi.unionByName(tempFilterDF.drop('filename'))\n",
    "\n",
    "            # We drop duplicates in case the negative values were for accounting/record purposes (although this is unlikely)\n",
    "            dupeTest = drop_dupes(dfi)\n",
    "            dfi = dupeTest[0]\n",
    "            addedDFI = dfi.count()-pre_existing\n",
    "            print(str(addedDFI)+\" additional records added\")\n",
    "            reAdded = reAdded + addedDFI\n",
    "        else:\n",
    "            print(\"File skipped - no cleaned up records\")\n",
    "            \n",
    "        # For our cleanup later, let's save even unmodified files\n",
    "        save_df(dfi,ntpath.basename(i), clean_FHV_directory)\n",
    "        \n",
    "    print(str(reAdded)+\" records added across all files\")\n",
    "    \n",
    "    if reAdded == 0:\n",
    "        print(\"WARNING: REPAIRED RECORDS COULD NOT BE RE-ADDED - ORIGINAL FILE WAS EMPTY\")\n",
    "    \n",
    "    # We have the dirty records saved on a monthly basis already anyway\n",
    "    # We will therefore save everdirtyFHV alone\n",
    "    # We will then have saved files per month with caught dirty records and one master file with unrecoverable records\n",
    "    # This will record unrecoverable values, but will not take hours to run\n",
    "    # We do not coalesce() here as the file is large (and we will not be reading it again outside of spark)\n",
    "    \n",
    "    alwaysdirtyFHVDF.write.options(header = True).mode(\"overwrite\").\\\n",
    "         csv(os.path.join(dirty_FHV_directory, \"irreparable_FHV_records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2017-06.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2017-07.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2017-08.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2017-09.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2017-10.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2017-11.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2017-12.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2018-01.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2018-02.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2018-03.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2018-04.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2018-05.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2018-06.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2018-07.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2018-08.csvclean\n",
      "36878 pre-existing clean records\n",
      "0 duplicate entries removed\n",
      "1 additional records added\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2018-09.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2018-10.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2018-11.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2018-12.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2019-01.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2019-02.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2019-03.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2019-04.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2019-05.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2019-06.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2019-07.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2019-08.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2019-09.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2019-10.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2019-11.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2020-01.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2020-03.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2020-04.csvclean\n",
      "File skipped - no cleaned up records\n",
      "opened /home/epb123/output/cleaned FHV/all CSV/fhv_tripdata_2020-05.csvclean\n",
      "File skipped - no cleaned up records\n",
      "1 records added across all files\n"
     ]
    }
   ],
   "source": [
    "FHV_combine(recleanedFHV, everdirtyFHV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/output/cleaned FHV/all CSV\n"
     ]
    }
   ],
   "source": [
    "# Clean the PySpark outputs from before\n",
    "FHV_cleanCSV_folderPath = cleanup(clean_FHV_directory)\n",
    "with open(v_direc + \"FHV_cleanCSV_folderPath\",'wb') as cleanedFHVCSVdirec:\n",
    "    pickle.dump(FHV_cleanCSV_folderPath,cleanedFHVCSVdirec)\n",
    "\n",
    "# FHV_dirtyCSV_folderPath = cleanup(dirty_FHV_directory)\n",
    "# with open(v_direc + \"FHV_dirtyCSV_folderPath\",'wb') as dirtyFHVCSVdirec:\n",
    "#     pickle.dump(FHV_dirtyCSV_folderPath,dirtyFHVCSVdirec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are deleting unused dataframes in order to preserve memory - we will need it for Yellow.\n",
    "del recleanedFHV\n",
    "del dirtyFHV\n",
    "del dirtyFHVanalysis\n",
    "del everdirtyFHV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the FHVHV dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FHVHV dataset is very similar to the FHV dataset - many of the same methods can therefore be re-used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values and validity constraints\n",
    "\n",
    "Variables are identical to FHV apart from hvfhs_license_num!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "hvfhs_license_num: a string either HV0002, HV0003, HV0004, HV0005  \n",
    "dispatching_base_num: TLC base license number - string  \n",
    "pickup_datetime: 2020-06-10 11:16:00  \n",
    "dropoff_datetime: 2020-06-10 11:16:00  \n",
    "pulocationid: int 1 to 263  \n",
    "dolocationid: int 1 to 263  \n",
    "sr_flag: int 1 or null\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and splitting off dirty records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will class a record (not a value) as being dirty if it is unusable for our purposes in 2.4.\n",
    "\n",
    "If the record itself is dirty, we will attempt to use the remaining variables to repair it, but otherwise we will only track errors caught by our constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/output/cleaned FHVHV\n",
      "folder already exists at /home/epb123/output/dirty FHVHV\n"
     ]
    }
   ],
   "source": [
    "clean_FHVHV_directory = create_folder(\"cleaned FHVHV\", save_directory)\n",
    "with open(v_direc + \"clean_FHVHV_directory\",'wb') as cleanedfhvHVdirec:\n",
    "    pickle.dump(clean_FHVHV_directory,cleanedfhvHVdirec)\n",
    "    \n",
    "dirty_FHVHV_directory = create_folder(\"dirty FHVHV\", save_directory)\n",
    "with open(v_direc + \"dirty_FHVHV_directory\",'wb') as dirtyfhvHVdirec:\n",
    "    pickle.dump(dirty_FHVHV_directory,dirtyfhvHVdirec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to split FHVHV can be almost identical to that for FHV - the only additional variable is the license number, which plays no role in our intended analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FHVHV_cleanup():\n",
    "# takes no arguments - splits FHV into clean and dirty data\n",
    "# returns a pyspark dataframe with dirty data\n",
    "\n",
    "    dropped_duplicates_count = 0\n",
    "    clean_count = 0\n",
    "    row_count = 0\n",
    "        \n",
    "    for i in fhvhv_integratedPaths:\n",
    "        dfi = create_df(i)\n",
    "        dfi_row_count = dfi.count()\n",
    "        row_count = row_count + dfi_row_count\n",
    "\n",
    "        # Adding validation columns\n",
    "        dfi = add_license_validation(dfi)\n",
    "        dfi = add_basenum_validation(dfi)\n",
    "        dfi = add_datetime_validation(dfi)\n",
    "        dfi = add_location_validation(dfi)\n",
    "        dfi = add_sr_validation(dfi)\n",
    "        dfi = add_FHV_HV_row_validation(dfi)\n",
    "\n",
    "        # Store records which are not dirty (this does not consider variables useless for 2.4 completion)\n",
    "        # We also want to keep records which we can use - namely those with a pair of locationID values\n",
    "        dfiClean = dfi.filter((dfi.row_val == 0))\n",
    "        \n",
    "        dfiClean = dfiClean.drop(*['hvfhs_license_num_val','dispatching_base_num_val','pickup_datetime_val','dropoff_datetime_val','pulocationid_val', \\\n",
    "                              'dolocationid_val','sr_flag_val','row_val'])\n",
    "\n",
    "        # We can drop duplicates in the clean dataset here - they will all have pickup and dropoff times or locations\n",
    "        # The times are stored to the millisecond and PySpark's drop_duplicates compares all columns\n",
    "        # If two records have pickup and dropoff times identical to the millisecond, we can safely drop them\n",
    "        # If two records have both location ids identical and identical dispatch numbers, we can also safely drop them\n",
    "        duplicates_result = drop_dupes(dfiClean)\n",
    "        dfiClean = duplicates_result[0]\n",
    "\n",
    "        dropped_duplicates_count = dropped_duplicates_count + duplicates_result[1]\n",
    "\n",
    "        dfiClean_row_count = duplicates_result[2]\n",
    "        clean_count = clean_count + dfiClean_row_count\n",
    "\n",
    "        # Save the cleaned dataframes - spark will save an empty dataframe, but it will have no schema!\n",
    "        if dfiClean_row_count > 0:\n",
    "            save_df(dfiClean,ntpath.basename(i) + 'clean', clean_FHVHV_directory)\n",
    "        else:\n",
    "            print(\"no remaining records - CSV not saved\")\n",
    "\n",
    "        dirtyInI = dfi.filter((dfi.row_val == 1) | (dfi.row_val == 2))\\\n",
    "            .withColumn(\"filename\", lit(ntpath.basename(i)))\n",
    "        \n",
    "        # We then add the dirty records to a spark dataframe, with an additional column specifying file of origin\n",
    "        if i == fhvhv_integratedPaths[0]:\n",
    "            dfiDirty = dirtyInI\n",
    "\n",
    "        else:\n",
    "            dfiDirty = dfiDirty.unionByName(dirtyInI)\n",
    "\n",
    "        save_df(dirtyInI,ntpath.basename(i) + 'dirty', dirty_FHVHV_directory)\n",
    "        \n",
    "        print(str(dfiClean_row_count) + ' clean records preserved out of ' + str(dfi_row_count) + \\\n",
    "              ' original records - ' + str(dfiClean_row_count/dfi_row_count*100)[0:4] + \"%\")\n",
    "        print(str(dirtyInI.filter(dirtyInI.row_val == 1).count()) + \" invalid records have been found\")\n",
    "        print(str(dirtyInI.filter(dirtyInI.row_val == 2).count()) + \" records with missing data have been found\")\n",
    "        \n",
    "        # We don't need them, but we can check for dirty sr_flag, license number and basenumber values\n",
    "        dfi = dfi.select([dfi.sr_flag_val, dfi.dispatching_base_num_val, dfi.hvfhs_license_num_val])\n",
    "        \n",
    "        print(\"For the variables useless for our 2.4 analysis, we have (among values not missing):\")\n",
    "        dfi.filter((dfi.sr_flag_val == 1)).\\\n",
    "                   agg(sql.sum(\"sr_flag_val\")).show()\n",
    "        dfi.filter((dfi.dispatching_base_num_val == 1)).\\\n",
    "                   agg(sql.sum(\"dispatching_base_num_val\")).show()\n",
    "        dfi.filter((dfi.hvfhs_license_num_val == 1)).\\\n",
    "                   agg(sql.sum(\"hvfhs_license_num_val\")).show()\n",
    "                   \n",
    "    print(\"Out of \" + str(row_count) + \" original records, \" + str(clean_count) + \" records were clean.\")\n",
    "    print(str(dropped_duplicates_count) + \" records were dropped as duplicates.\")\n",
    "    print(str(row_count - clean_count - dropped_duplicates_count) \\\n",
    "          + \" dirty records remain for resolution.\")\n",
    "    \n",
    "    return dfiDirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened /home/epb123/output/integrated FHVHV/fhvhv_tripdata_2019-02.csv\n",
      "0 duplicate entries removed\n",
      "39089 clean records preserved out of 40290 original records - 97.0%\n",
      "1201 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "+--------------------------+\n",
      "|sum(hvfhs_license_num_val)|\n",
      "+--------------------------+\n",
      "|                      null|\n",
      "+--------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHVHV/fhvhv_tripdata_2019-03.csv\n",
      "0 duplicate entries removed\n",
      "46287 clean records preserved out of 47703 original records - 97.0%\n",
      "1416 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "+--------------------------+\n",
      "|sum(hvfhs_license_num_val)|\n",
      "+--------------------------+\n",
      "|                      null|\n",
      "+--------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHVHV/fhvhv_tripdata_2019-04.csv\n",
      "0 duplicate entries removed\n",
      "42154 clean records preserved out of 43420 original records - 97.0%\n",
      "1266 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "+--------------------------+\n",
      "|sum(hvfhs_license_num_val)|\n",
      "+--------------------------+\n",
      "|                      null|\n",
      "+--------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHVHV/fhvhv_tripdata_2019-05.csv\n",
      "0 duplicate entries removed\n",
      "43225 clean records preserved out of 44613 original records - 96.8%\n",
      "1388 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "+--------------------------+\n",
      "|sum(hvfhs_license_num_val)|\n",
      "+--------------------------+\n",
      "|                      null|\n",
      "+--------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHVHV/fhvhv_tripdata_2019-06.csv\n",
      "0 duplicate entries removed\n",
      "40551 clean records preserved out of 41965 original records - 96.6%\n",
      "1414 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "+--------------------------+\n",
      "|sum(hvfhs_license_num_val)|\n",
      "+--------------------------+\n",
      "|                      null|\n",
      "+--------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHVHV/fhvhv_tripdata_2020-01.csv\n",
      "0 duplicate entries removed\n",
      "39803 clean records preserved out of 41138 original records - 96.7%\n",
      "1335 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "+--------------------------+\n",
      "|sum(hvfhs_license_num_val)|\n",
      "+--------------------------+\n",
      "|                      null|\n",
      "+--------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHVHV/fhvhv_tripdata_2020-03.csv\n",
      "0 duplicate entries removed\n",
      "26004 clean records preserved out of 26785 original records - 97.0%\n",
      "781 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "+--------------------------+\n",
      "|sum(hvfhs_license_num_val)|\n",
      "+--------------------------+\n",
      "|                      null|\n",
      "+--------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHVHV/fhvhv_tripdata_2020-04.csv\n",
      "0 duplicate entries removed\n",
      "8380 clean records preserved out of 8625 original records - 97.1%\n",
      "245 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "+--------------------------+\n",
      "|sum(hvfhs_license_num_val)|\n",
      "+--------------------------+\n",
      "|                      null|\n",
      "+--------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHVHV/fhvhv_tripdata_2020-05.csv\n",
      "0 duplicate entries removed\n",
      "11870 clean records preserved out of 12179 original records - 97.4%\n",
      "309 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "+--------------------------+\n",
      "|sum(hvfhs_license_num_val)|\n",
      "+--------------------------+\n",
      "|                      null|\n",
      "+--------------------------+\n",
      "\n",
      "opened /home/epb123/output/integrated FHVHV/fhvhv_tripdata_2020-06.csv\n",
      "0 duplicate entries removed\n",
      "14645 clean records preserved out of 15101 original records - 96.9%\n",
      "456 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "For the variables useless for our 2.4 analysis, we have (among values not missing):\n",
      "+----------------+\n",
      "|sum(sr_flag_val)|\n",
      "+----------------+\n",
      "|            null|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sum(dispatching_base_num_val)|\n",
      "+-----------------------------+\n",
      "|                         null|\n",
      "+-----------------------------+\n",
      "\n",
      "+--------------------------+\n",
      "|sum(hvfhs_license_num_val)|\n",
      "+--------------------------+\n",
      "|                      null|\n",
      "+--------------------------+\n",
      "\n",
      "Out of 321819 original records, 312008 records were clean.\n",
      "0 records were dropped as duplicates.\n",
      "9811 dirty records remain for resolution.\n"
     ]
    }
   ],
   "source": [
    "dirtyFHVHV = FHVHV_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the dirty dataframe separately to avoid unfortunate errors\n",
    "# We can update our cleaned values as we go\n",
    "\n",
    "recleanedFHVHV = dirtyFHVHV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9811 rows have damaged (not missing) data\n",
      "Checking hvfhs_license_num_val\n",
      "Checking dispatching_base_num_val\n",
      "Checking pickup_datetime_val\n",
      "Checking dropoff_datetime_val\n",
      "Checking pulocationid_val\n",
      "Checking dolocationid_val\n",
      "Checking sr_flag_val\n",
      "Checking row_val\n",
      "{'pickup_datetime_val': 1, 'dropoff_datetime_val': 1, 'pulocationid_val': 20, 'dolocationid_val': 9796, 'row_val': 9811}\n"
     ]
    }
   ],
   "source": [
    "FHVHVdamaged_rows = dirtyFHVHV.filter(dirtyFHVHV.row_val == 1).count()\n",
    "print(str(FHVHVdamaged_rows) + ' rows have damaged (not missing) data')\n",
    "\n",
    "# we can check filter by row_val = 1 first --> by construction\n",
    "# any row with a 1 anywhere will have taken row_val = 1\n",
    "# these rows contain invalid, but not missing variable entries\n",
    "\n",
    "def FHVHV_dirty_count(FHVHV1):\n",
    "# Count corrupted records by variable type\n",
    "    dirty_records = {}\n",
    "\n",
    "    for i in range (7,len(FHVHV1.columns)-1):\n",
    "        print('Checking ' + str(FHVHV1.columns[i]))\n",
    "        tick = FHVHV1.filter(col(FHVHV1.columns[i]) == 1).count()\n",
    "              \n",
    "        if tick != 0:\n",
    "            dirty_records[FHVHV1.columns[i]] = tick\n",
    "\n",
    "    return dirty_records\n",
    "\n",
    "# Let's start by seeing where the majority of errors lie (if there are any)\n",
    "if FHVHVdamaged_rows != 0:\n",
    "    print(FHVHV_dirty_count(dirtyFHVHV.filter(dirtyFHVHV.row_val == 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have very little errors aside from the dolocationID. As we need dolocationid for queries 2 and 8, we hope to repair them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with analysing the **dolocationid** and **pulocationid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 missing dropoff values\n",
      "0 missing pickup values\n"
     ]
    }
   ],
   "source": [
    "# Let's count our missing records first\n",
    "print(str(dirtyFHVHV.filter(dirtyFHVHV.dolocationid_val == 2).count())+\" missing dropoff values\")\n",
    "print(str(dirtyFHVHV.filter(dirtyFHVHV.pulocationid_val == 2).count())+\" missing pickup values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then start with **pulocationid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 0 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "# Let us now check to see if the values are incorrect strings\n",
    "# We know the locationids here are not null, thus we can do...\n",
    "\n",
    "dirtyFHVHVanalysis = dirtyFHVHV.filter(col('pulocationid_val') == 1)\n",
    "\n",
    "#CASTING AS INT HERE!!!!!!\n",
    "print(\"we have \" + str(dirtyFHVHVanalysis.select(\"pulocationid\",sql.col(\"pulocationid\").cast(\"int\").isNotNull().alias(\"strings\")).\\\n",
    "        filter(col('strings') == 'false').count()) + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 20 values above our threshold\n",
      "We have 0 zero values\n",
      "We have 0 values below our threshold\n"
     ]
    }
   ],
   "source": [
    "print(\"We have \"+str(recleanedFHVHV.filter(col('pulocationid') > 263).count())+\" values above our threshold\")\n",
    "print(\"We have \"+str(recleanedFHVHV.filter(col('pulocationid') == 0).count())+\" zero values\")\n",
    "print(\"We have \"+str(recleanedFHVHV.filter(col('pulocationid') < 0).count())+\" values below our threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot repair these - all results coming from our location conversion would have fallen within our boundaries. All remaining results must be from later schemas without conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "recleanedFHVHV = recleanedFHVHV.withColumn('pulocationid', sql.when(col('pulocationid') > 263,\\\n",
    "                                            None).otherwise(col('pulocationid')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 excess values remain after cleaning\n"
     ]
    }
   ],
   "source": [
    "print(str(recleanedFHVHV.filter(col('pulocationid') > 263).count())+\" excess values remain after cleaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to **dolocationid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 0 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "# Let us now check to see if the values are incorrect strings\n",
    "# We know the locationids here are not null, thus we can do...\n",
    "\n",
    "dirtyFHVHVanalysis = dirtyFHVHV.filter(col('dolocationid_val') == 1)\n",
    "\n",
    "#CASTING AS INT HERE!!!!!!\n",
    "print(\"we have \" + str(dirtyFHVHVanalysis.filter(col(\"dolocationid\").cast(\"int\").isNull()).count()) + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 9796 values above our threshold\n",
      "We have 0 zero values\n",
      "We have 0 values below our threshold\n"
     ]
    }
   ],
   "source": [
    "print(\"We have \"+str(recleanedFHVHV.filter(col('dolocationid') > 263).count())+\" values above our threshold\")\n",
    "print(\"We have \"+str(recleanedFHVHV.filter(col('dolocationid') == 0).count())+\" zero values\")\n",
    "print(\"We have \"+str(recleanedFHVHV.filter(col('dolocationid') < 0).count())+\" values below our threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "recleanedFHVHV = recleanedFHVHV.withColumn('dolocationid', sql.when(col('dolocationid') > 263,\\\n",
    "                                            None).otherwise(col('dolocationid')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 excess values remain after cleaning\n"
     ]
    }
   ],
   "source": [
    "print(str(recleanedFHVHV.filter(col('dolocationid') > 263).count())+\" excess values remain after cleaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot do anything about the missing values with the data at hand, so let's move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not forget to examine the incorrect **pickup/dropoff_datetime** columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1 dirty dropoff values and...\n",
      "0 missing dropoff values\n",
      "We have 1 dirty pickup values and...\n",
      "0 missing pickup values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where our dropped records came from\n",
    "print(\"We have \"+str(dirtyFHVHV.filter(dirtyFHVHV.dropoff_datetime_val == 1).count())+\" dirty dropoff values and...\")\n",
    "print(str(dirtyFHVHV.filter(dirtyFHVHV.dropoff_datetime_val == 2).count())+\" missing dropoff values\")\n",
    "print(\"We have \"+str(dirtyFHVHV.filter(dirtyFHVHV.pickup_datetime_val == 1).count())+\" dirty pickup values and...\")\n",
    "print(str(dirtyFHVHV.filter(dirtyFHVHV.pickup_datetime_val == 2).count())+\" missing pickup values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the dirty values must come from negative trip distances or invalid formats. Let us test for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will now drop 1 invalid values\n",
      "We will now repair 0 backwards values\n"
     ]
    }
   ],
   "source": [
    "# Make the switch Flag dummy\n",
    "recleanedFHVHV = recleanedFHVHV.withColumn('tsFlag',udfdatetime_switch('pickup_datetime','dropoff_datetime'))\n",
    "\n",
    "print(\"We will now drop \"+str(recleanedFHVHV.filter(col('tsFlag') == 2).count())+\" invalid values\")\n",
    "\n",
    "recleanedFHVHV = recleanedFHVHV.withColumn('dropoff_datetime',sql.when(col('tsFlag') == 2, None).\\\n",
    "                                           otherwise(col('dropoff_datetime')))\n",
    "recleanedFHVHV = recleanedFHVHV.withColumn('pickup_datetime',sql.when(col('tsFlag') == 2, None).\\\n",
    "                                           otherwise(col('pickup_datetime')))\n",
    "\n",
    "print(\"We will now repair \"+str(recleanedFHVHV.filter(col('tsFlag') == 1).count())+\" backwards values\")\n",
    "\n",
    "# Put dropoff values into a temporary column\n",
    "recleanedFHVHV = recleanedFHVHV.withColumn('dropoff_temp',col('dropoff_datetime'))\n",
    "\n",
    "# Move pickup values into dropoff when switch flag is 1\n",
    "recleanedFHVHV = recleanedFHVHV.withColumn('dropoff_datetime',sql.when(col('tsFlag') == 1,col('pickup_datetime')).\\\n",
    "                                           otherwise(col('dropoff_datetime')))\n",
    "# Move dropoff values from temporary column into pickup when switch flag is 1\n",
    "recleanedFHVHV = recleanedFHVHV.withColumn('pickup_datetime',sql.when(col('tsFlag') == 1,col('dropoff_temp')).\\\n",
    "                                           otherwise(col('pickup_datetime')))\n",
    "\n",
    "# Drop temporary columns\n",
    "recleanedFHVHV = recleanedFHVHV.drop(*['tsFlag','dropoff_temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 values which we could possibly use to compute query 2 for FHVHV records.\n"
     ]
    }
   ],
   "source": [
    "FHVHVpotential = dirtyFHVHV.filter((col('row_val') == 2) & (col('pulocationid_val') == 0) & (col('dolocationid_val') == 0)).count()\n",
    "print(\"There are \"+str(FHVHVpotential)+\" values which we could possibly use to compute query 2 for FHVHV records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/output/cleaned FHVHV/all CSV\n",
      "folder already exists at /home/epb123/output/dirty FHVHV/all CSV\n"
     ]
    }
   ],
   "source": [
    "# Run if saving as PySpark\n",
    "FHVHV_cleanCSV_folderPath = cleanup(clean_FHVHV_directory)\n",
    "with open(v_direc + \"FHVHV_cleanCSV_folderPath\",'wb') as cleanedFHVHVCSVdirec:\n",
    "    pickle.dump(FHVHV_cleanCSV_folderPath,cleanedFHVHVCSVdirec)\n",
    "\n",
    "FHVHV_dirtyCSV_folderPath = cleanup(dirty_FHVHV_directory)\n",
    "with open(v_direc + \"FHVHV_dirtyCSV_folderPath\",'wb') as dirtyFHVHVCSVdirec:\n",
    "    pickle.dump(FHVHV_dirtyCSV_folderPath,dirtyFHVHVCSVdirec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have managed to repair 0 complete records\n"
     ]
    }
   ],
   "source": [
    "# We've set irreparable values as Null, so...\n",
    "print(\"We have managed to repair \"+str(recleanedFHVHV.filter(\\\n",
    "                                                             (col('pickup_datetime').isNotNull()) &\\\n",
    "                                                             (col('dropoff_datetime').isNotNull()) &\\\n",
    "                                                             (col('dolocationid').isNotNull()) &\\\n",
    "                                                             (col('pulocationid').isNotNull())).count())+\" complete records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as if the massive number of unrepairable location errors means we cannot include even those repairable datetime values, and we do not want to add incomplete records to our clean dataset - we will save our dirty records and move on to Green, having lost all 9,811 dirty records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us save the dirty dataset   \n",
    "recleanedFHVHV.write.options(header = True).mode(\"overwrite\").csv(os.path.join(dirty_FHVHV_directory, \"irreparable_FHVHV_records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "del recleanedFHVHV\n",
    "del dirtyFHVHV\n",
    "del dirtyFHVHVanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions for cleanup of Green and Yellow\n",
    "Auxiliary code to help in the data cleaning process goes here\n",
    "\n",
    "We need further functions to check the Green and Yellow datasets, based on their greater number of variables. We again rely on UDFs as again - the expense is worth the convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-2015 improvement surcharge testing function stored as UDF\n",
      "Pre-2015 improvement surcharge testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def improvement_validation_post2015(imp):\n",
    "# Function to validate improvement surcharge float values\n",
    "# Function returns 0 if the figure is valid, 1 if it is invalid, 2 if it is None\n",
    "# This just tests the validation is not a string and is equal to either 0 or 0.3, the values given by the data dictionary.\n",
    "    \n",
    "    if imp == None or imp == \"\" or imp == \"nan\":\n",
    "        return 2\n",
    "    \n",
    "    try:\n",
    "        imp = float(imp)\n",
    "        if imp == 0 or imp == 0.3:\n",
    "            return 0\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "# Define function as a UDF for pyspark usage\n",
    "udfimp_validation_post2015 = udf(improvement_validation_post2015, IntegerType())\n",
    "\n",
    "print(\"Post-2015 improvement surcharge testing function stored as UDF\")\n",
    "\n",
    "def improvement_validation_pre2015(imp):\n",
    "# Function to validate improvement surcharge float values\n",
    "# Function returns 0 if the figure is valid, 1 if it is invalid\n",
    "    \n",
    "    if imp == None or imp == \"\" or imp == 'nan':\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        if float(imp) == 0:\n",
    "            return 0\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "# Define function as a UDF for pyspark usage\n",
    "udfimp_validation_pre2015 = udf(improvement_validation_pre2015, IntegerType())\n",
    "\n",
    "print(\"Pre-2015 improvement surcharge testing function stored as UDF\")   \n",
    "\n",
    "def add_improvement_surcharge_validation(dfIMPval, pathaddress):\n",
    "# Adds the improvement surcharge validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    if int(pathaddress[-11:-7]) < 2015:\n",
    "        dfIMPval = dfIMPval.withColumn('improvement_surcharge_val', udfimp_validation_pre2015('improvement_surcharge'))\n",
    "    else:                                      \n",
    "        dfIMPval = dfIMPval.withColumn('improvement_surcharge_val', udfimp_validation_post2015('improvement_surcharge'))\n",
    "                                        \n",
    "    return dfIMPval                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tax testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def tax_validation(tax):\n",
    "# Function to validate mta tax float values\n",
    "# Function returns 0 if the figure is valid, 1 if it is invalid, 2 if it is None\n",
    "# Tax has to be 0 or 0.5\n",
    "    \n",
    "    if tax == None or tax == \"\" or tax == 'nan':\n",
    "        return 2\n",
    "    \n",
    "    try:\n",
    "        tax = float(tax)\n",
    "        if tax == 0 or tax == 0.5:\n",
    "            return 0\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "# Define function as a UDF for pyspark usage\n",
    "udftax_validation = udf(tax_validation, IntegerType())\n",
    "\n",
    "print(\"tax testing function stored as UDF\")\n",
    "\n",
    "def add_mta_tax_validation(dfTXval):\n",
    "# Adds the tax validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfTXval = dfTXval.withColumn('mta_tax_val', udftax_validation('mta_tax'))\n",
    "\n",
    "    return dfTXval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def extra_validation(extra):\n",
    "# Function to validate extra float values\n",
    "# Function returns 0 if the figure is valid, 1 if it is invalid OR SUSPICIOUS, 2 if it is None\n",
    "# Extra has to be the values specified below, in addition to other criteria\n",
    "    \n",
    "    if extra == None or extra == \"\" or extra == 'nan':\n",
    "        return 2\n",
    "    \n",
    "    try:\n",
    "        extra = float(extra)\n",
    "    \n",
    "        # Want to flag values above $1.50 - data dictionary says $1.50 should be the max\n",
    "        if extra == 0 or extra == 0.5 or extra == 1 or extra == 1.5:\n",
    "            return 0\n",
    "    \n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "# Define function as a UDF for pyspark usage\n",
    "udfextra_validation = udf(extra_validation, IntegerType())\n",
    "\n",
    "print(\"extra testing function stored as UDF\")\n",
    "\n",
    "def add_extra_validation(dfEXval):\n",
    "# Adds the extra validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfEXval = dfEXval.withColumn('extra_val', udfextra_validation('extra'))\n",
    "\n",
    "    return dfEXval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def money_validation(cash):\n",
    "# Function to validate money/transaction float values\n",
    "# Function returns 0 if the figure is valid, 1 if it is invalid OR SUSPICIOUS, 2 if it is None\n",
    "# Want to flag values above 1000 - fares, tips, tolls, totals shouldn't be above 1000\n",
    "# items should be able to = 0, in particular tolls and tip\n",
    "# Due to their similarity, the same UDF can be used for multiple of the money-based variables.\n",
    "\n",
    "    if cash == None or cash == \"\" or cash == 'nan':\n",
    "        return 2\n",
    "    \n",
    "    try:\n",
    "        cash = float(cash)\n",
    "\n",
    "        if cash >= 0 and cash <= 1000:\n",
    "            return 0\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "# Define function as a UDF for pyspark usage\n",
    "udfmoney_validation = udf(money_validation, IntegerType())\n",
    "\n",
    "print(\"money testing function stored as UDF\")\n",
    "\n",
    "def add_fare_amount_validation(dfCASHval):\n",
    "# Adds the specified validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfCASHval = dfCASHval.withColumn('fare_amount_val', udfmoney_validation('fare_amount'))\n",
    "\n",
    "    return dfCASHval\n",
    "\n",
    "def add_tip_amount_validation(dfCASHval):\n",
    "# Adds the specified validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfCASHval = dfCASHval.withColumn('tip_amount_val', udfmoney_validation('tip_amount'))\n",
    "\n",
    "    return dfCASHval\n",
    "\n",
    "def add_tolls_amount_validation(dfCASHval):\n",
    "# Adds the specified validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfCASHval = dfCASHval.withColumn('tolls_amount_val', udfmoney_validation('tolls_amount'))\n",
    "\n",
    "    return dfCASHval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store-and-forward-flag testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def sflag_validation(sflag):\n",
    "# Function to validate store_and_fwd_flag values\n",
    "# Function returns 0 if the flag is valid, 1 if it is invalid, 2 if it is None\n",
    "    \n",
    "    if sflag == None or sflag == \"\" or sflag == \"nan\":\n",
    "        return 2\n",
    "    \n",
    "    if sflag == \"Y\" or sflag == \"N\":\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "# Define function as a UDF for pyspark usage\n",
    "udfsflag_validation = udf(sflag_validation, IntegerType())\n",
    "\n",
    "print(\"store-and-forward-flag testing function stored as UDF\")\n",
    "\n",
    "def add_store_and_forward_flag_validation(dfFLAGval):\n",
    "# Adds the store and forward flag validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfFLAGval = dfFLAGval.withColumn('store_and_fwd_flag_val', udfsflag_validation('store_and_fwd_flag'))\n",
    "\n",
    "    return dfFLAGval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip distance testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def distance_validation(tdist):\n",
    "# Function to validate trip distance values\n",
    "# Function returns 0 if the trip distance is valid, 1 if it is invalid OR SUSPICIOUS, 2 if it is None\n",
    "# Want to flag values above 300 - NYC to Boston/DC is ~300 miles and some people might use taxis for this\n",
    "# Anything above that is probably an error\n",
    "# Values =0 are not trips - probably errors\n",
    "\n",
    "    if tdist == None or tdist == \"\" or tdist == \"nan\":\n",
    "        return 2\n",
    "    \n",
    "    try:\n",
    "        tdist = float(tdist)\n",
    "\n",
    "        if tdist > 0 and tdist <= 300:\n",
    "            return 0\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "# Define function as a UDF for pyspark usage\n",
    "udfdistance_validation = udf(distance_validation, IntegerType())\n",
    "\n",
    "print(\"trip distance testing function stored as UDF\")\n",
    "\n",
    "def add_trip_distance_validation(dfDISTval):\n",
    "# Adds the trip distance validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfDISTval = dfDISTval.withColumn('trip_distance_val', udfdistance_validation('trip_distance'))\n",
    "\n",
    "    return dfDISTval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passenger count testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def passenger_count_validation(pcount):\n",
    "# Function to validate passenger count values\n",
    "# Function returns 0 if the passenger count is valid, 1 if it is invalid OR SUSPICIOUS, 2 if it is None\n",
    "# Want to flag values above 10 - it's probably an error if >10 people are in the same taxi\n",
    "    \n",
    "    if pcount == None or pcount == \"\" or pcount == \"nan\":\n",
    "        return 2\n",
    "    \n",
    "    # want to catch non-integer errors\n",
    "    try:\n",
    "        pcount = int(pcount)\n",
    "        if pcount > 0 and pcount <= 10:\n",
    "            return 0\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "# Define function as a UDF for pyspark usage\n",
    "udfpassengercount_validation = udf(passenger_count_validation, IntegerType())\n",
    "\n",
    "print(\"passenger count testing function stored as UDF\")\n",
    "\n",
    "def add_passenger_count_validation(dfPASSval):\n",
    "# Adds the passenger count validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfPASSval = dfPASSval.withColumn('passenger_count_val', udfpassengercount_validation('passenger_count'))\n",
    "\n",
    "    return dfPASSval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vendorid and trip type testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def vendorid_triptype_validation(vidttype):\n",
    "# Function to validate vendorid and trip type values\n",
    "# Function returns 0 if the vendorid or trip type is valid, 1 if it is invalid, 2 if it is None\n",
    "# We know from the dictionary that VendorID must be either 1 or 2\n",
    "    \n",
    "    if vidttype == None or vidttype == \"\" or vidttype == 'nan':\n",
    "        return 2\n",
    "    \n",
    "    # want to catch non-integer errors\n",
    "    try:\n",
    "        vidttype = int(vidttype)\n",
    "        if vidttype == 1 or vidttype == 2:\n",
    "            return 0\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "# Define function as a UDF for pyspark usage\n",
    "udfvendoridTripType_validation = udf(vendorid_triptype_validation, IntegerType())\n",
    "\n",
    "print(\"vendorid and trip type testing function stored as UDF\")\n",
    "\n",
    "def add_vendorid_validation(dfVIDval):\n",
    "# Adds the vendorid validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfVIDval = dfVIDval.withColumn('vendorid_val', udfvendoridTripType_validation('vendorid'))\n",
    "\n",
    "    return dfVIDval\n",
    "\n",
    "def add_trip_type_validation(dfTTval):\n",
    "# Adds the trip_type validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfTTval = dfTTval.withColumn('trip_type_val', udfvendoridTripType_validation('trip_type'))\n",
    "\n",
    "    return dfTTval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payment type and ratecodeid testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def payment_validation(paymentrate,tip):\n",
    "# Function to validate payment type and ratecodeid values\n",
    "# Function returns 0 if the payment type is valid, 1 if it is invalid, 2 if it is None\n",
    "# Explanations on the filtering mechanism follow below\n",
    "    \n",
    "    if paymentrate == None or paymentrate == \"\" or paymentrate == 'nan':\n",
    "        return 2\n",
    "    \n",
    "    # Flag non-integers\n",
    "    try:\n",
    "        paymentrate = int(paymentrate)   \n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "    # Flag invalid values\n",
    "    if paymentrate < 1 or paymentrate > 6:\n",
    "        return 1\n",
    "    \n",
    "    # if Payment_type is 3 or 6, we will flag the value (No charge or voided trip)\n",
    "    if paymentrate == 3 or paymentrate == 6:\n",
    "        return 1\n",
    "    \n",
    "    # If the payment_type corresponds to a cash payment, but the tip is positive, we will trust the tip entry\n",
    "    # We do this because the tip record is almost certainly more reliable than the cash/card button on the machine.\n",
    "    try:\n",
    "        tip = abs(float(tip))\n",
    "        if paymentrate == 2 and tip != 0:\n",
    "                return 1\n",
    "    except:\n",
    "        # If we can't check the tip due to an incorrect string, return 0\n",
    "        return 0\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "# Define function as a UDF for pyspark usage\n",
    "udfpaymentType_validation = udf(payment_validation, IntegerType())\n",
    "\n",
    "def add_payment_type_validation(dfPAYval):\n",
    "# Adds the payment type validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfPAYval = dfPAYval.withColumn('payment_type_val', udfpaymentType_validation('payment_type','tip_amount'))\n",
    "\n",
    "    return dfPAYval\n",
    "\n",
    "def ratecode_validation(paymentrate):\n",
    "# Function to validate payment type and ratecodeid values\n",
    "# Function returns 0 if the payment type is valid, 1 if it is invalid, 2 if it is None\n",
    "    \n",
    "    if paymentrate == None or paymentrate == \"\" or paymentrate == 'nan':\n",
    "        return 2\n",
    "    \n",
    "    # Flag non-integers\n",
    "    try:\n",
    "        paymentrate = int(paymentrate)\n",
    "    \n",
    "        if paymentrate >= 1 and paymentrate <= 6:\n",
    "            return 0\n",
    "    \n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "# Define function as a UDF for pyspark usage\n",
    "udfRatecodeID_validation = udf(ratecode_validation, IntegerType())\n",
    "\n",
    "def add_ratecodeID_validation(dfRIDval):\n",
    "# Adds the ratecode ID validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfRIDval = dfRIDval.withColumn('ratecodeid_val', udfRatecodeID_validation('ratecodeid'))\n",
    "\n",
    "    return dfRIDval\n",
    "\n",
    "print(\"payment type and ratecodeid testing function stored as UDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ehail testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def ehail_validation(ehail):\n",
    "# Function to validate ehail_fee values\n",
    "# Function returns 0 if the figure is valid, 1 if it is invalid OR SUSPICIOUS, 2 if it is None\n",
    "    \n",
    "    if ehail == None or ehail == \"\" or ehail == 'nan':\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "# Define function as a UDF for pyspark usage\n",
    "udfehail_validation = udf(ehail_validation, IntegerType())\n",
    "\n",
    "print(\"ehail testing function stored as UDF\")\n",
    "\n",
    "def add_ehail_validation(dfEHval):\n",
    "# Adds the ehail validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfEHval = dfEHval.withColumn('ehail_fee_val', udfehail_validation('ehail_fee'))\n",
    "\n",
    "    return dfEHval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green Congestion Surcharge testing function stored as UDF\n",
      "Yellow Congestion Surcharge testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def GREENcongestion_surcharge_validation(cs):\n",
    "# Function to validate congestion_surcharge values\n",
    "# Function returns 0 if the figure is valid, 1 if it is invalid OR SUSPICIOUS, 2 if it is None\n",
    "# This was not in the Data Dictionary, however the data appears to require 2.75 for green - we will explore this with the UDF\n",
    "\n",
    "    if cs == None or cs == \"\" or cs == 'nan':\n",
    "        return 2\n",
    "    \n",
    "    try:\n",
    "        cs = float(cs)\n",
    "    \n",
    "        # Want to flag values not = 2.75 or = 0\n",
    "        if cs == 2.75 or cs == 0:\n",
    "            return 0\n",
    "    \n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    return 1\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udfGREENCS_validation = udf(GREENcongestion_surcharge_validation, IntegerType())\n",
    "\n",
    "print(\"Green Congestion Surcharge testing function stored as UDF\")\n",
    "\n",
    "def add_GREEN_congestion_surcharge_validation(dfCSval):\n",
    "# Adds the extra validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfCSval = dfCSval.withColumn('congestion_surcharge_val', udfGREENCS_validation('congestion_surcharge'))\n",
    "\n",
    "    return dfCSval\n",
    "\n",
    "def YELLOWcongestion_surcharge_validation(cs):\n",
    "# Function to validate congestion_surcharge values\n",
    "# Function returns 0 if the figure is valid, 1 if it is invalid OR SUSPICIOUS, 2 if it is None\n",
    "# This was not in the Data Dictionary, however the data appears to require 2.5 for Yellow - we will explore this with the UDF\n",
    "\n",
    "    if cs == None or cs == \"\" or cs == 'nan':\n",
    "        return 2\n",
    "    \n",
    "    try:\n",
    "        cs = float(cs)\n",
    "    \n",
    "        # Want to flag values not = 2.5 or = 0\n",
    "        if cs == 2.5 or cs == 0:\n",
    "            return 0\n",
    "    \n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    return 1\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udfYELLOWCS_validation = udf(YELLOWcongestion_surcharge_validation, IntegerType())\n",
    "\n",
    "print(\"Yellow Congestion Surcharge testing function stored as UDF\")\n",
    "\n",
    "def add_YELLOW_congestion_surcharge_validation(dfCSval):\n",
    "# Adds the extra validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfCSval = dfCSval.withColumn('congestion_surcharge_val', udfYELLOWCS_validation('congestion_surcharge'))\n",
    "\n",
    "    return dfCSval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green receipt validation testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def GREENreceipt_val(total,cong_s,extra,fare,improv,tax,tip,toll,ehail):\n",
    "# Function to validate total_amount values\n",
    "# Function returns 0 if the figure is valid, 1 if it is invalid OR SUSPICIOUS, 2 if it is None\n",
    "# Essentially - we want to flag how many total_amounts are too far away from the sum of their components\n",
    "# We will of course be slightly arbitrary with this, but this is necessary\n",
    "# We settle on classifying as worth investigation all records with a discrepancy greater than 5% of the total value\n",
    "# We ignore here ehail as it is blank\n",
    "# We are taking the absolute value in order to avoid negative errors, which we will correct for later ahead of the cash filter\n",
    "    \n",
    "    if total == None or total == \"\" or total == 'nan':\n",
    "        return 2\n",
    "    \n",
    "    try:\n",
    "        total = float(total)\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    # we use absolute values here because we do not want incorrectly stored negatives to impact our filter\n",
    "    difference = abs(total)\n",
    "    \n",
    "    for i in [cong_s,extra,fare,improv,tax,tip,toll,ehail]:\n",
    "        if i == None or i == \"\" or i == 'nan' or i == 'NaN':\n",
    "            i = 0\n",
    "        try:\n",
    "            # we also do not want to let incorrectly stored strings get in our way\n",
    "            i = abs(float(i))\n",
    "            difference = difference - i\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # we needed a percentage figure here, and the 5% from a standard significance test seemed appropriate\n",
    "    # this will allow us to allow some leeway in the values of the other variables while still flagging what would be\n",
    "    # an incorrect calculation\n",
    "    # A trip of 0 should theoretically be possible - enter and leave the taxi with no movement\n",
    "    \n",
    "    if abs(difference) <= abs(total)*0.05 and total >= 0 and total <= 1000:\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udfGREENreceipt_val = udf(GREENreceipt_val, IntegerType())\n",
    "\n",
    "print(\"Green receipt validation testing function stored as UDF\")\n",
    "\n",
    "def add_GREEN_total_amount_validation(dfrval):\n",
    "# Adds the extra validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfrval = dfrval.withColumn('total_amount_val', udfGREENreceipt_val('total_amount','congestion_surcharge','extra',\\\n",
    "    'fare_amount','improvement_surcharge','mta_tax','tip_amount','tolls_amount','ehail_fee'))\n",
    "\n",
    "    return dfrval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YELLOW receipt validation testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def YELLOWreceipt_val(total,cong_s,extra,fare,improv,tax,tip,toll):\n",
    "# Function to validate total_amount values\n",
    "# Function returns 0 if the figure is valid, 1 if it is invalid OR SUSPICIOUS, 2 if it is None\n",
    "# Essentially - we want to flag how many total_amounts are too far away from the sum of their components\n",
    "# We will of course be slightly arbitrary with this, but this is necessary\n",
    "# We settle on classifying as worth investigation all records with a discrepancy greater than 5% of the total value\n",
    "# We ignore here ehail as it is blank\n",
    "# We are taking the absolute value in order to \n",
    "\n",
    "    if total == None or total == \"\" or total == 'nan':\n",
    "        return 2\n",
    "    \n",
    "    try:\n",
    "        total = float(total)\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    # we use absolute values here because we do not want incorrectly stored negatives to impact our filter\n",
    "    difference = abs(total)\n",
    "    \n",
    "    for i in [cong_s,extra,fare,improv,tax,tip,toll]:\n",
    "        if i == None or i == \"\" or i == 'nan' or i == 'NaN':\n",
    "            i = 0\n",
    "        try:\n",
    "            # we also do not want to let incorrectly stored strings get in our way\n",
    "            i = abs(float(i))\n",
    "            difference = difference - i\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # we needed a percentage figure here, and the 5% from a standard significance test seemed appropriate\n",
    "    # this will allow us to allow some leeway in the values of the other variables while still flagging what would be\n",
    "    # an incorrect calculation\n",
    "    \n",
    "    if abs(difference) <= abs(total)*0.05 and total >= 0 and total <= 1000:\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udfYELLOWreceipt_val = udf(YELLOWreceipt_val, IntegerType())\n",
    "\n",
    "print(\"YELLOW receipt validation testing function stored as UDF\")\n",
    "\n",
    "def add_YELLOW_total_amount_validation(dfrval):\n",
    "# Adds the extra validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfrval = dfrval.withColumn('total_amount_val', udfYELLOWreceipt_val('total_amount','congestion_surcharge','extra',\\\n",
    "    'fare_amount','improvement_surcharge','mta_tax','tip_amount','tolls_amount'))\n",
    "\n",
    "    return dfrval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colors date testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def COLOR_datetime_validation(pudt,dodt,tdist):\n",
    "# Function to validate datetime values\n",
    "# Function returns 0 if datetimes together are valid, 1 if they are invalid, 2 if they do not exist\n",
    "# We appear to need both for all queries with distance, so validation flags will be set together\n",
    "# Please see the following comments for a more detailed explanation\n",
    "    \n",
    "    if pudt == \"\" or pudt == None or pudt =='nan' or dodt == \"\" or dodt == None or dodt =='nan':\n",
    "        return 2\n",
    "    \n",
    "    # Checking overall length of date\n",
    "    if len(pudt) == 19 and len(dodt) == 19:   \n",
    "        # Convert to datetime to see if input follows the correct format\n",
    "        try:\n",
    "            pudt_DT = dt.strptime(pudt, '%Y-%m-%d %H:%M:%S')\n",
    "            dodt_DT = dt.strptime(dodt, '%Y-%m-%d %H:%M:%S')\n",
    "        except:\n",
    "            return 1\n",
    "    \n",
    "        timeDelta = (dodt_DT - pudt_DT).total_seconds()\n",
    "        # if the time difference is less than or equal to 0, we can return a 1\n",
    "        if timeDelta <= 0:\n",
    "            return 1\n",
    "\n",
    "        # Otherwise, we can calculate our speed (in miles per second)\n",
    "\n",
    "        # Try to validate speed (if distance exists and is itself valid)\n",
    "        if tdist != None and tdist != \"\" and tdist != \"nan\":    \n",
    "            try:\n",
    "                tdist = float(tdist)\n",
    "                tdist = abs(tdist)\n",
    "                # Use only verifiable distances for now\n",
    "                if tdist > 0 and tdist <= 300:\n",
    "                    \n",
    "                    speed = tdist/timeDelta*3600\n",
    "                    # This will now be miles per hour\n",
    "                    # Taxis could use freeway trips - 75mph is reasonable in come cases\n",
    "                    if speed <= 75:\n",
    "                        return 0\n",
    "                    else:\n",
    "                        return 1\n",
    "                else:\n",
    "                    return 0\n",
    "\n",
    "            except:\n",
    "            # We return 0 if nothing else is wrong but tdist is not verifiable or speed is incalculable\n",
    "                return 0\n",
    "\n",
    "        # We return 0 if nothing else is wrong but tdist is not verifiable or speed is incalculable\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    # If the date's length is invalid, return 1\n",
    "    return 1\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udfCOLORdatetime_validation = udf(COLOR_datetime_validation, IntegerType())\n",
    "\n",
    "print(\"Colors date testing function stored as UDF\")\n",
    "\n",
    "def add_GREEN_datetime_validation(dfDTval):\n",
    "# Adds the datetime validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfDTval = dfDTval.withColumn('lpep_pickup_datetime_val', udfCOLORdatetime_validation('lpep_pickup_datetime',\\\n",
    "                                                                                        'lpep_dropoff_datetime',\\\n",
    "                                                                                        'trip_distance'))\n",
    "    dfDTval = dfDTval.withColumn('lpep_dropoff_datetime_val', col('lpep_pickup_datetime_val'))\n",
    "\n",
    "    return dfDTval\n",
    "\n",
    "def add_YELLOW_datetime_validation(dfDTval):\n",
    "# Adds the datetime validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfDTval = dfDTval.withColumn('tpep_pickup_datetime_val', udfCOLORdatetime_validation('tpep_pickup_datetime',\\\n",
    "                                                                                        'tpep_dropoff_datetime',\\\n",
    "                                                                                        'trip_distance'))\n",
    "    dfDTval = dfDTval.withColumn('tpep_dropoff_datetime_val', col('tpep_pickup_datetime_val'))\n",
    "\n",
    "    return dfDTval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green and Yellow row testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "# We want to check if each row can be used in our query - the full row, not just a subset\n",
    "# We here test that all of our key variables are valid\n",
    "\n",
    "def COLOR_valid_row(pickup_datetime_val,\\\n",
    "                     dropoff_datetime_val,\\\n",
    "                     pulocationid_val,\\\n",
    "                     dolocationid_val,\\\n",
    "                     trip_distance_val,\\\n",
    "                     tip_amount_val,\\\n",
    "                     total_amount_val,\\\n",
    "                     payment_type_val):\n",
    "    \n",
    "    if pickup_datetime_val == 1\\\n",
    "    or dropoff_datetime_val== 1\\\n",
    "    or pulocationid_val == 1\\\n",
    "    or dolocationid_val == 1\\\n",
    "    or payment_type_val == 1\\\n",
    "    or trip_distance_val == 1\\\n",
    "    or tip_amount_val==1\\\n",
    "    or total_amount_val == 1:\n",
    "        \n",
    "        return 1\n",
    "    \n",
    "    elif pickup_datetime_val == 2\\\n",
    "    or dropoff_datetime_val == 2\\\n",
    "    or pulocationid_val == 2\\\n",
    "    or dolocationid_val == 2\\\n",
    "    or payment_type_val == 2\\\n",
    "    or trip_distance_val == 2\\\n",
    "    or tip_amount_val == 2\\\n",
    "    or total_amount_val == 2:\n",
    "        \n",
    "        return 2\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udfCOLOR_valid_row = udf(COLOR_valid_row, IntegerType())\n",
    "\n",
    "print(\"Green and Yellow row testing function stored as UDF\")\n",
    "\n",
    "def add_GREEN_row_validation(dfCOLORval):\n",
    "# Adds the row validation as a new dataframe column, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "\n",
    "    dfCOLORval = dfCOLORval.withColumn('row_val', udfCOLOR_valid_row('lpep_pickup_datetime_val',\\\n",
    "                                                                 'lpep_dropoff_datetime_val',\\\n",
    "                                                                 'pulocationid_val',\\\n",
    "                                                                 'dolocationid_val',\\\n",
    "                                                                 'trip_distance_val',\\\n",
    "                                                                 'tip_amount_val',\\\n",
    "                                                                 'total_amount_val',\\\n",
    "                                                                 'payment_type_val'))\n",
    "\n",
    "    return dfCOLORval\n",
    "\n",
    "def add_YELLOW_row_validation(dfCOLORval):\n",
    "# Adds the row validation as a new dataframe column, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "\n",
    "    dfCOLORval = dfCOLORval.withColumn('row_val', udfCOLOR_valid_row('tpep_pickup_datetime_val',\\\n",
    "                                                                 'tpep_dropoff_datetime_val',\\\n",
    "                                                                 'pulocationid_val',\\\n",
    "                                                                 'dolocationid_val',\\\n",
    "                                                                 'trip_distance_val',\\\n",
    "                                                                 'tip_amount_val',\\\n",
    "                                                                 'total_amount_val',\\\n",
    "                                                                 'payment_type_val'))\n",
    "\n",
    "    return dfCOLORval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again - what do we really need for our queries at a bare minimum - what can form the basis of our clean dataset?\n",
    "For the queries:  \n",
    "1. The monthly total number of trips: nothing\n",
    "2. Monthly total number of trips in Manhattan and Brooklyn: pu&dolocationid\n",
    "3. The monthly total receipts - exclude tips, but include fares, surcharges, taxes and tolls: total_amount & tip_amount\n",
    "4. Average trip receipt: same - total_amount & tip_amount\n",
    "5. Average cost per in-progress minute: pickup & dropoff datetime, total_amount & tip_amount\n",
    "6. Average tip per trip: tip_amount, payment type\n",
    "7. Median monthly average trip speed per borough: trip_distance & pickup & dropoff datetime\n",
    "8. How long does it take to go to the airport?L pu&dolocationid & pickup & dropoff datetime\n",
    "\n",
    "If a record has all of these values and meets our validity constraints, we can send it immediately to the clean data.  \n",
    "\n",
    "For the cost related queries, it is specified that Cost should exclude tips, but include fares, surcharges, taxes and tolls. The total_amount, as specified by the dictionary, less the tip amount, will satisfy this condition, and so we will prioritise taking that variable in a complete form, and potentially repairing it with the others if it is missing. This has the added benefit that the total amount is already the most likely to be accurate, as it would need to be correct for customer billing purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning the GREEN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values and validity constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vendorid: value is an int either 1 or 2  \n",
    "passenger_count: an int (above 10 is suspicious) \n",
    "ratecodeid: an int between 1 and 6  \n",
    "store_and_fwd_flag: string \"Y\" or \"N\"  \n",
    "payment_type: an int between 1 and 6 - if the payment_type is 2 while tip != 0, this is likely incorrect (we will trust the tip here)  \n",
    "fare_amount: a float  \n",
    "extra: a float either 0, 0.50, 1.00 \n",
    "mta_tax: a float either 0 or 0.50  \n",
    "improvement_surcharge: a float 0.30 ONLY 2015 ONWARDS  \n",
    "tolls_amount: a float  \n",
    "trip_type: int 1 or 2  \n",
    "ehail_fee: does not appear in data dictionary!!! appears to be blank  \n",
    "congestion_surcharge: does not appear in data dictionary!!! seems to need 2.75 or 0\n",
    "\n",
    "lpep_pickup_datetime: 2020-06-24 04:38:00  - this and the dropoff datetime should not be 0, or have a negative delta\n",
    "lpep_dropoff_datetime: 2020-06-24 04:38:00  \n",
    "trip_distance: a float, >0 a trip with a distance of 0 is not, by definition, a trip  \n",
    "pulocationid: int 1 to 263  \n",
    "dolocationid: int 1 to 263 but can be imputed from the ratecodeid in very specific instances\n",
    "tip_amount: a float, >0 we don't want to add further validity constraints here because you never know when a youtuber will get into a new york taxi looking for content. We have flagged it as worthy of investigating if it is >1000  \n",
    "total_amount: a float >0 and must approximately equal the sum of the other values (a >1% difference between the two figires will be flagged for investigation) - can be imputed from those values if missing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ehail_fee does not appear in the data dictionary, but it appears to be blank in all cases. The congestion_surcharge also does not appear in the dictionary, but when it takes a value in the data it appears to be 2.75 in Green and 2.5 in Yellow. We propose to add conditions covering these two variables regardless - they appear unimportant, but could play a role in the imputation of total_amount if necessary, and excessive/obviously incorrect values should be avoided.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will class a record (not a value) as being dirty if it is unusable for our purposes in 2.4.\n",
    "\n",
    "If the record itself is dirty, we will attempt to use the remaining variables to repair it, but otherwise we will only track errors caught by our constraints, and not repair them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and splitting off dirty records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/output/cleaned Green\n",
      "folder already exists at /home/epb123/output/dirty Green\n"
     ]
    }
   ],
   "source": [
    "clean_GREEN_directory = create_folder(\"cleaned Green\", save_directory)\n",
    "with open(v_direc + \"clean_GREEN_directory\",'wb') as cleanedGreendirec:\n",
    "    pickle.dump(clean_GREEN_directory,cleanedGreendirec)\n",
    "    \n",
    "dirty_GREEN_directory = create_folder(\"dirty Green\", save_directory)\n",
    "with open(v_direc + \"dirty_GREEN_directory\",'wb') as dirtyGreendirec:\n",
    "    pickle.dump(dirty_GREEN_directory,dirtyGreendirec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def green_cleanup():\n",
    "# takes no arguments - splits Green into clean and dirty data\n",
    "# returns a pyspark dataframe with dirty data\n",
    "\n",
    "    dropped_duplicates_count = 0\n",
    "    clean_count = 0\n",
    "    row_count = 0\n",
    "    \n",
    "    dirtyLst = []\n",
    "    missingLst = []\n",
    "            \n",
    "    for i in green_integratedPaths:\n",
    "        dfi = create_df(i)\n",
    "        dfi_row_count = dfi.count()\n",
    "        row_count = row_count + dfi_row_count\n",
    "\n",
    "        # Adding validation columns\n",
    "        dfi = add_location_validation(dfi) \n",
    "        dfi = add_GREEN_datetime_validation(dfi)\n",
    "        dfi = add_vendorid_validation(dfi)\n",
    "        dfi = add_improvement_surcharge_validation(dfi,i)\n",
    "        dfi = add_mta_tax_validation(dfi)\n",
    "        dfi = add_extra_validation(dfi)\n",
    "        dfi = add_fare_amount_validation(dfi)\n",
    "        dfi = add_tip_amount_validation(dfi) \n",
    "        dfi = add_tolls_amount_validation(dfi) \n",
    "        dfi = add_GREEN_total_amount_validation(dfi)\n",
    "        dfi = add_store_and_forward_flag_validation(dfi)\n",
    "        dfi = add_trip_distance_validation(dfi)\n",
    "        dfi = add_passenger_count_validation(dfi)\n",
    "        dfi = add_trip_type_validation(dfi)\n",
    "        dfi = add_payment_type_validation(dfi)\n",
    "        dfi = add_ratecodeID_validation(dfi)\n",
    "        dfi = add_ehail_validation(dfi)\n",
    "        dfi = add_GREEN_congestion_surcharge_validation(dfi)\n",
    "        dfi = add_GREEN_row_validation(dfi)\n",
    "\n",
    "        # Store records which are not dirty (this does not consider variables useless for 2.4 completion)\n",
    "        # We also want to keep records which we can use - namely those with a pair of locationID values\n",
    "        dfiClean = dfi.filter((dfi.row_val == 0))\n",
    "        \n",
    "        dfiClean = dfiClean.drop(*['pulocationid_val',\\\n",
    "                                   'dolocationid_val',\\\n",
    "                                   'lpep_pickup_datetime_val',\\\n",
    "                                   'lpep_dropoff_datetime_val',\\\n",
    "                                   'vendorid_val',\\\n",
    "                                   'improvement_surcharge_val',\\\n",
    "                                   'mta_tax_val',\\\n",
    "                                   'extra_val',\\\n",
    "                                   'fare_amount_val',\\\n",
    "                                   'tip_amount_val',\\\n",
    "                                   'tolls_amount_val',\\\n",
    "                                   'total_amount_val',\\\n",
    "                                   'store_and_fwd_flag_val',\\\n",
    "                                   'trip_distance_val',\\\n",
    "                                   'passenger_count_val',\\\n",
    "                                   'trip_type_val',\\\n",
    "                                   'payment_type_val',\\\n",
    "                                   'ratecodeid_val',\\\n",
    "                                   'ehail_fee_val',\\\n",
    "                                   'congestion_surcharge_val',\\\n",
    "                                   'row_val'])\n",
    "\n",
    "        # We can drop duplicates in the clean dataset here - they will all have pickup and dropoff times or locations\n",
    "        # The times are stored to the millisecond and PySpark's drop_duplicates compares all columns\n",
    "        # If two records have pickup and dropoff times identical to the millisecond, we can safely drop them\n",
    "        # If two records have both location ids identical and identical dispatch numbers, we can also safely drop them\n",
    "        duplicates_result = drop_dupes(dfiClean)\n",
    "        dfiClean = duplicates_result[0]\n",
    "\n",
    "        dropped_duplicates_count = dropped_duplicates_count + duplicates_result[1]\n",
    "\n",
    "        dfiClean_row_count = duplicates_result[2]\n",
    "        clean_count = clean_count + dfiClean_row_count\n",
    "\n",
    "        # Save the cleaned dataframes for repairing later\n",
    "        save_df(dfiClean,ntpath.basename(i) + 'clean', clean_GREEN_directory)\n",
    "        \n",
    "        dirtyInI = dfi.filter((dfi.row_val == 1) | (dfi.row_val == 2))\\\n",
    "            .withColumn(\"filename\", lit(ntpath.basename(i)))\n",
    "        \n",
    "        # We then add the dirty records to a spark dataframe, with an additional column specifying file of origin\n",
    "        if i == green_integratedPaths[0]:\n",
    "            dfiDirty = dirtyInI\n",
    "\n",
    "        else:\n",
    "            dfiDirty = dfiDirty.unionByName(dirtyInI)\n",
    "        \n",
    "        dirtyInI = dirtyInI.withColumn(\"filename\",col('filename').cast(\"string\"))\n",
    "        save_df(dirtyInI,ntpath.basename(i) + 'dirty', dirty_GREEN_directory)\n",
    "        \n",
    "        print(str(dfiClean_row_count) + ' clean records preserved out of ' + str(dfi_row_count) + \\\n",
    "              ' original records - ' + str(dfiClean_row_count/dfi_row_count*100)[0:4] + \"%\")\n",
    "        print(str(dirtyInI.filter(dirtyInI.row_val == 1).count()) + \" invalid records have been found\")\n",
    "        print(str(dirtyInI.filter(dirtyInI.row_val == 2).count()) + \" records with missing data have been found\")\n",
    "        \n",
    "        # We can use Pandas to keep track of what's going on - this really slows down the code, but statistics are nice.\n",
    "        # Note: if you comment this out to make the code run faster, do not forget to delete the return\n",
    "        \n",
    "        b = dfi.filter(col('lpep_dropoff_datetime_val')==2).count()\n",
    "        c = dfi.filter(col('lpep_pickup_datetime_val')==2).count()\n",
    "        d = dfi.filter(col('dolocationid_val')==2).count()\n",
    "        e = dfi.filter(col('pulocationid_val')==2).count()\n",
    "        f = dfi.filter(col('vendorid_val')==2).count()\n",
    "        g = dfi.filter(col('ratecodeid_val')==2).count()\n",
    "        h = dfi.filter(col('payment_type_val')==2).count()\n",
    "        j = dfi.filter(col('store_and_fwd_flag_val')==2).count()\n",
    "        k = dfi.filter(col('trip_distance_val')==2).count()\n",
    "        l = dfi.filter(col('passenger_count_val')==2).count()\n",
    "        m = dfi.filter(col('tip_amount_val')==2).count()\n",
    "        n = dfi.filter(col('fare_amount_val')==2).count()\n",
    "        o = dfi.filter(col('total_amount_val')==2).count()\n",
    "        p = dfi.filter(col('improvement_surcharge_val')==2).count()\n",
    "        q = dfi.filter(col('extra_val')==2).count()\n",
    "        r = dfi.count()\n",
    "        s = dfi.filter(col('mta_tax_val')==2).count()\n",
    "        t = dfi.filter(col('trip_type_val')==2).count()\n",
    "        \n",
    "        u = dfi.filter(col('tolls_amount_val')==2).count()\n",
    "        v = dfi.filter(col('ehail_fee_val')==2).count()\n",
    "        w = dfi.filter(col('congestion_surcharge_val')==2).count()\n",
    "        x = dfi.filter(col('row_val')==2).count()\n",
    "        \n",
    "        missingLst.append([i,r,b,c,d,e,f,g,h,j,k,l,m,n,o,p,q,s,t,u,v,w,x])\n",
    "        \n",
    "        b1 = dfi.filter(col('lpep_dropoff_datetime_val')==1).count()\n",
    "        c1 = dfi.filter(col('lpep_pickup_datetime_val')==1).count()\n",
    "        d1 = dfi.filter(col('dolocationid_val')==1).count()\n",
    "        e1 = dfi.filter(col('pulocationid_val')==1).count()\n",
    "        f1 = dfi.filter(col('vendorid_val')==1).count()\n",
    "        g1 = dfi.filter(col('ratecodeid_val')==1).count()\n",
    "        h1 = dfi.filter(col('payment_type_val')==1).count()\n",
    "        j1 = dfi.filter(col('store_and_fwd_flag_val')==1).count()\n",
    "        k1 = dfi.filter(col('trip_distance_val')==1).count()\n",
    "        l1 = dfi.filter(col('passenger_count_val')==1).count()\n",
    "        m1 = dfi.filter(col('tip_amount_val')==1).count()\n",
    "        n1 = dfi.filter(col('fare_amount_val')==1).count()\n",
    "        o1 = dfi.filter(col('total_amount_val')==1).count()\n",
    "        p1 = dfi.filter(col('improvement_surcharge_val')==1).count()\n",
    "        q1 = dfi.filter(col('extra_val')==1).count()\n",
    "        r1 = dfi.count()\n",
    "        s1 = dfi.filter(col('mta_tax_val')==1).count()\n",
    "        t1 = dfi.filter(col('trip_type_val')==1).count()\n",
    "        \n",
    "        u1 = dfi.filter(col('tolls_amount_val')==1).count()\n",
    "        v1 = dfi.filter(col('ehail_fee_val')==1).count()\n",
    "        w1 = dfi.filter(col('congestion_surcharge_val')==1).count()\n",
    "        x1 = dfi.filter(col('row_val')==1).count()\n",
    "        \n",
    "        dirtyLst.append([i,r1,b1,c1,d1,e1,f1,g1,h1,j1,k1,l1,m1,n1,o1,p1,q1,s1,t1,u1,v1,w1,x1])\n",
    "\n",
    "    PdfMissing=pd.DataFrame(missingLst,columns=['file','number of lines','lpep_dropoff_datetime_val','lpep_pickup_datetime_val',\\\n",
    "                                 'dolocationid_val','pulocationid_val','vendorid_val','ratecodeid_val',\\\n",
    "                                 'payment_type_val','store_and_fwd_flag_val','trip_distance_val',\\\n",
    "                                 'passenger_count_val', 'tip_amount_val','fare_amount_val','total_amount_val',\\\n",
    "                                 'improvement_surcharge_val','extra_val','mta_tax_val','trip_type_val','tolls_amount_val',\\\n",
    "                                 'ehail_fee_val','congestion_surcharge_val','row_val'])\n",
    "        \n",
    "    #Only the columns with at least one value different from zero are kept.\n",
    "    PdfMissing = PdfMissing.loc[:, (PdfMissing != 0).any(axis=0)] \n",
    "\n",
    "    PdfDirty=pd.DataFrame(dirtyLst,columns=['file','number of lines','lpep_dropoff_datetime_val','lpep_pickup_datetime_val',\\\n",
    "                             'dolocationid_val','pulocationid_val','vendorid_val','ratecodeid_val',\\\n",
    "                             'payment_type_val','store_and_fwd_flag_val','trip_distance_val',\\\n",
    "                             'passenger_count_val', 'tip_amount_val','fare_amount_val','total_amount_val',\\\n",
    "                             'improvement_surcharge_val','extra_val','mta_tax_val','trip_type_val','tolls_amount_val',\\\n",
    "                             'ehail_fee_val','congestion_surcharge_val','row_val'])\n",
    "\n",
    "    #Only the columns with at least one value different from zero are kept.\n",
    "    PdfDirty = PdfDirty.loc[:, (PdfDirty != 0).any(axis=0)] \n",
    "        \n",
    "    print(\"Out of \" + str(row_count) + \" original records, \" + str(clean_count) + \" records were already clean enough to use in queries.\")\n",
    "    print(str(dropped_duplicates_count) + \" records were dropped as duplicates.\")\n",
    "    print(str(row_count - clean_count - dropped_duplicates_count) \\\n",
    "          + \" dirty records remain for resolution.\")\n",
    "    \n",
    "    return dfiDirty, PdfMissing, PdfDirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened /home/epb123/output/integrated Green/green_tripdata_2013-08.csv\n",
      "0 duplicate entries removed\n",
      "13 clean records preserved out of 15 original records - 86.6%\n",
      "2 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2013-09.csv\n",
      "0 duplicate entries removed\n",
      "89 clean records preserved out of 98 original records - 90.8%\n",
      "9 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2013-10.csv\n",
      "0 duplicate entries removed\n",
      "321 clean records preserved out of 338 original records - 94.9%\n",
      "17 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2013-11.csv\n",
      "0 duplicate entries removed\n",
      "729 clean records preserved out of 756 original records - 96.4%\n",
      "27 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2013-12.csv\n",
      "0 duplicate entries removed\n",
      "1160 clean records preserved out of 1189 original records - 97.5%\n",
      "29 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2014-01.csv\n",
      "0 duplicate entries removed\n",
      "1527 clean records preserved out of 1586 original records - 96.2%\n",
      "59 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2014-02.csv\n",
      "0 duplicate entries removed\n",
      "1934 clean records preserved out of 1987 original records - 97.3%\n",
      "53 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2014-03.csv\n",
      "0 duplicate entries removed\n",
      "2489 clean records preserved out of 2568 original records - 96.9%\n",
      "79 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2014-04.csv\n",
      "0 duplicate entries removed\n",
      "2528 clean records preserved out of 2596 original records - 97.3%\n",
      "68 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2014-05.csv\n",
      "0 duplicate entries removed\n",
      "2754 clean records preserved out of 2822 original records - 97.5%\n",
      "68 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2014-06.csv\n",
      "0 duplicate entries removed\n",
      "2582 clean records preserved out of 2655 original records - 97.2%\n",
      "73 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2014-07.csv\n",
      "0 duplicate entries removed\n",
      "2462 clean records preserved out of 2528 original records - 97.3%\n",
      "66 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2014-08.csv\n",
      "0 duplicate entries removed\n",
      "2610 clean records preserved out of 2670 original records - 97.7%\n",
      "60 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2014-09.csv\n",
      "0 duplicate entries removed\n",
      "2642 clean records preserved out of 2702 original records - 97.7%\n",
      "60 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2014-10.csv\n",
      "0 duplicate entries removed\n",
      "2897 clean records preserved out of 2959 original records - 97.9%\n",
      "62 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2014-11.csv\n",
      "0 duplicate entries removed\n",
      "2998 clean records preserved out of 3070 original records - 97.6%\n",
      "72 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2014-12.csv\n",
      "0 duplicate entries removed\n",
      "3192 clean records preserved out of 3262 original records - 97.8%\n",
      "70 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2015-01.csv\n",
      "0 duplicate entries removed\n",
      "2934 clean records preserved out of 2992 original records - 98.0%\n",
      "58 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2015-02.csv\n",
      "0 duplicate entries removed\n",
      "3052 clean records preserved out of 3124 original records - 97.6%\n",
      "72 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2015-03.csv\n",
      "0 duplicate entries removed\n",
      "3330 clean records preserved out of 3418 original records - 97.4%\n",
      "88 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2015-04.csv\n",
      "0 duplicate entries removed\n",
      "3211 clean records preserved out of 3301 original records - 97.2%\n",
      "90 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2015-05.csv\n",
      "0 duplicate entries removed\n",
      "3458 clean records preserved out of 3546 original records - 97.5%\n",
      "88 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2015-06.csv\n",
      "0 duplicate entries removed\n",
      "3174 clean records preserved out of 3250 original records - 97.6%\n",
      "76 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2015-07.csv\n",
      "0 duplicate entries removed\n",
      "2982 clean records preserved out of 3058 original records - 97.5%\n",
      "76 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2015-08.csv\n",
      "0 duplicate entries removed\n",
      "2961 clean records preserved out of 3039 original records - 97.4%\n",
      "78 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2015-09.csv\n",
      "0 duplicate entries removed\n",
      "2889 clean records preserved out of 2968 original records - 97.3%\n",
      "79 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2015-10.csv\n",
      "0 duplicate entries removed\n",
      "3173 clean records preserved out of 3234 original records - 98.1%\n",
      "61 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2015-11.csv\n",
      "0 duplicate entries removed\n",
      "2968 clean records preserved out of 3037 original records - 97.7%\n",
      "69 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2015-12.csv\n",
      "0 duplicate entries removed\n",
      "3116 clean records preserved out of 3190 original records - 97.6%\n",
      "74 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2016-01.csv\n",
      "0 duplicate entries removed\n",
      "2798 clean records preserved out of 2870 original records - 97.4%\n",
      "72 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2016-02.csv\n",
      "0 duplicate entries removed\n",
      "2923 clean records preserved out of 2997 original records - 97.5%\n",
      "74 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2016-03.csv\n",
      "0 duplicate entries removed\n",
      "3065 clean records preserved out of 3128 original records - 97.9%\n",
      "63 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2016-04.csv\n",
      "0 duplicate entries removed\n",
      "2985 clean records preserved out of 3063 original records - 97.4%\n",
      "78 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2016-05.csv\n",
      "0 duplicate entries removed\n",
      "2989 clean records preserved out of 3049 original records - 98.0%\n",
      "60 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2016-06.csv\n",
      "0 duplicate entries removed\n",
      "2692 clean records preserved out of 2790 original records - 96.4%\n",
      "98 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2016-07.csv\n",
      "0 duplicate entries removed\n",
      "2555 clean records preserved out of 2653 original records - 96.3%\n",
      "98 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2016-08.csv\n",
      "0 duplicate entries removed\n",
      "2395 clean records preserved out of 2483 original records - 96.4%\n",
      "88 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2016-09.csv\n",
      "0 duplicate entries removed\n",
      "2216 clean records preserved out of 2312 original records - 95.8%\n",
      "96 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2016-10.csv\n",
      "0 duplicate entries removed\n",
      "2384 clean records preserved out of 2491 original records - 95.7%\n",
      "107 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2016-11.csv\n",
      "0 duplicate entries removed\n",
      "2215 clean records preserved out of 2285 original records - 96.9%\n",
      "70 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2016-12.csv\n",
      "0 duplicate entries removed\n",
      "2350 clean records preserved out of 2438 original records - 96.3%\n",
      "88 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2017-01.csv\n",
      "0 duplicate entries removed\n",
      "2044 clean records preserved out of 2132 original records - 95.8%\n",
      "88 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2017-02.csv\n",
      "0 duplicate entries removed\n",
      "1965 clean records preserved out of 2035 original records - 96.5%\n",
      "70 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2017-03.csv\n",
      "0 duplicate entries removed\n",
      "2221 clean records preserved out of 2303 original records - 96.4%\n",
      "82 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2017-04.csv\n",
      "0 duplicate entries removed\n",
      "2077 clean records preserved out of 2154 original records - 96.4%\n",
      "77 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2017-05.csv\n",
      "0 duplicate entries removed\n",
      "2036 clean records preserved out of 2110 original records - 96.4%\n",
      "74 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2017-06.csv\n",
      "0 duplicate entries removed\n",
      "1867 clean records preserved out of 1943 original records - 96.0%\n",
      "76 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2017-07.csv\n",
      "0 duplicate entries removed\n",
      "1725 clean records preserved out of 1820 original records - 94.7%\n",
      "95 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2017-08.csv\n",
      "0 duplicate entries removed\n",
      "1636 clean records preserved out of 1726 original records - 94.7%\n",
      "90 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2017-09.csv\n",
      "0 duplicate entries removed\n",
      "1657 clean records preserved out of 1755 original records - 94.4%\n",
      "98 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2017-10.csv\n",
      "0 duplicate entries removed\n",
      "1792 clean records preserved out of 1842 original records - 97.2%\n",
      "50 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2017-11.csv\n",
      "0 duplicate entries removed\n",
      "1687 clean records preserved out of 1739 original records - 97.0%\n",
      "52 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2017-12.csv\n",
      "0 duplicate entries removed\n",
      "1745 clean records preserved out of 1803 original records - 96.7%\n",
      "58 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2018-01.csv\n",
      "0 duplicate entries removed\n",
      "1529 clean records preserved out of 1582 original records - 96.6%\n",
      "53 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2018-02.csv\n",
      "0 duplicate entries removed\n",
      "1481 clean records preserved out of 1535 original records - 96.4%\n",
      "54 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2018-03.csv\n",
      "0 duplicate entries removed\n",
      "1626 clean records preserved out of 1666 original records - 97.5%\n",
      "40 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2018-04.csv\n",
      "0 duplicate entries removed\n",
      "1546 clean records preserved out of 1594 original records - 96.9%\n",
      "48 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2018-05.csv\n",
      "0 duplicate entries removed\n",
      "1548 clean records preserved out of 1588 original records - 97.4%\n",
      "40 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2018-06.csv\n",
      "0 duplicate entries removed\n",
      "1421 clean records preserved out of 1476 original records - 96.2%\n",
      "55 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2018-07.csv\n",
      "0 duplicate entries removed\n",
      "1333 clean records preserved out of 1368 original records - 97.4%\n",
      "35 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2018-08.csv\n",
      "0 duplicate entries removed\n",
      "1298 clean records preserved out of 1330 original records - 97.5%\n",
      "32 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2018-09.csv\n",
      "0 duplicate entries removed\n",
      "1295 clean records preserved out of 1331 original records - 97.2%\n",
      "36 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2018-10.csv\n",
      "0 duplicate entries removed\n",
      "1373 clean records preserved out of 1419 original records - 96.7%\n",
      "46 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2018-11.csv\n",
      "0 duplicate entries removed\n",
      "1264 clean records preserved out of 1311 original records - 96.4%\n",
      "47 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2018-12.csv\n",
      "0 duplicate entries removed\n",
      "1333 clean records preserved out of 1370 original records - 97.2%\n",
      "37 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2019-01.csv\n",
      "0 duplicate entries removed\n",
      "1228 clean records preserved out of 1258 original records - 97.6%\n",
      "30 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2019-02.csv\n",
      "0 duplicate entries removed\n",
      "1085 clean records preserved out of 1146 original records - 94.6%\n",
      "61 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2019-03.csv\n",
      "0 duplicate entries removed\n",
      "1127 clean records preserved out of 1196 original records - 94.2%\n",
      "69 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2019-04.csv\n",
      "0 duplicate entries removed\n",
      "964 clean records preserved out of 1023 original records - 94.2%\n",
      "59 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2019-05.csv\n",
      "0 duplicate entries removed\n",
      "946 clean records preserved out of 1004 original records - 94.2%\n",
      "58 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2019-06.csv\n",
      "0 duplicate entries removed\n",
      "883 clean records preserved out of 935 original records - 94.4%\n",
      "52 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2020-01.csv\n",
      "0 duplicate entries removed\n",
      "608 clean records preserved out of 895 original records - 67.9%\n",
      "66 invalid records have been found\n",
      "221 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2020-02.csv\n",
      "0 duplicate entries removed\n",
      "595 clean records preserved out of 797 original records - 74.6%\n",
      "44 invalid records have been found\n",
      "158 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2020-04.csv\n",
      "0 duplicate entries removed\n",
      "41 clean records preserved out of 71 original records - 57.7%\n",
      "10 invalid records have been found\n",
      "20 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2020-05.csv\n",
      "0 duplicate entries removed\n",
      "50 clean records preserved out of 114 original records - 43.8%\n",
      "18 invalid records have been found\n",
      "46 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Green/green_tripdata_2020-06.csv\n",
      "0 duplicate entries removed\n",
      "69 clean records preserved out of 126 original records - 54.7%\n",
      "12 invalid records have been found\n",
      "45 records with missing data have been found\n",
      "Out of 154014 original records, 148837 records were already clean enough to use in queries.\n",
      "0 records were dropped as duplicates.\n",
      "5177 dirty records remain for resolution.\n"
     ]
    }
   ],
   "source": [
    "green_cleanup_return = green_cleanup()\n",
    "dirtyGREEN = green_cleanup_return[0]\n",
    "missingGreen = green_cleanup_return[1]\n",
    "dirtyDetailedGreen = green_cleanup_return[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/output/cleaned Green/all CSV\n",
      "folder already exists at /home/epb123/output/dirty Green/all CSV\n"
     ]
    }
   ],
   "source": [
    "# Clean the PySpark outputs from before\n",
    "GREEN_cleanCSV_folderPath = cleanup(clean_GREEN_directory)\n",
    "with open(v_direc + \"GREEN_cleanCSV_folderPath\",'wb') as cleanedGREENCSVdirec:\n",
    "    pickle.dump(GREEN_cleanCSV_folderPath,cleanedGREENCSVdirec)\n",
    "\n",
    "GREEN_dirtyCSV_folderPath = cleanup(dirty_GREEN_directory)\n",
    "with open(v_direc + \"GREEN_dirtyCSV_folderPath\",'wb') as dirtyGREENCSVdirec:\n",
    "    pickle.dump(GREEN_dirtyCSV_folderPath,dirtyGREENCSVdirec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on filtering for records with all of the information needed for the queries, we appear to already be at 5,177 dirty or missing of 154,014 original records. Let us examine them and see what can be repaired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>number of lines</th>\n",
       "      <th>lpep_dropoff_datetime_val</th>\n",
       "      <th>lpep_pickup_datetime_val</th>\n",
       "      <th>dolocationid_val</th>\n",
       "      <th>pulocationid_val</th>\n",
       "      <th>ratecodeid_val</th>\n",
       "      <th>payment_type_val</th>\n",
       "      <th>trip_distance_val</th>\n",
       "      <th>passenger_count_val</th>\n",
       "      <th>tip_amount_val</th>\n",
       "      <th>fare_amount_val</th>\n",
       "      <th>total_amount_val</th>\n",
       "      <th>improvement_surcharge_val</th>\n",
       "      <th>extra_val</th>\n",
       "      <th>mta_tax_val</th>\n",
       "      <th>tolls_amount_val</th>\n",
       "      <th>congestion_surcharge_val</th>\n",
       "      <th>row_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>338</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>756</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1189</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1586</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1987</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2568</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2596</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2822</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2655</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2528</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2670</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2702</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2959</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3070</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3262</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3124</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3418</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3301</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3546</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3250</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3058</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3039</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2968</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3234</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3037</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3190</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2870</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1943</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1820</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1726</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1755</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1842</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1739</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1803</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1582</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1535</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1666</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1594</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1588</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1476</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1368</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1330</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1331</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1419</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1311</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1258</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1146</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1196</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1023</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1004</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>935</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>189</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>797</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 file  number of lines  \\\n",
       "0   /home/epb123/output/integrated Green/green_tri...               15   \n",
       "1   /home/epb123/output/integrated Green/green_tri...               98   \n",
       "2   /home/epb123/output/integrated Green/green_tri...              338   \n",
       "3   /home/epb123/output/integrated Green/green_tri...              756   \n",
       "4   /home/epb123/output/integrated Green/green_tri...             1189   \n",
       "5   /home/epb123/output/integrated Green/green_tri...             1586   \n",
       "6   /home/epb123/output/integrated Green/green_tri...             1987   \n",
       "7   /home/epb123/output/integrated Green/green_tri...             2568   \n",
       "8   /home/epb123/output/integrated Green/green_tri...             2596   \n",
       "9   /home/epb123/output/integrated Green/green_tri...             2822   \n",
       "10  /home/epb123/output/integrated Green/green_tri...             2655   \n",
       "11  /home/epb123/output/integrated Green/green_tri...             2528   \n",
       "12  /home/epb123/output/integrated Green/green_tri...             2670   \n",
       "13  /home/epb123/output/integrated Green/green_tri...             2702   \n",
       "14  /home/epb123/output/integrated Green/green_tri...             2959   \n",
       "15  /home/epb123/output/integrated Green/green_tri...             3070   \n",
       "16  /home/epb123/output/integrated Green/green_tri...             3262   \n",
       "17  /home/epb123/output/integrated Green/green_tri...             2992   \n",
       "18  /home/epb123/output/integrated Green/green_tri...             3124   \n",
       "19  /home/epb123/output/integrated Green/green_tri...             3418   \n",
       "20  /home/epb123/output/integrated Green/green_tri...             3301   \n",
       "21  /home/epb123/output/integrated Green/green_tri...             3546   \n",
       "22  /home/epb123/output/integrated Green/green_tri...             3250   \n",
       "23  /home/epb123/output/integrated Green/green_tri...             3058   \n",
       "24  /home/epb123/output/integrated Green/green_tri...             3039   \n",
       "25  /home/epb123/output/integrated Green/green_tri...             2968   \n",
       "26  /home/epb123/output/integrated Green/green_tri...             3234   \n",
       "27  /home/epb123/output/integrated Green/green_tri...             3037   \n",
       "28  /home/epb123/output/integrated Green/green_tri...             3190   \n",
       "29  /home/epb123/output/integrated Green/green_tri...             2870   \n",
       "..                                                ...              ...   \n",
       "46  /home/epb123/output/integrated Green/green_tri...             1943   \n",
       "47  /home/epb123/output/integrated Green/green_tri...             1820   \n",
       "48  /home/epb123/output/integrated Green/green_tri...             1726   \n",
       "49  /home/epb123/output/integrated Green/green_tri...             1755   \n",
       "50  /home/epb123/output/integrated Green/green_tri...             1842   \n",
       "51  /home/epb123/output/integrated Green/green_tri...             1739   \n",
       "52  /home/epb123/output/integrated Green/green_tri...             1803   \n",
       "53  /home/epb123/output/integrated Green/green_tri...             1582   \n",
       "54  /home/epb123/output/integrated Green/green_tri...             1535   \n",
       "55  /home/epb123/output/integrated Green/green_tri...             1666   \n",
       "56  /home/epb123/output/integrated Green/green_tri...             1594   \n",
       "57  /home/epb123/output/integrated Green/green_tri...             1588   \n",
       "58  /home/epb123/output/integrated Green/green_tri...             1476   \n",
       "59  /home/epb123/output/integrated Green/green_tri...             1368   \n",
       "60  /home/epb123/output/integrated Green/green_tri...             1330   \n",
       "61  /home/epb123/output/integrated Green/green_tri...             1331   \n",
       "62  /home/epb123/output/integrated Green/green_tri...             1419   \n",
       "63  /home/epb123/output/integrated Green/green_tri...             1311   \n",
       "64  /home/epb123/output/integrated Green/green_tri...             1370   \n",
       "65  /home/epb123/output/integrated Green/green_tri...             1258   \n",
       "66  /home/epb123/output/integrated Green/green_tri...             1146   \n",
       "67  /home/epb123/output/integrated Green/green_tri...             1196   \n",
       "68  /home/epb123/output/integrated Green/green_tri...             1023   \n",
       "69  /home/epb123/output/integrated Green/green_tri...             1004   \n",
       "70  /home/epb123/output/integrated Green/green_tri...              935   \n",
       "71  /home/epb123/output/integrated Green/green_tri...              895   \n",
       "72  /home/epb123/output/integrated Green/green_tri...              797   \n",
       "73  /home/epb123/output/integrated Green/green_tri...               71   \n",
       "74  /home/epb123/output/integrated Green/green_tri...              114   \n",
       "75  /home/epb123/output/integrated Green/green_tri...              126   \n",
       "\n",
       "    lpep_dropoff_datetime_val  lpep_pickup_datetime_val  dolocationid_val  \\\n",
       "0                           0                         0                 1   \n",
       "1                           0                         0                 2   \n",
       "2                           1                         1                 4   \n",
       "3                           3                         3                 8   \n",
       "4                           2                         2                 8   \n",
       "5                           3                         3                 6   \n",
       "6                           6                         6                13   \n",
       "7                           7                         7                11   \n",
       "8                          14                        14                14   \n",
       "9                          12                        12                11   \n",
       "10                         12                        12                19   \n",
       "11                         13                        13                 7   \n",
       "12                         16                        16                16   \n",
       "13                          5                         5                11   \n",
       "14                          7                         7                14   \n",
       "15                         10                        10                10   \n",
       "16                          7                         7                11   \n",
       "17                          1                         1                 8   \n",
       "18                          8                         8                12   \n",
       "19                          8                         8                15   \n",
       "20                          7                         7                16   \n",
       "21                         14                        14                15   \n",
       "22                          5                         5                12   \n",
       "23                         10                        10                16   \n",
       "24                         13                        13                10   \n",
       "25                         13                        13                13   \n",
       "26                          5                         5                12   \n",
       "27                         12                        12                10   \n",
       "28                          9                         9                10   \n",
       "29                          9                         9                15   \n",
       "..                        ...                       ...               ...   \n",
       "46                          5                         5                 2   \n",
       "47                          6                         6                 4   \n",
       "48                          4                         4                 3   \n",
       "49                          5                         5                 8   \n",
       "50                          3                         3                 5   \n",
       "51                          5                         5                 3   \n",
       "52                          6                         6                 2   \n",
       "53                          4                         4                 3   \n",
       "54                          4                         4                 7   \n",
       "55                          3                         3                 5   \n",
       "56                          2                         2                 2   \n",
       "57                          5                         5                 1   \n",
       "58                          3                         3                10   \n",
       "59                          1                         1                 5   \n",
       "60                          1                         1                 3   \n",
       "61                          3                         3                 2   \n",
       "62                          5                         5                 3   \n",
       "63                          4                         4                 6   \n",
       "64                          3                         3                 4   \n",
       "65                          2                         2                 1   \n",
       "66                          5                         5                 3   \n",
       "67                          5                         5                 8   \n",
       "68                          4                         4                 7   \n",
       "69                          3                         3                 5   \n",
       "70                          2                         2                 8   \n",
       "71                          3                         3                 5   \n",
       "72                          2                         2                 9   \n",
       "73                          0                         0                 0   \n",
       "74                          1                         1                 0   \n",
       "75                          1                         1                 1   \n",
       "\n",
       "    pulocationid_val  ratecodeid_val  payment_type_val  trip_distance_val  \\\n",
       "0                  1               0                 0                  2   \n",
       "1                  3               0                 0                  8   \n",
       "2                  2               0                 0                 13   \n",
       "3                  3               0                 3                 18   \n",
       "4                  2               0                 2                 18   \n",
       "5                  4               0                 5                 49   \n",
       "6                  2               0                 4                 37   \n",
       "7                  4               0                 8                 54   \n",
       "8                 10               0                 7                 36   \n",
       "9                  7               0                 5                 46   \n",
       "10                11               0                 5                 38   \n",
       "11                 6               0                 8                 38   \n",
       "12                 8               0                 4                 29   \n",
       "13                 2               0                 3                 43   \n",
       "14                 8               0                 6                 40   \n",
       "15                 8               0                 8                 44   \n",
       "16                 4               0                 9                 47   \n",
       "17                 4               0                11                 35   \n",
       "18                 5               0                 8                 47   \n",
       "19                 7               0                14                 54   \n",
       "20                 8               0                18                 48   \n",
       "21                 7               0                 8                 57   \n",
       "22                 7               0                13                 48   \n",
       "23                11               0                13                 47   \n",
       "24                 6               0                 9                 50   \n",
       "25                10               0                10                 45   \n",
       "26                 5               0                 6                 39   \n",
       "27                 4               0                12                 35   \n",
       "28                 7               0                 9                 45   \n",
       "29                14               0                17                 37   \n",
       "..               ...             ...               ...                ...   \n",
       "46                 2               0                 5                 21   \n",
       "47                 6               0                 5                 26   \n",
       "48                 1               0                13                 25   \n",
       "49                 0               0                 7                 12   \n",
       "50                 2               0                 9                 25   \n",
       "51                 2               0                12                 22   \n",
       "52                 2               0                 8                 22   \n",
       "53                 3               0                13                 23   \n",
       "54                 4               0                10                  9   \n",
       "55                 1               0                 5                 15   \n",
       "56                 2               0                13                 23   \n",
       "57                 1               0                 6                 11   \n",
       "58                 2               0                 6                 16   \n",
       "59                 0               0                 5                 16   \n",
       "60                 4               0                 4                 14   \n",
       "61                 1               0                 7                 12   \n",
       "62                 4               1                 4                 22   \n",
       "63                 4               0                 7                 16   \n",
       "64                 4               0                 9                 18   \n",
       "65                 4               0                 6                 15   \n",
       "66                 3               0                 8                 23   \n",
       "67                 4               0                 5                 25   \n",
       "68                 4               0                 7                 17   \n",
       "69                 3               0                 7                 15   \n",
       "70                 3               0                 5                 17   \n",
       "71                 1               0                 2                 31   \n",
       "72                 0               0                 2                 19   \n",
       "73                 0               0                 0                  8   \n",
       "74                 1               0                 0                  8   \n",
       "75                 0               0                 0                  6   \n",
       "\n",
       "    passenger_count_val  tip_amount_val  fare_amount_val  total_amount_val  \\\n",
       "0                     0               0                0                 0   \n",
       "1                     0               0                0                 0   \n",
       "2                     0               0                0                 0   \n",
       "3                     0               0                0                 0   \n",
       "4                     0               0                0                 0   \n",
       "5                     1               0                0                 0   \n",
       "6                     0               0                0                 0   \n",
       "7                     0               0                0                 0   \n",
       "8                     0               0                0                 0   \n",
       "9                     0               0                0                 1   \n",
       "10                    0               0                0                 0   \n",
       "11                    0               0                4                 4   \n",
       "12                    1               0                1                 1   \n",
       "13                    0               0                1                 2   \n",
       "14                    1               0                5                 5   \n",
       "15                    1               0                4                 5   \n",
       "16                    2               0                5                 5   \n",
       "17                    1               0                5                 8   \n",
       "18                    2               0                8                10   \n",
       "19                    1               1                6                 8   \n",
       "20                    1               0                8                 8   \n",
       "21                    0               0                4                 4   \n",
       "22                    1               0                6                 6   \n",
       "23                    1               1                5                 6   \n",
       "24                    0               0                4                 4   \n",
       "25                    1               0                1                 1   \n",
       "26                    1               0                2                 3   \n",
       "27                    3               0                9                 9   \n",
       "28                    1               0                4                 4   \n",
       "29                    0               1                4                 4   \n",
       "..                  ...             ...              ...               ...   \n",
       "46                    0               0                2                48   \n",
       "47                    0               0                2                53   \n",
       "48                    0               0                5                58   \n",
       "49                    0               0                3                70   \n",
       "50                    0               0                5                16   \n",
       "51                    1               0                8                19   \n",
       "52                    0               0                2                26   \n",
       "53                    0               0                3                20   \n",
       "54                    2               0                3                28   \n",
       "55                    3               0                2                12   \n",
       "56                    4               0                6                16   \n",
       "57                    1               0                3                22   \n",
       "58                    6               0                5                24   \n",
       "59                    4               0                1                10   \n",
       "60                    6               0                1                13   \n",
       "61                    1               0                3                15   \n",
       "62                    4               0                1                17   \n",
       "63                    1               0                4                21   \n",
       "64                    2               0                4                12   \n",
       "65                    2               0                2                 8   \n",
       "66                    2               0                4                32   \n",
       "67                    1               0                6                43   \n",
       "68                    3               0                4                36   \n",
       "69                    2               0                6                34   \n",
       "70                    2               0                4                30   \n",
       "71                    0               0                2                26   \n",
       "72                    0               0                1                19   \n",
       "73                    0               0                0                 2   \n",
       "74                    0               0                0                 8   \n",
       "75                    0               0                0                 4   \n",
       "\n",
       "    improvement_surcharge_val  extra_val  mta_tax_val  tolls_amount_val  \\\n",
       "0                           0          0            0                 0   \n",
       "1                           0          0            0                 0   \n",
       "2                           0          1            0                 0   \n",
       "3                           0          3            0                 0   \n",
       "4                           0          0            0                 0   \n",
       "5                           0          0            0                 0   \n",
       "6                           0          0            0                 0   \n",
       "7                           0          0            0                 0   \n",
       "8                           0          0            0                 0   \n",
       "9                           0          0            0                 0   \n",
       "10                          0          0            0                 0   \n",
       "11                          0          1            4                 0   \n",
       "12                          0          1            1                 0   \n",
       "13                          0          1            1                 0   \n",
       "14                          0          2            5                 0   \n",
       "15                          0          2            4                 0   \n",
       "16                          0          2            5                 0   \n",
       "17                          0          1            4                 0   \n",
       "18                          0          4            8                 0   \n",
       "19                          0          3            6                 0   \n",
       "20                          7          4            8                 0   \n",
       "21                          3          2            4                 0   \n",
       "22                          6          4            6                 0   \n",
       "23                          5          3            5                 0   \n",
       "24                          3          3            3                 0   \n",
       "25                          0          0            0                 1   \n",
       "26                          2          0            2                 0   \n",
       "27                          8          5            8                 0   \n",
       "28                          4          3            4                 0   \n",
       "29                          4          1            4                 0   \n",
       "..                        ...        ...          ...               ...   \n",
       "46                          2          3            2                 0   \n",
       "47                          2          1            2                 0   \n",
       "48                          5          5            5                 0   \n",
       "49                          3          1            3                 0   \n",
       "50                          4          3            4                 0   \n",
       "51                          8          6            8                 0   \n",
       "52                          2          3            2                 0   \n",
       "53                          3          2            3                 0   \n",
       "54                          3          0            3                 0   \n",
       "55                          2          2            2                 0   \n",
       "56                          6          4            6                 0   \n",
       "57                          3          2            3                 0   \n",
       "58                          5          2            5                 0   \n",
       "59                          1          1            1                 0   \n",
       "60                          1          2            1                 0   \n",
       "61                          3          3            3                 0   \n",
       "62                          1          1            1                 0   \n",
       "63                          4          3            4                 0   \n",
       "64                          4          4            4                 0   \n",
       "65                          2          0            2                 0   \n",
       "66                          4         16            4                 0   \n",
       "67                          6         30            6                 0   \n",
       "68                          4         27            4                 0   \n",
       "69                          6         25            6                 0   \n",
       "70                          4         21            4                 0   \n",
       "71                          2        189            1                 0   \n",
       "72                          1        170            1                 0   \n",
       "73                          0          0            0                 0   \n",
       "74                          0          1            0                 0   \n",
       "75                          0          2            0                 0   \n",
       "\n",
       "    congestion_surcharge_val  row_val  \n",
       "0                          0        2  \n",
       "1                          0        9  \n",
       "2                          0       17  \n",
       "3                          0       27  \n",
       "4                          0       29  \n",
       "5                          0       59  \n",
       "6                          0       53  \n",
       "7                          0       79  \n",
       "8                          0       68  \n",
       "9                          0       68  \n",
       "10                         0       73  \n",
       "11                         0       66  \n",
       "12                         0       60  \n",
       "13                         0       60  \n",
       "14                         0       62  \n",
       "15                         0       72  \n",
       "16                         0       70  \n",
       "17                         0       58  \n",
       "18                         0       72  \n",
       "19                         0       88  \n",
       "20                         0       90  \n",
       "21                         0       88  \n",
       "22                         0       76  \n",
       "23                         0       76  \n",
       "24                         0       78  \n",
       "25                         0       79  \n",
       "26                         0       61  \n",
       "27                         0       69  \n",
       "28                         0       74  \n",
       "29                         0       72  \n",
       "..                       ...      ...  \n",
       "46                         0       76  \n",
       "47                         0       95  \n",
       "48                         0       90  \n",
       "49                         0       98  \n",
       "50                         0       50  \n",
       "51                         0       52  \n",
       "52                         0       58  \n",
       "53                         0       53  \n",
       "54                         0       54  \n",
       "55                         0       40  \n",
       "56                         0       48  \n",
       "57                         0       40  \n",
       "58                         0       55  \n",
       "59                         0       35  \n",
       "60                         0       32  \n",
       "61                         0       36  \n",
       "62                         0       46  \n",
       "63                         0       47  \n",
       "64                         0       37  \n",
       "65                         0       30  \n",
       "66                         1       61  \n",
       "67                         3       69  \n",
       "68                         1       59  \n",
       "69                         1       58  \n",
       "70                         1       52  \n",
       "71                         0       66  \n",
       "72                         0       44  \n",
       "73                         0       10  \n",
       "74                         0       18  \n",
       "75                         0       12  \n",
       "\n",
       "[76 rows x 19 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirtyDetailedGreen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>number of lines</th>\n",
       "      <th>vendorid_val</th>\n",
       "      <th>ratecodeid_val</th>\n",
       "      <th>payment_type_val</th>\n",
       "      <th>store_and_fwd_flag_val</th>\n",
       "      <th>passenger_count_val</th>\n",
       "      <th>trip_type_val</th>\n",
       "      <th>congestion_surcharge_val</th>\n",
       "      <th>row_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>338</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>306</td>\n",
       "      <td>338</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1189</td>\n",
       "      <td>1189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1586</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1585</td>\n",
       "      <td>1586</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1230</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2568</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>543</td>\n",
       "      <td>2568</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2596</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>2596</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2822</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2822</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2655</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2528</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2670</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2670</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2702</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2959</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2959</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3070</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3070</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3262</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2992</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3418</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3418</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3301</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3301</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3546</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3546</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3058</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3058</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3039</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3039</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2968</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>3190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>2870</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1943</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1820</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1726</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1726</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1755</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1755</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1842</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1842</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1739</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1803</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1582</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1582</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1535</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1535</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1666</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1594</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1594</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1588</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1476</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1476</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1368</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1330</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1330</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1331</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1419</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1419</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1311</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1370</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1370</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1079</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>1004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>935</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>895</td>\n",
       "      <td>230</td>\n",
       "      <td>230</td>\n",
       "      <td>230</td>\n",
       "      <td>230</td>\n",
       "      <td>230</td>\n",
       "      <td>230</td>\n",
       "      <td>230</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>797</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>71</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>114</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>/home/epb123/output/integrated Green/green_tri...</td>\n",
       "      <td>126</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 file  number of lines  \\\n",
       "0   /home/epb123/output/integrated Green/green_tri...               15   \n",
       "1   /home/epb123/output/integrated Green/green_tri...               98   \n",
       "2   /home/epb123/output/integrated Green/green_tri...              338   \n",
       "3   /home/epb123/output/integrated Green/green_tri...              756   \n",
       "4   /home/epb123/output/integrated Green/green_tri...             1189   \n",
       "5   /home/epb123/output/integrated Green/green_tri...             1586   \n",
       "6   /home/epb123/output/integrated Green/green_tri...             1987   \n",
       "7   /home/epb123/output/integrated Green/green_tri...             2568   \n",
       "8   /home/epb123/output/integrated Green/green_tri...             2596   \n",
       "9   /home/epb123/output/integrated Green/green_tri...             2822   \n",
       "10  /home/epb123/output/integrated Green/green_tri...             2655   \n",
       "11  /home/epb123/output/integrated Green/green_tri...             2528   \n",
       "12  /home/epb123/output/integrated Green/green_tri...             2670   \n",
       "13  /home/epb123/output/integrated Green/green_tri...             2702   \n",
       "14  /home/epb123/output/integrated Green/green_tri...             2959   \n",
       "15  /home/epb123/output/integrated Green/green_tri...             3070   \n",
       "16  /home/epb123/output/integrated Green/green_tri...             3262   \n",
       "17  /home/epb123/output/integrated Green/green_tri...             2992   \n",
       "18  /home/epb123/output/integrated Green/green_tri...             3124   \n",
       "19  /home/epb123/output/integrated Green/green_tri...             3418   \n",
       "20  /home/epb123/output/integrated Green/green_tri...             3301   \n",
       "21  /home/epb123/output/integrated Green/green_tri...             3546   \n",
       "22  /home/epb123/output/integrated Green/green_tri...             3250   \n",
       "23  /home/epb123/output/integrated Green/green_tri...             3058   \n",
       "24  /home/epb123/output/integrated Green/green_tri...             3039   \n",
       "25  /home/epb123/output/integrated Green/green_tri...             2968   \n",
       "26  /home/epb123/output/integrated Green/green_tri...             3234   \n",
       "27  /home/epb123/output/integrated Green/green_tri...             3037   \n",
       "28  /home/epb123/output/integrated Green/green_tri...             3190   \n",
       "29  /home/epb123/output/integrated Green/green_tri...             2870   \n",
       "..                                                ...              ...   \n",
       "46  /home/epb123/output/integrated Green/green_tri...             1943   \n",
       "47  /home/epb123/output/integrated Green/green_tri...             1820   \n",
       "48  /home/epb123/output/integrated Green/green_tri...             1726   \n",
       "49  /home/epb123/output/integrated Green/green_tri...             1755   \n",
       "50  /home/epb123/output/integrated Green/green_tri...             1842   \n",
       "51  /home/epb123/output/integrated Green/green_tri...             1739   \n",
       "52  /home/epb123/output/integrated Green/green_tri...             1803   \n",
       "53  /home/epb123/output/integrated Green/green_tri...             1582   \n",
       "54  /home/epb123/output/integrated Green/green_tri...             1535   \n",
       "55  /home/epb123/output/integrated Green/green_tri...             1666   \n",
       "56  /home/epb123/output/integrated Green/green_tri...             1594   \n",
       "57  /home/epb123/output/integrated Green/green_tri...             1588   \n",
       "58  /home/epb123/output/integrated Green/green_tri...             1476   \n",
       "59  /home/epb123/output/integrated Green/green_tri...             1368   \n",
       "60  /home/epb123/output/integrated Green/green_tri...             1330   \n",
       "61  /home/epb123/output/integrated Green/green_tri...             1331   \n",
       "62  /home/epb123/output/integrated Green/green_tri...             1419   \n",
       "63  /home/epb123/output/integrated Green/green_tri...             1311   \n",
       "64  /home/epb123/output/integrated Green/green_tri...             1370   \n",
       "65  /home/epb123/output/integrated Green/green_tri...             1258   \n",
       "66  /home/epb123/output/integrated Green/green_tri...             1146   \n",
       "67  /home/epb123/output/integrated Green/green_tri...             1196   \n",
       "68  /home/epb123/output/integrated Green/green_tri...             1023   \n",
       "69  /home/epb123/output/integrated Green/green_tri...             1004   \n",
       "70  /home/epb123/output/integrated Green/green_tri...              935   \n",
       "71  /home/epb123/output/integrated Green/green_tri...              895   \n",
       "72  /home/epb123/output/integrated Green/green_tri...              797   \n",
       "73  /home/epb123/output/integrated Green/green_tri...               71   \n",
       "74  /home/epb123/output/integrated Green/green_tri...              114   \n",
       "75  /home/epb123/output/integrated Green/green_tri...              126   \n",
       "\n",
       "    vendorid_val  ratecodeid_val  payment_type_val  store_and_fwd_flag_val  \\\n",
       "0              0               0                 0                       0   \n",
       "1              0               0                 0                       0   \n",
       "2              0               0                 0                       0   \n",
       "3              0               0                 0                       0   \n",
       "4              0               0                 0                       0   \n",
       "5              0               0                 0                       0   \n",
       "6              0               0                 0                       0   \n",
       "7              0               0                 0                       0   \n",
       "8              0               0                 0                       0   \n",
       "9              0               0                 0                       0   \n",
       "10             0               0                 0                       0   \n",
       "11             0               0                 0                       0   \n",
       "12             0               0                 0                       0   \n",
       "13             0               0                 0                       0   \n",
       "14             0               0                 0                       0   \n",
       "15             0               0                 0                       0   \n",
       "16             0               0                 0                       0   \n",
       "17             0               0                 0                       0   \n",
       "18             0               0                 0                       0   \n",
       "19             0               0                 0                       0   \n",
       "20             0               0                 0                       0   \n",
       "21             0               0                 0                       0   \n",
       "22             0               0                 0                       0   \n",
       "23             0               0                 0                       0   \n",
       "24             0               0                 0                       0   \n",
       "25             0               0                 0                       0   \n",
       "26             0               0                 0                       0   \n",
       "27             0               0                 0                       0   \n",
       "28             0               0                 0                       0   \n",
       "29             0               0                 0                       0   \n",
       "..           ...             ...               ...                     ...   \n",
       "46             0               0                 0                       0   \n",
       "47             0               0                 0                       0   \n",
       "48             0               0                 0                       0   \n",
       "49             0               0                 0                       0   \n",
       "50             0               0                 0                       0   \n",
       "51             0               0                 0                       0   \n",
       "52             0               0                 0                       0   \n",
       "53             0               0                 0                       0   \n",
       "54             0               0                 0                       0   \n",
       "55             0               0                 0                       0   \n",
       "56             0               0                 0                       0   \n",
       "57             0               0                 0                       0   \n",
       "58             0               0                 0                       0   \n",
       "59             0               0                 0                       0   \n",
       "60             0               0                 0                       0   \n",
       "61             0               0                 0                       0   \n",
       "62             0               0                 0                       0   \n",
       "63             0               0                 0                       0   \n",
       "64             0               0                 0                       0   \n",
       "65             0               0                 0                       0   \n",
       "66             0               0                 0                       0   \n",
       "67             0               0                 0                       0   \n",
       "68             0               0                 0                       0   \n",
       "69             0               0                 0                       0   \n",
       "70             0               0                 0                       0   \n",
       "71           230             230               230                     230   \n",
       "72           160             160               160                     160   \n",
       "73            21              21                21                      21   \n",
       "74            54              54                54                      54   \n",
       "75            47              47                47                      47   \n",
       "\n",
       "    passenger_count_val  trip_type_val  congestion_surcharge_val  row_val  \n",
       "0                     0             15                        15        0  \n",
       "1                     0             98                        98        0  \n",
       "2                     0            306                       338        0  \n",
       "3                     0            756                       756        0  \n",
       "4                     0           1189                      1189        0  \n",
       "5                     0           1585                      1586        0  \n",
       "6                     0           1230                      1987        0  \n",
       "7                     0            543                      2568        0  \n",
       "8                     0            167                      2596        0  \n",
       "9                     0              0                      2822        0  \n",
       "10                    0              0                      2655        0  \n",
       "11                    0              0                      2528        0  \n",
       "12                    0              0                      2670        0  \n",
       "13                    0              0                      2702        0  \n",
       "14                    0              0                      2959        0  \n",
       "15                    0              0                      3070        0  \n",
       "16                    0              0                      3262        0  \n",
       "17                    0              0                      2992        0  \n",
       "18                    0              0                      3124        0  \n",
       "19                    0              0                      3418        0  \n",
       "20                    0              0                      3301        0  \n",
       "21                    0              0                      3546        0  \n",
       "22                    0              0                      3250        0  \n",
       "23                    0              0                      3058        0  \n",
       "24                    0              0                      3039        0  \n",
       "25                    0              0                      2968        0  \n",
       "26                    0              0                      3234        0  \n",
       "27                    0              0                      3037        0  \n",
       "28                    0              0                      3190        0  \n",
       "29                    0              0                      2870        0  \n",
       "..                  ...            ...                       ...      ...  \n",
       "46                    0              0                      1943        0  \n",
       "47                    0              0                      1820        0  \n",
       "48                    0              0                      1726        0  \n",
       "49                    0              0                      1755        0  \n",
       "50                    0              0                      1842        0  \n",
       "51                    0              0                      1739        0  \n",
       "52                    0              0                      1803        0  \n",
       "53                    0              0                      1582        0  \n",
       "54                    0              0                      1535        0  \n",
       "55                    0              0                      1666        0  \n",
       "56                    0              0                      1594        0  \n",
       "57                    0              0                      1588        0  \n",
       "58                    0              0                      1476        0  \n",
       "59                    0              0                      1368        0  \n",
       "60                    0              0                      1330        0  \n",
       "61                    0              0                      1331        0  \n",
       "62                    0              1                      1419        0  \n",
       "63                    0              0                      1311        0  \n",
       "64                    0              0                      1370        0  \n",
       "65                    0              0                      1079        0  \n",
       "66                    0              0                         0        0  \n",
       "67                    0              0                         0        0  \n",
       "68                    0              0                         0        0  \n",
       "69                    0              0                         0        0  \n",
       "70                    0              0                         0        0  \n",
       "71                  230            230                       230      221  \n",
       "72                  160            160                       160      158  \n",
       "73                   21             21                        21       20  \n",
       "74                   54             54                        54       46  \n",
       "75                   47             47                        47       45  \n",
       "\n",
       "[76 rows x 10 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missingGreen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the dirty dataframe separately to avoid unfortunate errors\n",
    "# We can update our cleaned values as we go\n",
    "\n",
    "recleanedGREEN = dirtyGREEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of our core variables, what is broken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 4687 rows containing dirty values and...\n",
      "490 rows not containing dirty values but containing missing values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where our dropped records came from\n",
    "print(\"We have \"+str(dirtyGREEN.filter(dirtyGREEN.row_val == 1).count())+\" rows containing dirty values and...\")\n",
    "print(str(dirtyGREEN.filter(dirtyGREEN.row_val == 2).count())+\" rows not containing dirty values but containing missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is very interesting. Looking at the summary, we can see that out of what is important, we still have quite a good set:  \n",
    "- pickup_datetime: clean, not missing\n",
    "- dropoff_datetime: clean, not missing\n",
    "- pulocationid: dirty, not missing\n",
    "- dolocationid: dirty, not missing\n",
    "- trip_distance: dirty, not missing\n",
    "- tip_amount: dirty, not missing\n",
    "- total_amount: dirty, not missing\n",
    "- payment_type: dirty, missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue with **total_amount**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1378 dirty values and...\n",
      "0 missing values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where our dropped records came from\n",
    "print(\"We have \"+str(dirtyGREEN.filter(dirtyGREEN.total_amount_val == 1).count())+\" dirty values and...\")\n",
    "print(str(dirtyGREEN.filter(dirtyGREEN.total_amount_val == 2).count())+\" missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 0 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "# We can then analyse the dirty values - we know the 5% criterion will have caused many records to drop\n",
    "# Let us first check to see if the values are incorrect strings\n",
    "# We know the total_amounts here are not null, thus we can do...\n",
    "\n",
    "dirtyGREENanalysis = dirtyGREEN.filter(dirtyGREEN.total_amount_val == 1)\n",
    "print(\"we have \" + str(dirtyGREENanalysis.select(\"total_amount\",sql.col(\"total_amount\").cast(\"float\").isNotNull().alias(\"strings\")).\\\n",
    "        filter(col('strings') == 'false').count()) + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we appear to have 255 negative total_amount entries\n"
     ]
    }
   ],
   "source": [
    "# We can then continue with negatives\n",
    "\n",
    "dirtyGREENanalysis = dirtyGREENanalysis.withColumn('total_amount',col('total_amount').cast(FloatType()))\n",
    "dirtyGREENanalysis = dirtyGREENanalysis.withColumn('negative_total', sql.when(col('total_amount') < 0.0, 1).otherwise(0))\n",
    "print(\"we appear to have \" + str(dirtyGREENanalysis.filter(col('negative_total') == 1).count()) + \" negative total_amount entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is 1 lucky taxi driver\n",
      "+--------------------+------------+\n",
      "|            filename|total_amount|\n",
      "+--------------------+------------+\n",
      "|green_tripdata_20...|      1202.3|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us now check to see if any of the records tripped the 1000 upper bound\n",
    "\n",
    "print(\"there is \"+str(dirtyGREENanalysis.filter(dirtyGREENanalysis.total_amount > 1000).count())+\" lucky taxi driver\")\n",
    "dirtyGREENanalysis.filter(dirtyGREENanalysis.total_amount > 1000).select([col('filename'),col('total_amount')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All other conditions were checked, so we must have approx. 1,122 total_amount records which are more than 5% away from the sum of their components. Given that the total_amount record should be the most accurate of the monetary records (as it is what would be actually received in the company accounts), we have a verification issue. It is of course possible that the record was mistakenly entered, but we should theoretically trust it over the others. The UDF to flag mistaken total_amount values also depends on non-repaired entries for the other cash-based variables, however it does correct for what would be common non-direct value related errors (string storage and negativity). We, in the end, choose to set the flagged total_amount values to null,  as the basic checking and the 5% margin of error should correct for small mistakes in the other variables while capturing large errors which would cast doubt over the record's integrity. \n",
    "\n",
    "Negative values can be then converted into positive ones, under the assumption that they are mistakenly entered. If we then remove duplicates after the re-addition to the clean records, we can cover ourselves in case the negative values are some sort of accounting reversal.  \n",
    "\n",
    "For the lucky taxi driver, given that the total amount is within reason (i.e. the youtuber in New York), we can keep the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receipt updating testing function stored as UDF\n",
      "1122 irreconcilable totals removed\n"
     ]
    }
   ],
   "source": [
    "# Let's make the updates to the recleanedGREEN, which we will later inject back into the dataset.\n",
    "\n",
    "# We have no strings to update - we can cast the column as a float\n",
    "recleanedGREEN = recleanedGREEN.withColumn('total_amount',col('total_amount').cast(FloatType()))\n",
    "# So first update values as absolute - probably faster to abs() the whole column than rely on conditionals\n",
    "recleanedGREEN = recleanedGREEN.withColumn('total_amount', sql.abs(col('total_amount')))\n",
    "# We accept the discovered outlier as probably correct\n",
    "# Let us set the remainder of records = Null\n",
    "\n",
    "def GREENtotalupdate(total,cong_s,extra,fare,improv,tax,tip,toll,ehail):\n",
    "# Function to transform total_amount to null if conditions not held\n",
    "# Function returns Null if it is invalid, total if it is valid\n",
    "    \n",
    "    #no need for this as we know it's not missing\n",
    "#     if total == None or total == \"\" or total == 'nan':\n",
    "#         return 2\n",
    "#     try:\n",
    "#         total = float(total)\n",
    "#     except:\n",
    "#         return 1 \n",
    "    # No need for this as already done\n",
    "    #difference = abs(total)\n",
    "    difference = total\n",
    "    for i in [cong_s,extra,fare,improv,tax,tip,toll,ehail]:\n",
    "        if i == None or i == \"\" or i == 'nan' or i == 'NaN':\n",
    "            i = 0\n",
    "        try:\n",
    "            i = abs(float(i))\n",
    "        # Note we use abs here so the fact that we haven't checked tip yet does not affect it\n",
    "            difference = difference - i\n",
    "        except:\n",
    "        # We do the same for float values - we simply want to ignore incorrect strings here\n",
    "            pass\n",
    "        \n",
    "    if abs(difference) > total*0.05: #and total >= 0 and total <= 1000:\n",
    "    # if the difference is greater than 5% of total, it is incorrect and we return None\n",
    "        return None\n",
    "    \n",
    "    return total\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udfGREENtotalbroken = udf(GREENtotalupdate, FloatType())\n",
    "\n",
    "print(\"Receipt updating testing function stored as UDF\")\n",
    "\n",
    "def add_GREEN_total_amount_update(dfrval):\n",
    "# Adds the extra validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfrval = dfrval.withColumn('total_amount', udfGREENtotalbroken('total_amount','congestion_surcharge','extra',\\\n",
    "    'fare_amount','improvement_surcharge','mta_tax','tip_amount','tolls_amount','ehail_fee'))\n",
    "\n",
    "    return dfrval\n",
    "\n",
    "recleanedGREEN = add_GREEN_total_amount_update(recleanedGREEN)\n",
    "\n",
    "# we only need to count once - we did not previously have missing values\n",
    "print(str(recleanedGREEN.filter(col('total_amount').isNull()).count()) + \" irreconcilable totals removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then move onto **tip_amount**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3 dirty values and...\n",
      "0 missing values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where any dropped records came from\n",
    "print(\"We have \"+str(dirtyGREEN.filter(dirtyGREEN.tip_amount_val == 1).count())+\" dirty values and...\")\n",
    "print(str(dirtyGREEN.filter(dirtyGREEN.tip_amount_val == 2).count())+\" missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 0 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "# Let us now check to see if the values are incorrect strings\n",
    "# We know the tip_amounts here are not null, thus we can do...\n",
    "\n",
    "dirtyGREENanalysis = dirtyGREEN.filter(dirtyGREEN.tip_amount_val == 1)\n",
    "print(\"we have \" + str(dirtyGREENanalysis.select(\"tip_amount\",sql.col(\"tip_amount\").cast(\"float\").isNotNull().alias(\"strings\")).\\\n",
    "        filter(col('strings') == 'false').count()) + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we appear to have 3 negative tip_amount entries\n"
     ]
    }
   ],
   "source": [
    "# We can then continue with negatives\n",
    "\n",
    "dirtyGREENanalysis = dirtyGREENanalysis.withColumn('tip_amount',col('tip_amount').cast(FloatType()))\n",
    "dirtyGREENanalysis = dirtyGREENanalysis.withColumn('negative_tip', sql.when(col('tip_amount') < 0.0, 1).otherwise(0))\n",
    "print(\"we appear to have \" + str(dirtyGREENanalysis.filter(col('negative_tip') == 1).count()) + \" negative tip_amount entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well - as before, those three tip_amount entries can be repaired and put back into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make the updates to the recleanedGREEN, which we will later inject back into the dataset.\n",
    "\n",
    "# We have no strings to update - we can cast the column as a float\n",
    "recleanedGREEN = recleanedGREEN.withColumn('tip_amount',col('tip_amount').cast(FloatType()))\n",
    "# So update values as absolute - probably faster to abs() the whole column than rely on conditionals\n",
    "recleanedGREEN = recleanedGREEN.withColumn('tip_amount', sql.abs(col('tip_amount')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start on **payment_type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 589 dirty values and...\n",
      "512 missing values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where our dropped records came from\n",
    "print(\"We have \"+str(dirtyGREEN.filter(dirtyGREEN.payment_type_val == 1).count())+\" dirty values and...\")\n",
    "print(str(dirtyGREEN.filter(dirtyGREEN.payment_type_val == 2).count())+\" missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can impute missing values - we have just fixed the tip amount  \n",
    "- If there is a Null value but tip exists, we store as card  \n",
    "- If there is a Null value but tip == 0, we store as cash  \n",
    "\n",
    "We assume the tip would not exist with a void trip or a no charge trip.\n",
    "We also make the assumption that no tip strongly correlates with a cash trip (in a country where restaurant tips are 20% standard, we believe this is reasonable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 72 trips with a tip but no payment type stored\n",
      "we have 440 trips with no tip and no payment type stored\n"
     ]
    }
   ],
   "source": [
    "# We can impute missing values - we have just fixed the tip amount\n",
    "# If there is a Null value but tip exists, we store as card\n",
    "# If there is a Null value but tip == 0, we store as cash\n",
    "# We assume the tip would not exist with a void trip or a no charge trip\n",
    "dirtyGREENanalysis = dirtyGREEN\n",
    "print(\"we have \" + str(dirtyGREENanalysis.filter((col(\"payment_type\").isNull()) & (col(\"tip_amount\") != 0)).count())\\\n",
    "                       + \" trips with a tip but no payment type stored\")\n",
    "print(\"we have \" + str(dirtyGREENanalysis.filter((col(\"payment_type\").isNull()) & (col(\"tip_amount\") == 0)).count())\\\n",
    "                       + \" trips with no tip and no payment type stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "recleanedGREEN = recleanedGREEN.withColumn('payment_type',sql.when((col(\"payment_type\").isNull())\\\n",
    "                                                                   & (col(\"tip_amount\") != 0), '1')\\\n",
    "                                           .otherwise(col('payment_type')))\n",
    "recleanedGREEN = recleanedGREEN.withColumn('payment_type',sql.when((col(\"payment_type\").isNull())\\\n",
    "                                                                   & (col(\"tip_amount\") == 0), '2')\\\n",
    "                                           .otherwise(col('payment_type')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 0 remaining trips with a tip but no payment type stored\n",
      "we have 0 remaining trips with no tip and no payment type stored\n"
     ]
    }
   ],
   "source": [
    "print(\"we have \" + str(recleanedGREEN.filter((col(\"payment_type\").isNull()) & (col(\"tip_amount\") != 0)).count())\\\n",
    "                       + \" remaining trips with a tip but no payment type stored\")\n",
    "print(\"we have \" + str(recleanedGREEN.filter((col(\"payment_type\").isNull()) & (col(\"tip_amount\") == 0)).count())\\\n",
    "                       + \" remaining trips with no tip and no payment type stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 0 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "dirtyGREENanalysis = dirtyGREEN.filter(dirtyGREEN.payment_type_val == 1)\n",
    "\n",
    "# We can test invalid payment types\n",
    "# Let us first check to see if the values are incorrect strings\n",
    "# We know the payment_types here are not null, thus we can do...\n",
    "\n",
    "print(\"we have \" + str(dirtyGREENanalysis.filter(col(\"payment_type\").cast(\"int\").isNull()).count())\\\n",
    "                       + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the strings are dealt with, we can test valid entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 0 values above 6\n",
      "We have 0 values below 1\n"
     ]
    }
   ],
   "source": [
    "print(\"We have \" + str(dirtyGREENanalysis.filter(col(\"payment_type\").cast(\"int\") > 6).count())\\\n",
    "                       + \" values above 6\")\n",
    "print(\"We have \" + str(dirtyGREENanalysis.filter(col(\"payment_type\").cast(\"int\") < 1).count())\\\n",
    "                       + \" values below 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check how many trips were either 3 or 6:  \n",
    "1) Credit card  \n",
    "2) Cash  \n",
    "3) No charge  \n",
    "4) Dispute  \n",
    "5) Unknown  \n",
    "6) Voided trip  \n",
    "\n",
    "We have flagged trips in 3 or 6 for removal because they are invalid - we do not want to analyse free trips or trips which did not exist, particularly with queries based on amounts. Trips in unknown, we cannot classify, and just because someone decided to dispute their fare did not mean by itself that it was not valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will need to drop 589 trips with no charge.\n",
      "We will need to drop 0 voided trips.\n"
     ]
    }
   ],
   "source": [
    "print(\"We will need to drop \"+str(dirtyGREENanalysis.filter(col('payment_type') == 3).count())\\\n",
    "      + \" trips with no charge.\")\n",
    "print(\"We will need to drop \"+str(dirtyGREENanalysis.filter(col('payment_type') == 6).count())\\\n",
    "      + \" voided trips.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "recleanedGREEN = recleanedGREEN.filter(col('payment_type') != 3)\n",
    "recleanedGREEN = recleanedGREEN.filter(col('payment_type') != 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 0 trips with no charge.\n",
      "We now have 0 voided trips.\n"
     ]
    }
   ],
   "source": [
    "print(\"We now have \"+str(recleanedGREEN.filter(col('payment_type') == 3).count())\\\n",
    "      + \" trips with no charge.\")\n",
    "print(\"We now have \"+str(recleanedGREEN.filter(col('payment_type') == 6).count())\\\n",
    "      + \" voided trips.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting that there were no recorded tips in a cash flagged trip, but it is possible the cab driver needs to press the 'card' button to take a card payment, eliminating the error. As we will see later, we do not have the same behaviour from our Yellow drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then move onto **trip_distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2167 dirty values and...\n",
      "0 missing values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where our dropped records came from\n",
    "print(\"We have \"+str(dirtyGREEN.filter(dirtyGREEN.trip_distance_val == 1).count())+\" dirty values and...\")\n",
    "print(str(dirtyGREEN.filter(dirtyGREEN.trip_distance_val == 2).count())+\" missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 0 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "# Let us now check to see if the values are incorrect strings\n",
    "# We know the trip_distances here are not null, thus we can do...\n",
    "\n",
    "dirtyGREENanalysis = dirtyGREEN.filter(dirtyGREEN.trip_distance_val == 1)\n",
    "print(\"we have \" + str(dirtyGREENanalysis.select(\"trip_distance\",sql.col(\"trip_distance\").cast(\"float\").isNotNull().alias(\"strings\")).\\\n",
    "        filter(col('strings') == 'false').count()) + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we appear to have 1 negative trip_distance entries\n"
     ]
    }
   ],
   "source": [
    "# We can then continue with negatives\n",
    "\n",
    "dirtyGREENanalysis = dirtyGREENanalysis.withColumn('trip_distance',col('trip_distance').cast(FloatType()))\n",
    "dirtyGREENanalysis = dirtyGREENanalysis.withColumn('negative_trip', sql.when(col('trip_distance') < 0.0, 1).otherwise(0))\n",
    "print(\"we appear to have \" + str(dirtyGREENanalysis.filter(col('negative_trip') == 1).count()) + \" negative trip_distance entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 2166 trips to nowhere\n"
     ]
    }
   ],
   "source": [
    "print(\"there are \"+str(dirtyGREENanalysis.filter(dirtyGREENanalysis.trip_distance == 0).count())+\" trips to nowhere\")\n",
    "#dirtyGREENanalysis.filter(dirtyGREENanalysis.trip_distance == 0).select([col('filename'),col('trip_distance')]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 0 trips to somewhere far far away\n",
      "+--------+-------------+\n",
      "|filename|trip_distance|\n",
      "+--------+-------------+\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Last is to check trips with distance >= 300\n",
    "print(\"there are \"+str(dirtyGREENanalysis.filter(sql.abs(dirtyGREENanalysis.trip_distance) >= 300.0).count())+\" trips to somewhere far far away\")\n",
    "dirtyGREENanalysis.filter(sql.abs(dirtyGREENanalysis.trip_distance) >= 300).select([col('filename'),col('trip_distance')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an explanation for our dirty values. The question is now to determine what was an actual trip with an incorrect distance and what was a \"trip\" which never moved. Both are possible - imagine getting in a taxi and then getting out at the next intersection because you forgot something - but we would be better off knowing which case was which. As a simple rule, we shall change the trip_distance to null if the distance is 0, but the total_amount is > 5 or the tip_amount is >0, as a 0 distance here is misleading. We will, however, keep trips with distances truly = 0 (based on the amount).\n",
    "\n",
    "We originally only used the total_amount, but we have already cleaned it and have therefore lost some values above the margin of error. We can use tip_amount alongside it - if a fare pays a tip, the distance could not possibly be 0 (although it is New York, this is still highly unlikely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 1314 trips with 0 distances and a nonzero tip or an amount > $5\n",
      "we have 0 expensive trips with 0 distances and a nonzero tip or an amount > $5 remaining\n"
     ]
    }
   ],
   "source": [
    "# Let's make the updates to the recleanedGREEN, which we will later inject back into the dataset.\n",
    "\n",
    "# let's first handle that negative value\n",
    "# Abs-ing the entire column is a little bit inelegant, but will suffice\n",
    "\n",
    "# We have no strings to update - we can cast the column as a float\n",
    "recleanedGREEN = recleanedGREEN.withColumn('trip_distance',col('trip_distance').cast(FloatType()))\n",
    "# So first update values as absolute - probably faster to abs() the whole column than rely on conditionals\n",
    "recleanedGREEN = recleanedGREEN.withColumn('trip_distance', sql.abs(col('trip_distance')))\n",
    "\n",
    "# Count the dirty records - use the pre-filtered dataset here\n",
    "print(\"we have \"+str(recleanedGREEN.filter(( col('trip_distance') == 0.0)\\\n",
    "            & ( (col('total_amount') >= 5.0) | (col(\"tip_amount\") > 0.0))).count())+\\\n",
    "            \" trips with 0 distances and a nonzero tip or an amount > $5\")\n",
    "\n",
    "# Next let's apply our filtering criteria to those trip_distances\n",
    "recleanedGREEN = recleanedGREEN.withColumn('trip_distance', sql.when( ( col('trip_distance') == 0.0)\\\n",
    "                                            & ( (col('total_amount') >= 5.0) | (col(\"tip_amount\") > 0.0)),\\\n",
    "                                            None).otherwise(col('trip_distance')))\n",
    "\n",
    "print(\"we have \"+str(recleanedGREEN.filter(( col('trip_distance') == 0.0)\\\n",
    "            & ( (col('total_amount') >= 5.0) | (col(\"tip_amount\") > 0.0))).count())+\\\n",
    "            \" expensive trips with 0 distances and a nonzero tip or an amount > $5 remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the rest we will keep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can next analyse the **dolocationid** and **pulocationid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 592 dirty dropoff values and...\n",
      "0 missing dropoff values\n",
      "We have 320 dirty pickup values and...\n",
      "0 missing pickup values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where our dropped records came from\n",
    "print(\"We have \"+str(recleanedGREEN.filter(recleanedGREEN.dolocationid_val == 1).count())+\" dirty dropoff values and...\")\n",
    "print(str(recleanedGREEN.filter(dirtyGREEN.dolocationid_val == 2).count())+\" missing dropoff values\")\n",
    "print(\"We have \"+str(recleanedGREEN.filter(recleanedGREEN.pulocationid_val == 1).count())+\" dirty pickup values and...\")\n",
    "print(str(recleanedGREEN.filter(recleanedGREEN.pulocationid_val == 2).count())+\" missing pickup values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then start with **pulocationid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 208 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "# Let us now check to see if the values are incorrect strings\n",
    "# We know the locationids here are not null, thus we can do...\n",
    "# We need to use recleanedGREEN because we have dropped some records already\n",
    "\n",
    "dirtyGREENanalysis = recleanedGREEN.filter(col('pulocationid_val') == 1)\n",
    "\n",
    "#CASTING AS INT HERE!!!!!!\n",
    "print(\"we have \" + str(dirtyGREENanalysis.filter(col(\"pulocationid\").cast(\"int\").isNull()).count())\\\n",
    "      + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These would result from a \"failed to match\" failure coming out of the 2.2 locationid calculations. Basically, if the coordinates were corrupt - i.e. something was stored at 5 deg. longitude and 36 deg latitude, the shapefile would not identify the location as being in New York. We can set these equal to Null to avoid any later confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 208 match failures\n",
      "we have re-valued 208 match failures\n"
     ]
    }
   ],
   "source": [
    "# Testing the theory...\n",
    "print(\"we have \"+str(recleanedGREEN.filter(col('pulocationid') == \"failed to match\").count())+\" match failures\")\n",
    "\n",
    "# Next let's apply our filtering criteria to those locationids\n",
    "recleanedGREEN = recleanedGREEN.withColumn('pulocationid', sql.when(col('pulocationid') == \"failed to match\",\\\n",
    "                                            None).otherwise(col('pulocationid')))\n",
    "\n",
    "print(\"we have re-valued \"+str(recleanedGREEN.filter(col('pulocationid').isNull()).count())+\" match failures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have run the locationid testing code in 2.2, and the resulting errors must therefore come from values 264, 265, and above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 112 values above our threshold\n"
     ]
    }
   ],
   "source": [
    "print(\"We have \"+str(recleanedGREEN.filter(col('pulocationid') > 263).count())+\" values above our threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 values remain after cleaning\n"
     ]
    }
   ],
   "source": [
    "recleanedGREEN = recleanedGREEN.withColumn('pulocationid', sql.when(col('pulocationid') > 263,\\\n",
    "                                            None).otherwise(col('pulocationid')))\n",
    "print(str(recleanedGREEN.filter(col('pulocationid') > 263).count())+\" values remain after cleaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dolocationid appears to have the same type of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 390 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "# Let us now check to see if the values are incorrect strings\n",
    "# We know the locationids here are not null, thus we can do...\n",
    "# Need to use recleanedGREEN here!!!\n",
    "\n",
    "dirtyGREENanalysis = recleanedGREEN.filter(col('dolocationid_val') == 1)\n",
    "\n",
    "#CASTING AS INT HERE!!!!!!\n",
    "print(\"we have \" + str(dirtyGREENanalysis.filter(col(\"dolocationid\").cast(\"int\").isNull()).count()) + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 390 match failures\n",
      "we have re-valued 390 match failures\n"
     ]
    }
   ],
   "source": [
    "# Testing the theory...\n",
    "print(\"we have \"+str(recleanedGREEN.filter(col('dolocationid') == \"failed to match\").count())+\" match failures\")\n",
    "\n",
    "# Next let's apply our filtering criteria to those locationids\n",
    "recleanedGREEN = recleanedGREEN.withColumn('dolocationid', sql.when(col('dolocationid') == \"failed to match\",\\\n",
    "                                            None).otherwise(col('dolocationid')))\n",
    "\n",
    "print(\"we have re-valued \"+str(recleanedGREEN.filter(col('dolocationid').isNull()).count())+\" match failures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 202 values above our threshold\n"
     ]
    }
   ],
   "source": [
    "print(\"We have \"+str(recleanedGREEN.filter(col('dolocationid') > 263).count())+\" values above our threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 values remain above the threshold\n"
     ]
    }
   ],
   "source": [
    "recleanedGREEN = recleanedGREEN.withColumn('dolocationid', sql.when(col('dolocationid') > 263,\\\n",
    "                                            None).otherwise(col('dolocationid')))\n",
    "\n",
    "print(str(recleanedGREEN.filter(col('dolocationid') > 263).count())+\" values remain above the threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not forget to examine the incorrect **lpep_pickup/dropoff_datetime** columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 450 dirty dropoff values and...\n",
      "0 missing dropoff values\n",
      "We have 450 dirty pickup values and...\n",
      "0 missing pickup values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where our dropped records came from\n",
    "print(\"We have \"+str(dirtyGREEN.filter(dirtyGREEN.lpep_dropoff_datetime_val == 1).count())+\" dirty dropoff values and...\")\n",
    "print(str(dirtyGREEN.filter(dirtyGREEN.lpep_dropoff_datetime_val == 2).count())+\" missing dropoff values\")\n",
    "print(\"We have \"+str(dirtyGREEN.filter(dirtyGREEN.lpep_pickup_datetime_val == 1).count())+\" dirty pickup values and...\")\n",
    "print(str(dirtyGREEN.filter(dirtyGREEN.lpep_pickup_datetime_val == 2).count())+\" missing pickup values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dirtyGREEN\n",
    "del dirtyGREENanalysis\n",
    "# We are no longer using these, and will need the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the prior issues with trip_distance, we can be reasonably confident that there will be further mismatches. Any trip with an average speed >75mph in central NYC will be an incorrect record - while the several long journeys we have would allow us to assume someone might average 55 (speed limit in NY) or so on the highway, an average above 75 when so much time would be lost moving through NYC would be almost impossible. We will therefore first correct for any trips demonstrating this error - they will almost certainly be incorrect, and so we will drop the records. \n",
    "\n",
    "Along with this, we can correct for invalid formats (by setting to Null), and afterwards we can examine dropoffs before pickups (where we will repair by switching pickuptime and dropoff time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colors drop flag function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "# We will here make a UDF to set null all excessive speed records (and all 0 time deltas and format errors while we are at it)\n",
    "\n",
    "def COLOR_datetime_speed_update(pudt,dodt,tdist):\n",
    "# Let us return a flag for whether or not to drop - True to delete       \n",
    "\n",
    "    # If one column is already gone, we want to set both columns to 0 regardless as we are only interested in the difference\n",
    "    if pudt == \"\" or pudt == None or pudt =='nan' or dodt == \"\" or dodt == None or dodt =='nan':\n",
    "        return True\n",
    "    \n",
    "    # Tf the time does not follow a correct format, delete it\n",
    "    try:\n",
    "        pudt_DT = dt.strptime(pudt, '%Y-%m-%d %H:%M:%S')\n",
    "        dodt_DT = dt.strptime(dodt, '%Y-%m-%d %H:%M:%S')\n",
    "    except:\n",
    "        return True\n",
    "    \n",
    "    # If we haven't hit a None yet, timeDelta will exist, but we have not yet switched values if less than 0\n",
    "    timeDelta = (dodt_DT - pudt_DT).total_seconds()\n",
    "    \n",
    "    if timeDelta == 0:\n",
    "    # we can drop if the difference in time, to the millisecond, is 0 - this was not a trip\n",
    "        return True\n",
    "    \n",
    "    timeDelta = abs(timeDelta)\n",
    "\n",
    "    # Try to validate speed (if distance exists and is itself valid)\n",
    "    # We have already converted tdist, but in case we move the order around we still want this to be valid\n",
    "    if tdist != None and tdist != \"\" and tdist != \"nan\":    \n",
    "        try:\n",
    "            tdist = float(tdist)\n",
    "            tdist = abs(tdist)\n",
    "            \n",
    "            speed = tdist/timeDelta*3600\n",
    "            # This will now be miles per hour\n",
    "            # Taxis could use freeway trips - 75mph is reasonable in come cases\n",
    "            if speed > 75:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "        except:\n",
    "        # We have already converted tdist, but if we move the order we want this to be valid\n",
    "        # We have already returned if timeDelta == 0, so we do not need to catch the exception otherwise\n",
    "            return False\n",
    "    else:\n",
    "    # if we have no distance, we don't want to drop\n",
    "        return False\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udfCOLORdatetime_drop_flag = udf(COLOR_datetime_speed_update, BooleanType())\n",
    "\n",
    "print(\"Colors drop flag function stored as UDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colors date testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "# We will here make the UDF such that it is applicable to Colorful taxi types:\n",
    "# If we have moved past our exclusion criteria, we want to see if we should swap the columns to repair the records\n",
    "\n",
    "def COLOR_datetime_switch_flag(pudt,dodt):\n",
    "# We have just set invalid timestamps to Null, so\n",
    "    if pudt == \"\" or pudt == None or pudt =='nan' or dodt == \"\" or dodt == None or dodt =='nan':\n",
    "    # If Null, we don't want to switch them anyway\n",
    "        return 0\n",
    "    \n",
    "    pudt_DT = dt.strptime(pudt, '%Y-%m-%d %H:%M:%S')\n",
    "    dodt_DT = dt.strptime(dodt, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    timeDelta = (dodt_DT - pudt_DT).total_seconds()\n",
    "    if timeDelta < 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udfCOLORdatetime_switch = udf(COLOR_datetime_switch_flag, IntegerType())\n",
    "\n",
    "print(\"Colors date testing function stored as UDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "recleanedGREEN = recleanedGREEN.withColumn('dropdt', udfCOLORdatetime_drop_flag('lpep_pickup_datetime',\\\n",
    "                                                                                'lpep_dropoff_datetime',\\\n",
    "                                                                                'trip_distance'))\n",
    "# Drop columns when datetime flag is True!\n",
    "recleanedGREEN = recleanedGREEN.withColumn('lpep_dropoff_datetime',sql.when(col('dropdt') == True, None).\\\n",
    "                                           otherwise(col('lpep_dropoff_datetime')))\n",
    "recleanedGREEN = recleanedGREEN.withColumn('lpep_pickup_datetime',sql.when(col('dropdt') == True, None).\\\n",
    "                                           otherwise(col('lpep_pickup_datetime')))\n",
    "\n",
    "recleanedGREEN = recleanedGREEN.drop('dropdt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the current version of the cleaned file, we now have... 373 fewer dropoff values and 373 fewer pickup values.\n"
     ]
    }
   ],
   "source": [
    "# We can do this because we have no pre-existing Nulls\n",
    "\n",
    "print(\"In the current version of the cleaned file, we now have... \"+\\\n",
    "     str(recleanedGREEN.filter(recleanedGREEN.lpep_dropoff_datetime.isNull()).count())+\\\n",
    "     \" fewer dropoff values and \"+\\\n",
    "     str(recleanedGREEN.filter(recleanedGREEN.lpep_pickup_datetime.isNull()).count())+\\\n",
    "     \" fewer pickup values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will now repair 1 backwards values\n"
     ]
    }
   ],
   "source": [
    "# Make the switch Flag dummy\n",
    "recleanedGREEN = recleanedGREEN.withColumn('tsFlag',udfCOLORdatetime_switch('lpep_pickup_datetime','lpep_dropoff_datetime'))\n",
    "print(\"We will now repair \"+str(recleanedGREEN.filter(col('tsFlag') == 1).count())+\" backwards values\")\n",
    "\n",
    "# Put dropoff values into a temporary column\n",
    "recleanedGREEN = recleanedGREEN.withColumn('dropoff_temp',col('lpep_dropoff_datetime'))\n",
    "\n",
    "# Move pickup values into dropoff when switch flag is 1\n",
    "recleanedGREEN = recleanedGREEN.withColumn('lpep_dropoff_datetime',sql.when(col('tsFlag') == 1,col('lpep_pickup_datetime')).\\\n",
    "                                           otherwise(col('lpep_dropoff_datetime')))\n",
    "# Move dropoff values from temporary column into pickup when switch flag is 1\n",
    "recleanedGREEN = recleanedGREEN.withColumn('lpep_pickup_datetime',sql.when(col('tsFlag') == 1,col('dropoff_temp')).\\\n",
    "                                           otherwise(col('lpep_pickup_datetime')))\n",
    "\n",
    "# Drop temporary columns\n",
    "recleanedGREEN = recleanedGREEN.drop(*['tsFlag','dropoff_temp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a total of 450 errors caught and 450 errors dealt with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have essentially isolated and fixed each category of error which our filters picked up. While we still have further errors in other variables, our main set and the set relevant for the query has been fully analysed. We have replaced incorrect and irreparable values with Null, and repaired those which we could. We can now re-merge our recleanedGREEN dataset with the other data.  \n",
    "\n",
    "Let's start by checking how many complete (for 2.4 purposes) records we can extract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have managed to repair 1244 complete records\n"
     ]
    }
   ],
   "source": [
    "# We've set irreparable values as Null, so...\n",
    "print(\"We have managed to repair \"+str(recleanedGREEN.filter(\\\n",
    "                                                             (col('lpep_pickup_datetime').isNotNull()) &\\\n",
    "                                                             (col('lpep_dropoff_datetime').isNotNull()) &\\\n",
    "                                                             (col('dolocationid').isNotNull()) &\\\n",
    "                                                             (col('pulocationid').isNotNull()) &\\\n",
    "                                                             (col('trip_distance').isNotNull()) &\\\n",
    "                                                             (col('total_amount').isNotNull()) &\\\n",
    "                                                             (col('tip_amount').isNotNull()) &\n",
    "                                                             (col('payment_type').isNotNull())\\\n",
    "                                                             ).count())+\" complete records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this is approximately 24% out of all of the dirty records we discovered, to have 1244 complete and usable additions is an acceptable result. The question is then what we do with the remaining incomplete records. Given that this represents ~2% of the raw dataset, it would be nice to include them. However, we would then have a changing overall database size across queries. Our results would therefore be inconsistent across queries.  \n",
    "\n",
    "We thus proceed by returning these fully cleaned values to their respective datasets, and storing the 3933 dirty ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us filter for the values we want\n",
    "everdirtyGREEN = recleanedGREEN.filter(\\\n",
    "                                     (col('lpep_pickup_datetime').isNull()) |\\\n",
    "                                     (col('lpep_dropoff_datetime').isNull()) |\\\n",
    "                                     (col('dolocationid').isNull()) |\\\n",
    "                                     (col('pulocationid').isNull()) |\\\n",
    "                                     (col('trip_distance').isNull()) |\\\n",
    "                                     (col('total_amount').isNull()) |\\\n",
    "                                     (col('tip_amount').isNull()) |\\\n",
    "                                     (col('payment_type').isNull()))\n",
    "\n",
    "recleanedGREEN = recleanedGREEN.filter(\\\n",
    "                                     (col('lpep_pickup_datetime').isNotNull()) &\\\n",
    "                                     (col('lpep_dropoff_datetime').isNotNull()) &\\\n",
    "                                     (col('dolocationid').isNotNull()) &\\\n",
    "                                     (col('pulocationid').isNotNull()) &\\\n",
    "                                     (col('trip_distance').isNotNull()) &\\\n",
    "                                     (col('total_amount').isNotNull()) &\\\n",
    "                                     (col('tip_amount').isNotNull()) &\\\n",
    "                                     (col('payment_type').isNotNull()))\n",
    "\n",
    "# Before re-integration, we must remember that we have re-cast a lot of columns as floats and the rest of the data is StringType\n",
    "everdirtyGREEN = everdirtyGREEN.select([col(i).cast(\"string\") for i in everdirtyGREEN.columns])\n",
    "recleanedGREEN = recleanedGREEN.select([col(i).cast(\"string\") for i in recleanedGREEN.columns])\n",
    "\n",
    "# We can then drop the validation columns\n",
    "recleanedGREEN = recleanedGREEN.drop(*['pulocationid_val',\\\n",
    "                                   'dolocationid_val',\\\n",
    "                                   'lpep_pickup_datetime_val',\\\n",
    "                                   'lpep_dropoff_datetime_val',\\\n",
    "                                   'vendorid_val',\\\n",
    "                                   'improvement_surcharge_val',\\\n",
    "                                   'mta_tax_val',\\\n",
    "                                   'extra_val',\\\n",
    "                                   'fare_amount_val',\\\n",
    "                                   'tip_amount_val',\\\n",
    "                                   'tolls_amount_val',\\\n",
    "                                   'total_amount_val',\\\n",
    "                                   'store_and_fwd_flag_val',\\\n",
    "                                   'trip_distance_val',\\\n",
    "                                   'passenger_count_val',\\\n",
    "                                   'trip_type_val',\\\n",
    "                                   'payment_type_val',\\\n",
    "                                   'ratecodeid_val',\\\n",
    "                                   'ehail_fee_val',\\\n",
    "                                   'congestion_surcharge_val',\\\n",
    "                                   'row_val'])\n",
    "\n",
    "# leaving us with just the filename variable as something which differentiates the schema from our clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GREEN_combine(cleanedupGREENDF,alwaysdirtyGREENDF):\n",
    "    \n",
    "    reAdded = 0\n",
    "    repair_filepaths = [xxx.filename for xxx in recleanedGREEN.select('filename').distinct().collect()]\n",
    "\n",
    "    for i in range(0,len(repair_filepaths)):\n",
    "        repair_filepaths[i] = clean_GREEN_directory+\"/all CSV/\"+repair_filepaths[i]+\"clean\"\n",
    "    \n",
    "    repair_filepaths.sort()\n",
    "    for i in repair_filepaths:\n",
    "        tempFilterDF = recleanedGREEN.filter(col('filename') == i.replace(clean_GREEN_directory,\"\").\\\n",
    "                                                                      replace('/all CSV/',\"\").\\\n",
    "                                                                      replace('csvclean','csv'))\n",
    "        dfi = create_df(i)\n",
    "        pre_existing = dfi.count()\n",
    "\n",
    "        # We do not repair duplicates here as it is too expensive for the cluster and catches 0 results\n",
    "        \n",
    "        if pre_existing == 0:\n",
    "            dfi = tempFilterDF    \n",
    "        else:\n",
    "            dfi = dfi.unionByName(tempFilterDF.drop('filename'))\n",
    "        addedDFI = dfi.count()-pre_existing\n",
    "        print(str(addedDFI)+\" additional records added\")\n",
    "        reAdded = reAdded + addedDFI\n",
    "        save_df(dfi,ntpath.basename(i), clean_GREEN_directory)\n",
    "\n",
    "    print(str(reAdded)+\" records added across all files\")\n",
    "    \n",
    "    if reAdded == 0:\n",
    "        print(\"WARNING: REPAIRED RECORDS COULD NOT BE RE-ADDED - ORIGINAL FILE WAS EMPTY\")\n",
    "    \n",
    "    # We have the dirty records saved on a monthly basis already anyway\n",
    "    # We will therefore save everdirtyGREEN alone\n",
    "    # We will then have saved files per month with caught dirty records and one master file with unrecoverable records\n",
    "    # This will record unrecoverable values, but will not take hours to run\n",
    "    # We do not coalesce() here as the file is large (and we will not be reading it again outside of spark)\n",
    "    \n",
    "    alwaysdirtyGREENDF.write.options(header = True).mode(\"overwrite\").\\\n",
    "         csv(os.path.join(dirty_GREEN_directory, \"irreparable_GREEN_records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2013-08.csvclean\n",
      "1 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2013-09.csvclean\n",
      "4 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2013-10.csvclean\n",
      "8 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2013-11.csvclean\n",
      "9 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2013-12.csvclean\n",
      "5 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2014-01.csvclean\n",
      "14 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2014-02.csvclean\n",
      "17 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2014-03.csvclean\n",
      "27 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2014-04.csvclean\n",
      "9 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2014-05.csvclean\n",
      "10 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2014-06.csvclean\n",
      "13 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2014-07.csvclean\n",
      "19 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2014-08.csvclean\n",
      "14 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2014-09.csvclean\n",
      "18 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2014-10.csvclean\n",
      "18 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2014-11.csvclean\n",
      "15 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2014-12.csvclean\n",
      "15 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2015-01.csvclean\n",
      "17 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2015-02.csvclean\n",
      "12 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2015-03.csvclean\n",
      "18 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2015-04.csvclean\n",
      "16 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2015-05.csvclean\n",
      "15 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2015-06.csvclean\n",
      "13 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2015-07.csvclean\n",
      "13 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2015-08.csvclean\n",
      "14 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2015-09.csvclean\n",
      "17 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2015-10.csvclean\n",
      "13 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2015-11.csvclean\n",
      "18 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2015-12.csvclean\n",
      "20 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2016-01.csvclean\n",
      "11 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2016-02.csvclean\n",
      "13 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2016-03.csvclean\n",
      "15 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2016-04.csvclean\n",
      "17 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2016-05.csvclean\n",
      "10 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2016-06.csvclean\n",
      "10 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2016-07.csvclean\n",
      "8 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2016-08.csvclean\n",
      "10 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2016-09.csvclean\n",
      "8 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2016-10.csvclean\n",
      "8 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2016-11.csvclean\n",
      "9 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2016-12.csvclean\n",
      "7 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2017-01.csvclean\n",
      "5 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2017-02.csvclean\n",
      "6 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2017-03.csvclean\n",
      "13 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2017-04.csvclean\n",
      "4 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2017-05.csvclean\n",
      "10 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2017-06.csvclean\n",
      "7 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2017-07.csvclean\n",
      "12 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2017-08.csvclean\n",
      "5 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2017-09.csvclean\n",
      "11 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2017-10.csvclean\n",
      "7 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2017-11.csvclean\n",
      "11 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2017-12.csvclean\n",
      "3 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2018-01.csvclean\n",
      "8 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2018-02.csvclean\n",
      "3 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2018-03.csvclean\n",
      "6 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2018-04.csvclean\n",
      "9 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2018-05.csvclean\n",
      "4 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2018-06.csvclean\n",
      "8 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2018-07.csvclean\n",
      "8 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2018-08.csvclean\n",
      "3 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2018-09.csvclean\n",
      "7 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2018-10.csvclean\n",
      "6 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2018-11.csvclean\n",
      "9 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2018-12.csvclean\n",
      "9 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2019-01.csvclean\n",
      "9 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2019-02.csvclean\n",
      "7 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2019-03.csvclean\n",
      "7 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2019-04.csvclean\n",
      "4 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2019-05.csvclean\n",
      "5 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2019-06.csvclean\n",
      "6 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2020-01.csvclean\n",
      "229 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2020-02.csvclean\n",
      "161 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2020-04.csvclean\n",
      "21 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2020-05.csvclean\n",
      "47 additional records added\n",
      "opened /home/epb123/output/cleaned Green/all CSV/green_tripdata_2020-06.csvclean\n",
      "46 additional records added\n",
      "1244 records added across all files\n"
     ]
    }
   ],
   "source": [
    "GREEN_combine(recleanedGREEN, everdirtyGREEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/output/cleaned Green/all CSV\n"
     ]
    }
   ],
   "source": [
    "# Clean the PySpark outputs from before\n",
    "GREEN_cleanCSV_folderPath = cleanup(clean_GREEN_directory)\n",
    "with open(v_direc + \"GREEN_cleanCSV_folderPath\",'wb') as cleanedGREENCSVdirec:\n",
    "    pickle.dump(GREEN_cleanCSV_folderPath,cleanedGREENCSVdirec)\n",
    "\n",
    "# GREEN_dirtyCSV_folderPath = cleanup(dirty_GREEN_directory)\n",
    "# with open(v_direc + \"GREEN_dirtyCSV_folderPath\",'wb') as dirtyGREENCSVdirec:\n",
    "#     pickle.dump(GREEN_dirtyCSV_folderPath,dirtyGREENCSVdirec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "del recleanedGREEN\n",
    "del everdirtyGREEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cleaning the YELLOW dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values and validity constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "vendorid: value is an int either 1 or 2  \n",
    "passenger_count: an int (above 10 is suspicious) \n",
    "ratecodeid: an int between 1 and 6  \n",
    "store_and_fwd_flag: string \"Y\" or \"N\"  \n",
    "payment_type: an int between 1 and 6  \n",
    "fare_amount: a float  \n",
    "extra: a float either 0, 0.50, 1.00 \n",
    "mta_tax: a float either 0 or 0.50  \n",
    "improvement_surcharge: a float 0.30 ONLY 2015 ONWARDS  \n",
    "tolls_amount: a float  \n",
    "congestion_surcharge: does not appear in data dictionary!!! seems to need 2.75 or 0\n",
    "\n",
    "tpep_pickup_datetime: 2020-06-24 04:38:00  \n",
    "tpep_dropoff_datetime: 2020-06-24 04:38:00  \n",
    "trip_distance: a float, >0 a trip with a distance of 0 is not, by definition, a trip  \n",
    "pulocationid: int 1 to 263  \n",
    "dolocationid: int 1 to 263 but can be imputed from the ratecodeid  \n",
    "tip_amount: a float, >0 we don't want to add further validity constraints here because you never know when a youtuber will get into a new york taxi looking for content. We have flagged it as worthy of investigating if it is >1000  \n",
    "total_amount: a float >0 and must approximately equal the sum of the other values (a >1% difference between the two figires will be flagged for investigation) - can be imputed from those values if missing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have minor differences between the green and yellow datasets - namely the renaming of the lpep to tpep, and the absense of ehail and trip_type - but they are minor enough that we can use similar functions to those we developed for Green."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and splitting off dirty records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/output/cleaned YELLOW\n",
      "folder already exists at /home/epb123/output/dirty YELLOW\n"
     ]
    }
   ],
   "source": [
    "clean_YELLOW_directory = create_folder(\"cleaned YELLOW\", save_directory)\n",
    "with open(v_direc + \"clean_YELLOW_directory\",'wb') as cleanedYELLOWdirec:\n",
    "    pickle.dump(clean_YELLOW_directory,cleanedYELLOWdirec)\n",
    "    \n",
    "dirty_YELLOW_directory = create_folder(\"dirty YELLOW\", save_directory)\n",
    "with open(v_direc + \"dirty_YELLOW_directory\",'wb') as dirtyYELLOWdirec:\n",
    "    pickle.dump(dirty_YELLOW_directory,dirtyYELLOWdirec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yellow_cleanup():\n",
    "# takes no arguments - splits Yellow into clean and dirty data\n",
    "# returns a pyspark dataframe with dirty data\n",
    "\n",
    "    dropped_duplicates_count = 0\n",
    "    clean_count = 0\n",
    "    row_count = 0\n",
    "    \n",
    "    dirtyLst = []\n",
    "    missingLst = []\n",
    "            \n",
    "    for i in yellow_integratedPaths:\n",
    "        dfi = create_df(i)\n",
    "        dfi_row_count = dfi.count()\n",
    "        row_count = row_count + dfi_row_count\n",
    "\n",
    "        # Adding validation columns\n",
    "        # Note - for YELLOW, the UDFs seem to be very slow.\n",
    "        # In future, further spark native functions should be used\n",
    "        dfi = add_location_validation(dfi) \n",
    "        dfi = add_YELLOW_datetime_validation(dfi)\n",
    "        dfi = add_vendorid_validation(dfi)\n",
    "        dfi = add_improvement_surcharge_validation(dfi,i)\n",
    "        dfi = add_mta_tax_validation(dfi)\n",
    "        dfi = add_extra_validation(dfi)\n",
    "        dfi = add_fare_amount_validation(dfi)\n",
    "        dfi = add_tip_amount_validation(dfi) \n",
    "        dfi = add_tolls_amount_validation(dfi) \n",
    "        dfi = add_YELLOW_total_amount_validation(dfi)\n",
    "        dfi = add_store_and_forward_flag_validation(dfi)\n",
    "        dfi = add_trip_distance_validation(dfi)\n",
    "        dfi = add_passenger_count_validation(dfi)\n",
    "        dfi = add_payment_type_validation(dfi)\n",
    "        dfi = add_ratecodeID_validation(dfi)\n",
    "        dfi = add_YELLOW_congestion_surcharge_validation(dfi)\n",
    "        dfi = add_YELLOW_row_validation(dfi)\n",
    "\n",
    "        # Store records which are not dirty (this does not consider variables useless for 2.4 completion)\n",
    "        # We also want to keep records which we can use - namely those with a pair of locationID values\n",
    "        dfiClean = dfi.filter((dfi.row_val == 0))\n",
    "        \n",
    "        dfiClean = dfiClean.drop(*['pulocationid_val',\\\n",
    "                                   'dolocationid_val',\\\n",
    "                                   'tpep_pickup_datetime_val',\\\n",
    "                                   'tpep_dropoff_datetime_val',\\\n",
    "                                   'vendorid_val',\\\n",
    "                                   'improvement_surcharge_val',\\\n",
    "                                   'mta_tax_val',\\\n",
    "                                   'extra_val',\\\n",
    "                                   'fare_amount_val',\\\n",
    "                                   'tip_amount_val',\\\n",
    "                                   'tolls_amount_val',\\\n",
    "                                   'total_amount_val',\\\n",
    "                                   'store_and_fwd_flag_val',\\\n",
    "                                   'trip_distance_val',\\\n",
    "                                   'passenger_count_val',\\\n",
    "                                   'payment_type_val',\\\n",
    "                                   'ratecodeid_val',\\\n",
    "                                   'congestion_surcharge_val',\\\n",
    "                                   'row_val'])\n",
    "\n",
    "        # We can drop duplicates in the clean dataset here - they will all have pickup and dropoff times or locations\n",
    "        # The times are stored to the millisecond and PySpark's drop_duplicates compares all columns\n",
    "        # If two records have pickup and dropoff times identical to the millisecond, we can safely drop them\n",
    "        # If two records have both location ids identical and identical dispatch numbers, we can also safely drop them\n",
    "        duplicates_result = drop_dupes(dfiClean)\n",
    "        dfiClean = duplicates_result[0]\n",
    "\n",
    "        dropped_duplicates_count = dropped_duplicates_count + duplicates_result[1]\n",
    "\n",
    "        dfiClean_row_count = duplicates_result[2]\n",
    "        clean_count = clean_count + dfiClean_row_count\n",
    "\n",
    "        # Save the cleaned dataframes for repairing later\n",
    "        save_df(dfiClean,ntpath.basename(i) + 'clean', clean_YELLOW_directory)\n",
    "        \n",
    "        dirtyInI = dfi.filter((dfi.row_val == 1) | (dfi.row_val == 2))\\\n",
    "            .withColumn(\"filename\", lit(ntpath.basename(i)))\n",
    "        \n",
    "        # We then add the dirty records to a spark dataframe, with an additional column specifying file of origin\n",
    "        if i == yellow_integratedPaths[0]:\n",
    "            dfiDirty = dirtyInI\n",
    "\n",
    "        else:\n",
    "            dfiDirty = dfiDirty.unionByName(dirtyInI)\n",
    "\n",
    "        dirtyInI = dirtyInI.withColumn(\"filename\", col('filename').cast(\"string\"))\n",
    "        save_df(dirtyInI,ntpath.basename(i) + 'dirty', dirty_YELLOW_directory)\n",
    "        \n",
    "        print(str(dfiClean_row_count) + ' clean records preserved out of ' + str(dfi_row_count) + \\\n",
    "              ' original records - ' + str(dfiClean_row_count/dfi_row_count*100)[0:4] + \"%\")\n",
    "        print(str(dirtyInI.filter(dirtyInI.row_val == 1).count()) + \" invalid records have been found\")\n",
    "        print(str(dirtyInI.filter(dirtyInI.row_val == 2).count()) + \" records with missing data have been found\")\n",
    "        \n",
    "        # We can use Pandas to keep track of what's going on - this really slows down the code, but statistics are nice.\n",
    "        # Note: if you comment this out to make the code run faster, do not forget to delete the return\n",
    "\n",
    "        b = dfi.filter(col('tpep_dropoff_datetime_val')==2).count()\n",
    "        c = dfi.filter(col('tpep_pickup_datetime_val')==2).count()\n",
    "        d = dfi.filter(col('dolocationid_val')==2).count()\n",
    "        e = dfi.filter(col('pulocationid_val')==2).count()\n",
    "        f = dfi.filter(col('vendorid_val')==2).count()\n",
    "        g = dfi.filter(col('ratecodeid_val')==2).count()\n",
    "        h = dfi.filter(col('payment_type_val')==2).count()\n",
    "        j = dfi.filter(col('store_and_fwd_flag_val')==2).count()\n",
    "        k = dfi.filter(col('trip_distance_val')==2).count()\n",
    "        l = dfi.filter(col('passenger_count_val')==2).count()\n",
    "        m = dfi.filter(col('tip_amount_val')==2).count()\n",
    "        n = dfi.filter(col('fare_amount_val')==2).count()\n",
    "        o = dfi.filter(col('total_amount_val')==2).count()\n",
    "        p = dfi.filter(col('improvement_surcharge_val')==2).count()\n",
    "        q = dfi.filter(col('extra_val')==2).count()\n",
    "        r = dfi.count()\n",
    "        s = dfi.filter(col('mta_tax_val')==2).count()\n",
    "        \n",
    "        u = dfi.filter(col('tolls_amount_val')==2).count()\n",
    "        w = dfi.filter(col('congestion_surcharge_val')==2).count()\n",
    "        x = dfi.filter(col('row_val')==2).count()\n",
    "        \n",
    "        missingLst.append([i,r,b,c,d,e,f,g,h,j,k,l,m,n,o,p,q,s,u,w,x])\n",
    "        \n",
    "        b1 = dfi.filter(col('tpep_dropoff_datetime_val')==1).count()\n",
    "        c1 = dfi.filter(col('tpep_pickup_datetime_val')==1).count()\n",
    "        d1 = dfi.filter(col('dolocationid_val')==1).count()\n",
    "        e1 = dfi.filter(col('pulocationid_val')==1).count()\n",
    "        f1 = dfi.filter(col('vendorid_val')==1).count()\n",
    "        g1 = dfi.filter(col('ratecodeid_val')==1).count()\n",
    "        h1 = dfi.filter(col('payment_type_val')==1).count()\n",
    "        j1 = dfi.filter(col('store_and_fwd_flag_val')==1).count()\n",
    "        k1 = dfi.filter(col('trip_distance_val')==1).count()\n",
    "        l1 = dfi.filter(col('passenger_count_val')==1).count()\n",
    "        m1 = dfi.filter(col('tip_amount_val')==1).count()\n",
    "        n1 = dfi.filter(col('fare_amount_val')==1).count()\n",
    "        o1 = dfi.filter(col('total_amount_val')==1).count()\n",
    "        p1 = dfi.filter(col('improvement_surcharge_val')==1).count()\n",
    "        q1 = dfi.filter(col('extra_val')==1).count()\n",
    "        r1 = dfi.count()\n",
    "        s1 = dfi.filter(col('mta_tax_val')==1).count()\n",
    "        \n",
    "        u1 = dfi.filter(col('tolls_amount_val')==1).count()\n",
    "        w1 = dfi.filter(col('congestion_surcharge_val')==1).count()\n",
    "        x1 = dfi.filter(col('row_val')==1).count()\n",
    "        \n",
    "        dirtyLst.append([i,r1,b1,c1,d1,e1,f1,g1,h1,j1,k1,l1,m1,n1,o1,p1,q1,s1,u1,w1,x1])\n",
    "    \n",
    "    PdfMissing=pd.DataFrame(missingLst,columns=['file','number of lines','tpep_dropoff_datetime_val','tpep_pickup_datetime_val',\\\n",
    "                                 'dolocationid_val','pulocationid_val','vendorid_val','ratecodeid_val',\\\n",
    "                                 'payment_type_val','store_and_fwd_flag_val','trip_distance_val',\\\n",
    "                                 'passenger_count_val', 'tip_amount_val','fare_amount_val','total_amount_val',\\\n",
    "                                 'improvement_surcharge_val','extra_val','mta_tax_val','tolls_amount_val',\\\n",
    "                                 'congestion_surcharge_val','row_val'])\n",
    "    \n",
    "    #Only the columns with at least one value different from zero are kept.\n",
    "    PdfMissing = PdfMissing.loc[:, (PdfMissing != 0).any(axis=0)] \n",
    "\n",
    "    PdfDirty=pd.DataFrame(dirtyLst,columns=['file','number of lines','tpep_dropoff_datetime_val','tpep_pickup_datetime_val',\\\n",
    "                             'dolocationid_val','pulocationid_val','vendorid_val','ratecodeid_val',\\\n",
    "                             'payment_type_val','store_and_fwd_flag_val','trip_distance_val',\\\n",
    "                             'passenger_count_val', 'tip_amount_val','fare_amount_val','total_amount_val',\\\n",
    "                             'improvement_surcharge_val','extra_val','mta_tax_val','tolls_amount_val',\\\n",
    "                             'congestion_surcharge_val','row_val'])\n",
    "\n",
    "    #Only the columns with at least one value different from zero are kept.\n",
    "    PdfDirty = PdfDirty.loc[:, (PdfDirty != 0).any(axis=0)] \n",
    "        \n",
    "    print(\"Out of \" + str(row_count) + \" original records, \" + str(clean_count) + \" records were already clean enough to use in queries.\")\n",
    "    print(str(dropped_duplicates_count) + \" records were dropped as duplicates.\")\n",
    "    print(str(row_count - clean_count - dropped_duplicates_count) \\\n",
    "          + \" dirty records remain for resolution.\")\n",
    "    \n",
    "    return dfiDirty, PdfMissing, PdfDirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2009-01.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 28187 original records - 0.0%\n",
      "28187 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2009-02.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 26763 original records - 0.0%\n",
      "26763 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2009-03.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 28782 original records - 0.0%\n",
      "28782 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2009-04.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 28599 original records - 0.0%\n",
      "28599 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2009-05.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29606 original records - 0.0%\n",
      "29606 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2009-06.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 28375 original records - 0.0%\n",
      "28375 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2009-07.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 27260 original records - 0.0%\n",
      "27260 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2009-08.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 27377 original records - 0.0%\n",
      "27377 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2009-09.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 27974 original records - 0.0%\n",
      "27974 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2009-10.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 31216 original records - 0.0%\n",
      "31216 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2009-11.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 28560 original records - 0.0%\n",
      "28560 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2009-12.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29175 original records - 0.0%\n",
      "29175 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2010-01.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29739 original records - 0.0%\n",
      "29739 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2010-02.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 22291 original records - 0.0%\n",
      "22291 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2010-03.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 25768 original records - 0.0%\n",
      "25768 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2010-04.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 30298 original records - 0.0%\n",
      "30298 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2010-05.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 30969 original records - 0.0%\n",
      "30969 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2010-06.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29658 original records - 0.0%\n",
      "29658 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2010-07.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29321 original records - 0.0%\n",
      "29321 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2010-08.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 25056 original records - 0.0%\n",
      "25056 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2010-09.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 31085 original records - 0.0%\n",
      "31085 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2010-10.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 28401 original records - 0.0%\n",
      "28401 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2010-11.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 27826 original records - 0.0%\n",
      "27826 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2010-12.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 27638 original records - 0.0%\n",
      "27638 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2011-01.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 26932 original records - 0.0%\n",
      "26932 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2011-02.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 28411 original records - 0.0%\n",
      "28411 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2011-03.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 32150 original records - 0.0%\n",
      "32150 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2011-04.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29443 original records - 0.0%\n",
      "29443 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2011-05.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 31115 original records - 0.0%\n",
      "31115 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2011-06.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 30201 original records - 0.0%\n",
      "30201 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2011-07.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29490 original records - 0.0%\n",
      "29490 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2011-08.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 26529 original records - 0.0%\n",
      "26529 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2011-09.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29261 original records - 0.0%\n",
      "29261 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2011-10.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 31423 original records - 0.0%\n",
      "31423 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2011-11.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29062 original records - 0.0%\n",
      "29062 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2011-12.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29861 original records - 0.0%\n",
      "29861 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2012-01.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29943 original records - 0.0%\n",
      "29943 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2012-02.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29971 original records - 0.0%\n",
      "29971 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2012-03.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 32300 original records - 0.0%\n",
      "32300 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2012-04.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 30959 original records - 0.0%\n",
      "30959 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2012-05.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 31142 original records - 0.0%\n",
      "31142 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2012-06.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 30197 original records - 0.0%\n",
      "30197 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2012-07.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 28767 original records - 0.0%\n",
      "28767 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2012-08.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 28771 original records - 0.0%\n",
      "28771 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2012-09.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29098 original records - 0.0%\n",
      "29098 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2012-10.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29056 original records - 0.0%\n",
      "29056 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2012-11.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 27554 original records - 0.0%\n",
      "27554 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2012-12.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29401 original records - 0.0%\n",
      "29401 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-01.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29556 original records - 0.0%\n",
      "29556 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-02.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 27988 original records - 0.0%\n",
      "27988 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-03.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 31501 original records - 0.0%\n",
      "31501 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-04.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 30202 original records - 0.0%\n",
      "30202 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-05.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 30575 original records - 0.0%\n",
      "30575 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-06.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 28767 original records - 0.0%\n",
      "28767 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-07.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 27642 original records - 0.0%\n",
      "27642 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-08.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 25190 original records - 0.0%\n",
      "25190 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-09.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 28221 original records - 0.0%\n",
      "28221 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-10.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 30008 original records - 0.0%\n",
      "30008 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-11.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 28786 original records - 0.0%\n",
      "28786 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-12.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 27941 original records - 0.0%\n",
      "27941 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2014-01.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 27559 original records - 0.0%\n",
      "27559 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2014-02.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 26134 original records - 0.0%\n",
      "26134 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2014-03.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 30862 original records - 0.0%\n",
      "30862 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2014-04.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29250 original records - 0.0%\n",
      "29250 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2014-05.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 29554 original records - 0.0%\n",
      "29554 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2014-06.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 27623 original records - 0.0%\n",
      "27623 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2014-07.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 26224 original records - 0.0%\n",
      "26224 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2014-08.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 25378 original records - 0.0%\n",
      "25378 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2014-09.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 26748 original records - 0.0%\n",
      "26748 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2014-10.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 28465 original records - 0.0%\n",
      "28465 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2014-11.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 26440 original records - 0.0%\n",
      "26440 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2014-12.csv\n",
      "0 duplicate entries removed\n",
      "0 clean records preserved out of 26029 original records - 0.0%\n",
      "26029 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2015-01.csv\n",
      "0 duplicate entries removed\n",
      "24588 clean records preserved out of 25472 original records - 96.5%\n",
      "884 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2015-02.csv\n",
      "0 duplicate entries removed\n",
      "24144 clean records preserved out of 24877 original records - 97.0%\n",
      "733 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2015-03.csv\n",
      "0 duplicate entries removed\n",
      "25868 clean records preserved out of 26683 original records - 96.9%\n",
      "815 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2015-04.csv\n",
      "0 duplicate entries removed\n",
      "25372 clean records preserved out of 26119 original records - 97.1%\n",
      "747 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2015-05.csv\n",
      "0 duplicate entries removed\n",
      "25570 clean records preserved out of 26294 original records - 97.2%\n",
      "724 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2015-06.csv\n",
      "0 duplicate entries removed\n",
      "23986 clean records preserved out of 24628 original records - 97.3%\n",
      "642 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2015-07.csv\n",
      "0 duplicate entries removed\n",
      "22551 clean records preserved out of 23102 original records - 97.6%\n",
      "551 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2015-08.csv\n",
      "0 duplicate entries removed\n",
      "21712 clean records preserved out of 22241 original records - 97.6%\n",
      "529 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2015-09.csv\n",
      "0 duplicate entries removed\n",
      "21864 clean records preserved out of 22427 original records - 97.4%\n",
      "563 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2015-10.csv\n",
      "0 duplicate entries removed\n",
      "23954 clean records preserved out of 24612 original records - 97.3%\n",
      "658 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2015-11.csv\n",
      "0 duplicate entries removed\n",
      "22014 clean records preserved out of 22606 original records - 97.3%\n",
      "592 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2015-12.csv\n",
      "0 duplicate entries removed\n",
      "22326 clean records preserved out of 22902 original records - 97.4%\n",
      "576 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2016-01.csv\n",
      "0 duplicate entries removed\n",
      "21227 clean records preserved out of 21799 original records - 97.3%\n",
      "572 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2016-02.csv\n",
      "0 duplicate entries removed\n",
      "22138 clean records preserved out of 22747 original records - 97.3%\n",
      "609 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2016-03.csv\n",
      "0 duplicate entries removed\n",
      "23725 clean records preserved out of 24399 original records - 97.2%\n",
      "674 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2016-04.csv\n",
      "0 duplicate entries removed\n",
      "23204 clean records preserved out of 23849 original records - 97.2%\n",
      "645 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2016-05.csv\n",
      "0 duplicate entries removed\n",
      "23082 clean records preserved out of 23655 original records - 97.5%\n",
      "573 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2016-06.csv\n",
      "0 duplicate entries removed\n",
      "21704 clean records preserved out of 22250 original records - 97.5%\n",
      "546 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2016-07.csv\n",
      "0 duplicate entries removed\n",
      "20019 clean records preserved out of 20589 original records - 97.2%\n",
      "570 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2016-08.csv\n",
      "0 duplicate entries removed\n",
      "19314 clean records preserved out of 19887 original records - 97.1%\n",
      "573 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2016-09.csv\n",
      "0 duplicate entries removed\n",
      "19647 clean records preserved out of 20238 original records - 97.0%\n",
      "591 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2016-10.csv\n",
      "0 duplicate entries removed\n",
      "21100 clean records preserved out of 21703 original records - 97.2%\n",
      "603 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2016-11.csv\n",
      "0 duplicate entries removed\n",
      "19640 clean records preserved out of 20211 original records - 97.1%\n",
      "571 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2016-12.csv\n",
      "0 duplicate entries removed\n",
      "20221 clean records preserved out of 20897 original records - 96.7%\n",
      "676 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2017-01.csv\n",
      "0 duplicate entries removed\n",
      "18838 clean records preserved out of 19409 original records - 97.0%\n",
      "571 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2017-02.csv\n",
      "0 duplicate entries removed\n",
      "17770 clean records preserved out of 18330 original records - 96.9%\n",
      "560 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2017-03.csv\n",
      "0 duplicate entries removed\n",
      "19961 clean records preserved out of 20589 original records - 96.9%\n",
      "628 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2017-04.csv\n",
      "0 duplicate entries removed\n",
      "19507 clean records preserved out of 20091 original records - 97.0%\n",
      "584 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2017-05.csv\n",
      "0 duplicate entries removed\n",
      "19625 clean records preserved out of 20205 original records - 97.1%\n",
      "580 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2017-06.csv\n",
      "0 duplicate entries removed\n",
      "18751 clean records preserved out of 19302 original records - 97.1%\n",
      "551 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2017-07.csv\n",
      "0 duplicate entries removed\n",
      "16672 clean records preserved out of 17168 original records - 97.1%\n",
      "496 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2017-08.csv\n",
      "0 duplicate entries removed\n",
      "16341 clean records preserved out of 16831 original records - 97.0%\n",
      "490 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2017-09.csv\n",
      "0 duplicate entries removed\n",
      "17406 clean records preserved out of 17886 original records - 97.3%\n",
      "480 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2017-10.csv\n",
      "0 duplicate entries removed\n",
      "18974 clean records preserved out of 19532 original records - 97.1%\n",
      "558 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2017-11.csv\n",
      "0 duplicate entries removed\n",
      "17958 clean records preserved out of 18564 original records - 96.7%\n",
      "606 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2017-12.csv\n",
      "0 duplicate entries removed\n",
      "18397 clean records preserved out of 19008 original records - 96.7%\n",
      "611 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2018-01.csv\n",
      "0 duplicate entries removed\n",
      "16970 clean records preserved out of 17510 original records - 96.9%\n",
      "540 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2018-02.csv\n",
      "0 duplicate entries removed\n",
      "16468 clean records preserved out of 16969 original records - 97.0%\n",
      "501 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2018-03.csv\n",
      "0 duplicate entries removed\n",
      "18269 clean records preserved out of 18852 original records - 96.9%\n",
      "583 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2018-04.csv\n",
      "0 duplicate entries removed\n",
      "18088 clean records preserved out of 18606 original records - 97.2%\n",
      "518 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2018-05.csv\n",
      "0 duplicate entries removed\n",
      "17935 clean records preserved out of 18443 original records - 97.2%\n",
      "508 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2018-06.csv\n",
      "0 duplicate entries removed\n",
      "16911 clean records preserved out of 17421 original records - 97.0%\n",
      "510 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2018-07.csv\n",
      "0 duplicate entries removed\n",
      "15211 clean records preserved out of 15686 original records - 96.9%\n",
      "475 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2018-08.csv\n",
      "0 duplicate entries removed\n",
      "15258 clean records preserved out of 15684 original records - 97.2%\n",
      "426 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2018-09.csv\n",
      "0 duplicate entries removed\n",
      "15602 clean records preserved out of 16062 original records - 97.1%\n",
      "460 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2018-10.csv\n",
      "0 duplicate entries removed\n",
      "17071 clean records preserved out of 17638 original records - 96.7%\n",
      "567 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2018-11.csv\n",
      "0 duplicate entries removed\n",
      "15724 clean records preserved out of 16270 original records - 96.6%\n",
      "546 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2018-12.csv\n",
      "0 duplicate entries removed\n",
      "15747 clean records preserved out of 16326 original records - 96.4%\n",
      "579 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2019-01.csv\n",
      "0 duplicate entries removed\n",
      "14771 clean records preserved out of 15324 original records - 96.3%\n",
      "553 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2019-02.csv\n",
      "0 duplicate entries removed\n",
      "9312 clean records preserved out of 14023 original records - 66.4%\n",
      "4711 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2019-03.csv\n",
      "0 duplicate entries removed\n",
      "10224 clean records preserved out of 15637 original records - 65.3%\n",
      "5413 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2019-04.csv\n",
      "0 duplicate entries removed\n",
      "9550 clean records preserved out of 14847 original records - 64.3%\n",
      "5297 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2019-05.csv\n",
      "0 duplicate entries removed\n",
      "9839 clean records preserved out of 15108 original records - 65.1%\n",
      "5269 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2019-06.csv\n",
      "0 duplicate entries removed\n",
      "9215 clean records preserved out of 13869 original records - 66.4%\n",
      "4654 invalid records have been found\n",
      "0 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2020-01.csv\n",
      "0 duplicate entries removed\n",
      "8544 clean records preserved out of 12810 original records - 66.6%\n",
      "4124 invalid records have been found\n",
      "142 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2020-02.csv\n",
      "0 duplicate entries removed\n",
      "8502 clean records preserved out of 12598 original records - 67.4%\n",
      "3990 invalid records have been found\n",
      "106 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2020-04.csv\n",
      "0 duplicate entries removed\n",
      "278 clean records preserved out of 476 original records - 58.4%\n",
      "166 invalid records have been found\n",
      "32 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2020-05.csv\n",
      "0 duplicate entries removed\n",
      "351 clean records preserved out of 696 original records - 50.4%\n",
      "236 invalid records have been found\n",
      "109 records with missing data have been found\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2020-06.csv\n",
      "0 duplicate entries removed\n",
      "609 clean records preserved out of 1099 original records - 55.4%\n",
      "398 invalid records have been found\n",
      "92 records with missing data have been found\n",
      "Out of 3170660 original records, 1039619 records were already clean enough to use in queries.\n",
      "0 records were dropped as duplicates.\n",
      "2131041 dirty records remain for resolution.\n"
     ]
    }
   ],
   "source": [
    "yellow_cleanup_return = yellow_cleanup()\n",
    "dirtyYELLOW = yellow_cleanup_return[0]\n",
    "missingYellow = yellow_cleanup_return[1]\n",
    "dirtyDetailedYellow = yellow_cleanup_return[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/output/cleaned YELLOW/all CSV\n",
      "folder already exists at /home/epb123/output/dirty YELLOW/all CSV\n"
     ]
    }
   ],
   "source": [
    "# Clean the PySpark outputs from before\n",
    "YELLOW_cleanCSV_folderPath = cleanup(clean_YELLOW_directory)\n",
    "with open(v_direc + \"YELLOW_cleanCSV_folderPath\",'wb') as cleanedYELLOWCSVdirec:\n",
    "    pickle.dump(YELLOW_cleanCSV_folderPath,cleanedYELLOWCSVdirec)\n",
    "\n",
    "YELLOW_dirtyCSV_folderPath = cleanup(dirty_YELLOW_directory)\n",
    "with open(v_direc + \"YELLOW_dirtyCSV_folderPath\",'wb') as dirtyYELLOWCSVdirec:\n",
    "    pickle.dump(YELLOW_dirtyCSV_folderPath,dirtyYELLOWCSVdirec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on filtering for records with all of the information needed for the queries, we appear to here have 2,131,041 records for cleaning - a number which might shock. Many of this could have come from the payment_type filter - a lot of early yellow records did not use correct payment types.\n",
    "\n",
    "Let us examine the data for now and see what can be repaired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>number of lines</th>\n",
       "      <th>tpep_dropoff_datetime_val</th>\n",
       "      <th>tpep_pickup_datetime_val</th>\n",
       "      <th>dolocationid_val</th>\n",
       "      <th>pulocationid_val</th>\n",
       "      <th>vendorid_val</th>\n",
       "      <th>ratecodeid_val</th>\n",
       "      <th>payment_type_val</th>\n",
       "      <th>store_and_fwd_flag_val</th>\n",
       "      <th>...</th>\n",
       "      <th>passenger_count_val</th>\n",
       "      <th>tip_amount_val</th>\n",
       "      <th>fare_amount_val</th>\n",
       "      <th>total_amount_val</th>\n",
       "      <th>improvement_surcharge_val</th>\n",
       "      <th>extra_val</th>\n",
       "      <th>mta_tax_val</th>\n",
       "      <th>tolls_amount_val</th>\n",
       "      <th>congestion_surcharge_val</th>\n",
       "      <th>row_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>28187</td>\n",
       "      <td>575</td>\n",
       "      <td>575</td>\n",
       "      <td>585</td>\n",
       "      <td>563</td>\n",
       "      <td>28187</td>\n",
       "      <td>0</td>\n",
       "      <td>28187</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>26763</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>571</td>\n",
       "      <td>554</td>\n",
       "      <td>26763</td>\n",
       "      <td>0</td>\n",
       "      <td>26763</td>\n",
       "      <td>8584</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>28782</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>613</td>\n",
       "      <td>578</td>\n",
       "      <td>28782</td>\n",
       "      <td>0</td>\n",
       "      <td>28782</td>\n",
       "      <td>10322</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>28599</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>622</td>\n",
       "      <td>613</td>\n",
       "      <td>28599</td>\n",
       "      <td>0</td>\n",
       "      <td>28599</td>\n",
       "      <td>12891</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>29606</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>637</td>\n",
       "      <td>657</td>\n",
       "      <td>29606</td>\n",
       "      <td>0</td>\n",
       "      <td>29606</td>\n",
       "      <td>13316</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>28375</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>462</td>\n",
       "      <td>460</td>\n",
       "      <td>28375</td>\n",
       "      <td>0</td>\n",
       "      <td>28375</td>\n",
       "      <td>12809</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>27260</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>462</td>\n",
       "      <td>451</td>\n",
       "      <td>27260</td>\n",
       "      <td>0</td>\n",
       "      <td>27260</td>\n",
       "      <td>12218</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>27377</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>482</td>\n",
       "      <td>482</td>\n",
       "      <td>27377</td>\n",
       "      <td>0</td>\n",
       "      <td>27377</td>\n",
       "      <td>12385</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>27974</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>482</td>\n",
       "      <td>494</td>\n",
       "      <td>27974</td>\n",
       "      <td>0</td>\n",
       "      <td>27974</td>\n",
       "      <td>12409</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>31216</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>612</td>\n",
       "      <td>608</td>\n",
       "      <td>31216</td>\n",
       "      <td>0</td>\n",
       "      <td>31216</td>\n",
       "      <td>13989</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>28560</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>687</td>\n",
       "      <td>680</td>\n",
       "      <td>28560</td>\n",
       "      <td>0</td>\n",
       "      <td>28560</td>\n",
       "      <td>12698</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>982</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>29175</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>647</td>\n",
       "      <td>649</td>\n",
       "      <td>29175</td>\n",
       "      <td>0</td>\n",
       "      <td>29175</td>\n",
       "      <td>12730</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>29739</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>674</td>\n",
       "      <td>666</td>\n",
       "      <td>29739</td>\n",
       "      <td>0</td>\n",
       "      <td>29739</td>\n",
       "      <td>13271</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>468</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>22291</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>519</td>\n",
       "      <td>511</td>\n",
       "      <td>22291</td>\n",
       "      <td>2</td>\n",
       "      <td>22291</td>\n",
       "      <td>12217</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>25768</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>668</td>\n",
       "      <td>655</td>\n",
       "      <td>25768</td>\n",
       "      <td>10</td>\n",
       "      <td>25768</td>\n",
       "      <td>13762</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>30298</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>883</td>\n",
       "      <td>886</td>\n",
       "      <td>30298</td>\n",
       "      <td>42</td>\n",
       "      <td>30298</td>\n",
       "      <td>13441</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>30969</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>927</td>\n",
       "      <td>908</td>\n",
       "      <td>30969</td>\n",
       "      <td>80</td>\n",
       "      <td>30969</td>\n",
       "      <td>13899</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>29658</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>862</td>\n",
       "      <td>852</td>\n",
       "      <td>29658</td>\n",
       "      <td>31</td>\n",
       "      <td>29658</td>\n",
       "      <td>13278</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>29321</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>896</td>\n",
       "      <td>881</td>\n",
       "      <td>29321</td>\n",
       "      <td>30</td>\n",
       "      <td>29321</td>\n",
       "      <td>13099</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>25056</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>768</td>\n",
       "      <td>751</td>\n",
       "      <td>25056</td>\n",
       "      <td>8</td>\n",
       "      <td>25056</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>31085</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>923</td>\n",
       "      <td>892</td>\n",
       "      <td>31085</td>\n",
       "      <td>7</td>\n",
       "      <td>31085</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>28401</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>815</td>\n",
       "      <td>807</td>\n",
       "      <td>28401</td>\n",
       "      <td>1</td>\n",
       "      <td>28401</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>27826</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>856</td>\n",
       "      <td>845</td>\n",
       "      <td>27826</td>\n",
       "      <td>6</td>\n",
       "      <td>27826</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>27638</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>736</td>\n",
       "      <td>704</td>\n",
       "      <td>27638</td>\n",
       "      <td>4</td>\n",
       "      <td>27638</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>26932</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>763</td>\n",
       "      <td>733</td>\n",
       "      <td>26932</td>\n",
       "      <td>2</td>\n",
       "      <td>26932</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>28411</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>939</td>\n",
       "      <td>950</td>\n",
       "      <td>28411</td>\n",
       "      <td>10</td>\n",
       "      <td>28411</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>253</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>32150</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1057</td>\n",
       "      <td>1070</td>\n",
       "      <td>32150</td>\n",
       "      <td>4</td>\n",
       "      <td>32150</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>318</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>29443</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>862</td>\n",
       "      <td>854</td>\n",
       "      <td>29443</td>\n",
       "      <td>19</td>\n",
       "      <td>29443</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>390</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>31115</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>834</td>\n",
       "      <td>827</td>\n",
       "      <td>31115</td>\n",
       "      <td>8</td>\n",
       "      <td>31115</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>451</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>30201</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>760</td>\n",
       "      <td>733</td>\n",
       "      <td>30201</td>\n",
       "      <td>7</td>\n",
       "      <td>30201</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>482</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>19302</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>267</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>55</td>\n",
       "      <td>13</td>\n",
       "      <td>99</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>17168</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>237</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>7</td>\n",
       "      <td>79</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>16831</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>246</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>88</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>17886</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>254</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>19532</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>316</td>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>86</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>18564</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>352</td>\n",
       "      <td>348</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>76</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>19008</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>348</td>\n",
       "      <td>333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>48</td>\n",
       "      <td>18</td>\n",
       "      <td>86</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>17510</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>303</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>16969</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>275</td>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>18852</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>342</td>\n",
       "      <td>321</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>18606</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>299</td>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "      <td>79</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>18443</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>234</td>\n",
       "      <td>236</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "      <td>10</td>\n",
       "      <td>80</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>17421</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>269</td>\n",
       "      <td>269</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>87</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>15686</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>245</td>\n",
       "      <td>240</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>15684</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>219</td>\n",
       "      <td>204</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>16</td>\n",
       "      <td>68</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>16062</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>236</td>\n",
       "      <td>232</td>\n",
       "      <td>165</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "      <td>76</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>17638</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>312</td>\n",
       "      <td>287</td>\n",
       "      <td>275</td>\n",
       "      <td>4</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>43</td>\n",
       "      <td>17</td>\n",
       "      <td>88</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>16270</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>285</td>\n",
       "      <td>281</td>\n",
       "      <td>292</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>9</td>\n",
       "      <td>66</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>16326</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>319</td>\n",
       "      <td>325</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>42</td>\n",
       "      <td>16</td>\n",
       "      <td>79</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>15324</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>337</td>\n",
       "      <td>325</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>222</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>47</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>14023</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>156</td>\n",
       "      <td>143</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>4486</td>\n",
       "      <td>24</td>\n",
       "      <td>4655</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>15637</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>156</td>\n",
       "      <td>116</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>279</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>5198</td>\n",
       "      <td>24</td>\n",
       "      <td>5407</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>5413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>14847</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>178</td>\n",
       "      <td>146</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>282</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>5057</td>\n",
       "      <td>23</td>\n",
       "      <td>5267</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>5297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>15108</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>207</td>\n",
       "      <td>138</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>291</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>4988</td>\n",
       "      <td>29</td>\n",
       "      <td>5217</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>5269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>13869</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>183</td>\n",
       "      <td>131</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>4410</td>\n",
       "      <td>27</td>\n",
       "      <td>4614</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>12810</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>143</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3867</td>\n",
       "      <td>29</td>\n",
       "      <td>4133</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>4124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>12598</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>95</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>3789</td>\n",
       "      <td>47</td>\n",
       "      <td>3996</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>476</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>156</td>\n",
       "      <td>2</td>\n",
       "      <td>151</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>696</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>215</td>\n",
       "      <td>2</td>\n",
       "      <td>210</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>1099</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>351</td>\n",
       "      <td>6</td>\n",
       "      <td>344</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  file  number of lines  \\\n",
       "0    /home/epb123/output/integrated Yellow/yellow_t...            28187   \n",
       "1    /home/epb123/output/integrated Yellow/yellow_t...            26763   \n",
       "2    /home/epb123/output/integrated Yellow/yellow_t...            28782   \n",
       "3    /home/epb123/output/integrated Yellow/yellow_t...            28599   \n",
       "4    /home/epb123/output/integrated Yellow/yellow_t...            29606   \n",
       "5    /home/epb123/output/integrated Yellow/yellow_t...            28375   \n",
       "6    /home/epb123/output/integrated Yellow/yellow_t...            27260   \n",
       "7    /home/epb123/output/integrated Yellow/yellow_t...            27377   \n",
       "8    /home/epb123/output/integrated Yellow/yellow_t...            27974   \n",
       "9    /home/epb123/output/integrated Yellow/yellow_t...            31216   \n",
       "10   /home/epb123/output/integrated Yellow/yellow_t...            28560   \n",
       "11   /home/epb123/output/integrated Yellow/yellow_t...            29175   \n",
       "12   /home/epb123/output/integrated Yellow/yellow_t...            29739   \n",
       "13   /home/epb123/output/integrated Yellow/yellow_t...            22291   \n",
       "14   /home/epb123/output/integrated Yellow/yellow_t...            25768   \n",
       "15   /home/epb123/output/integrated Yellow/yellow_t...            30298   \n",
       "16   /home/epb123/output/integrated Yellow/yellow_t...            30969   \n",
       "17   /home/epb123/output/integrated Yellow/yellow_t...            29658   \n",
       "18   /home/epb123/output/integrated Yellow/yellow_t...            29321   \n",
       "19   /home/epb123/output/integrated Yellow/yellow_t...            25056   \n",
       "20   /home/epb123/output/integrated Yellow/yellow_t...            31085   \n",
       "21   /home/epb123/output/integrated Yellow/yellow_t...            28401   \n",
       "22   /home/epb123/output/integrated Yellow/yellow_t...            27826   \n",
       "23   /home/epb123/output/integrated Yellow/yellow_t...            27638   \n",
       "24   /home/epb123/output/integrated Yellow/yellow_t...            26932   \n",
       "25   /home/epb123/output/integrated Yellow/yellow_t...            28411   \n",
       "26   /home/epb123/output/integrated Yellow/yellow_t...            32150   \n",
       "27   /home/epb123/output/integrated Yellow/yellow_t...            29443   \n",
       "28   /home/epb123/output/integrated Yellow/yellow_t...            31115   \n",
       "29   /home/epb123/output/integrated Yellow/yellow_t...            30201   \n",
       "..                                                 ...              ...   \n",
       "101  /home/epb123/output/integrated Yellow/yellow_t...            19302   \n",
       "102  /home/epb123/output/integrated Yellow/yellow_t...            17168   \n",
       "103  /home/epb123/output/integrated Yellow/yellow_t...            16831   \n",
       "104  /home/epb123/output/integrated Yellow/yellow_t...            17886   \n",
       "105  /home/epb123/output/integrated Yellow/yellow_t...            19532   \n",
       "106  /home/epb123/output/integrated Yellow/yellow_t...            18564   \n",
       "107  /home/epb123/output/integrated Yellow/yellow_t...            19008   \n",
       "108  /home/epb123/output/integrated Yellow/yellow_t...            17510   \n",
       "109  /home/epb123/output/integrated Yellow/yellow_t...            16969   \n",
       "110  /home/epb123/output/integrated Yellow/yellow_t...            18852   \n",
       "111  /home/epb123/output/integrated Yellow/yellow_t...            18606   \n",
       "112  /home/epb123/output/integrated Yellow/yellow_t...            18443   \n",
       "113  /home/epb123/output/integrated Yellow/yellow_t...            17421   \n",
       "114  /home/epb123/output/integrated Yellow/yellow_t...            15686   \n",
       "115  /home/epb123/output/integrated Yellow/yellow_t...            15684   \n",
       "116  /home/epb123/output/integrated Yellow/yellow_t...            16062   \n",
       "117  /home/epb123/output/integrated Yellow/yellow_t...            17638   \n",
       "118  /home/epb123/output/integrated Yellow/yellow_t...            16270   \n",
       "119  /home/epb123/output/integrated Yellow/yellow_t...            16326   \n",
       "120  /home/epb123/output/integrated Yellow/yellow_t...            15324   \n",
       "121  /home/epb123/output/integrated Yellow/yellow_t...            14023   \n",
       "122  /home/epb123/output/integrated Yellow/yellow_t...            15637   \n",
       "123  /home/epb123/output/integrated Yellow/yellow_t...            14847   \n",
       "124  /home/epb123/output/integrated Yellow/yellow_t...            15108   \n",
       "125  /home/epb123/output/integrated Yellow/yellow_t...            13869   \n",
       "126  /home/epb123/output/integrated Yellow/yellow_t...            12810   \n",
       "127  /home/epb123/output/integrated Yellow/yellow_t...            12598   \n",
       "128  /home/epb123/output/integrated Yellow/yellow_t...              476   \n",
       "129  /home/epb123/output/integrated Yellow/yellow_t...              696   \n",
       "130  /home/epb123/output/integrated Yellow/yellow_t...             1099   \n",
       "\n",
       "     tpep_dropoff_datetime_val  tpep_pickup_datetime_val  dolocationid_val  \\\n",
       "0                          575                       575               585   \n",
       "1                          201                       201               571   \n",
       "2                          111                       111               613   \n",
       "3                          120                       120               622   \n",
       "4                          120                       120               637   \n",
       "5                           97                        97               462   \n",
       "6                           78                        78               462   \n",
       "7                           93                        93               482   \n",
       "8                           99                        99               482   \n",
       "9                           96                        96               612   \n",
       "10                         111                       111               687   \n",
       "11                          76                        76               647   \n",
       "12                          70                        70               674   \n",
       "13                          40                        40               519   \n",
       "14                          57                        57               668   \n",
       "15                          79                        79               883   \n",
       "16                          72                        72               927   \n",
       "17                          85                        85               862   \n",
       "18                          73                        73               896   \n",
       "19                          47                        47               768   \n",
       "20                          41                        41               923   \n",
       "21                          53                        53               815   \n",
       "22                          60                        60               856   \n",
       "23                          59                        59               736   \n",
       "24                          42                        42               763   \n",
       "25                          94                        94               939   \n",
       "26                         100                       100              1057   \n",
       "27                          96                        96               862   \n",
       "28                          90                        90               834   \n",
       "29                          74                        74               760   \n",
       "..                         ...                       ...               ...   \n",
       "101                         41                        41               267   \n",
       "102                         29                        29               237   \n",
       "103                         25                        25               246   \n",
       "104                         36                        36               254   \n",
       "105                         35                        35               316   \n",
       "106                         34                        34               352   \n",
       "107                         39                        39               348   \n",
       "108                         26                        26               303   \n",
       "109                         29                        29               275   \n",
       "110                         37                        37               342   \n",
       "111                         23                        23               299   \n",
       "112                         19                        19               234   \n",
       "113                         23                        23               269   \n",
       "114                         30                        30               245   \n",
       "115                         23                        23               219   \n",
       "116                         28                        28               236   \n",
       "117                         34                        34               312   \n",
       "118                         32                        32               285   \n",
       "119                         25                        25               319   \n",
       "120                         31                        31               337   \n",
       "121                         27                        27               156   \n",
       "122                         27                        27               156   \n",
       "123                         16                        16               178   \n",
       "124                         32                        32               207   \n",
       "125                         28                        28               183   \n",
       "126                         16                        16               143   \n",
       "127                         17                        17                95   \n",
       "128                          1                         1                 7   \n",
       "129                          3                         3                11   \n",
       "130                          1                         1                13   \n",
       "\n",
       "     pulocationid_val  vendorid_val  ratecodeid_val  payment_type_val  \\\n",
       "0                 563         28187               0             28187   \n",
       "1                 554         26763               0             26763   \n",
       "2                 578         28782               0             28782   \n",
       "3                 613         28599               0             28599   \n",
       "4                 657         29606               0             29606   \n",
       "5                 460         28375               0             28375   \n",
       "6                 451         27260               0             27260   \n",
       "7                 482         27377               0             27377   \n",
       "8                 494         27974               0             27974   \n",
       "9                 608         31216               0             31216   \n",
       "10                680         28560               0             28560   \n",
       "11                649         29175               0             29175   \n",
       "12                666         29739               0             29739   \n",
       "13                511         22291               2             22291   \n",
       "14                655         25768              10             25768   \n",
       "15                886         30298              42             30298   \n",
       "16                908         30969              80             30969   \n",
       "17                852         29658              31             29658   \n",
       "18                881         29321              30             29321   \n",
       "19                751         25056               8             25056   \n",
       "20                892         31085               7             31085   \n",
       "21                807         28401               1             28401   \n",
       "22                845         27826               6             27826   \n",
       "23                704         27638               4             27638   \n",
       "24                733         26932               2             26932   \n",
       "25                950         28411              10             28411   \n",
       "26               1070         32150               4             32150   \n",
       "27                854         29443              19             29443   \n",
       "28                827         31115               8             31115   \n",
       "29                733         30201               7             30201   \n",
       "..                ...           ...             ...               ...   \n",
       "101               263             0               1               100   \n",
       "102               219             0               0               100   \n",
       "103               237             0               0                83   \n",
       "104               232             0               0                96   \n",
       "105               301             0               0               103   \n",
       "106               348             0               0                94   \n",
       "107               333             0               0                90   \n",
       "108               298             0               0                87   \n",
       "109               267             0               0               103   \n",
       "110               321             0               1               111   \n",
       "111               301             0               0                79   \n",
       "112               236             6               0               101   \n",
       "113               269             2               1                90   \n",
       "114               240             6               1                89   \n",
       "115               204            50               1                80   \n",
       "116               232           165               1                74   \n",
       "117               287           275               4                84   \n",
       "118               281           292               0                84   \n",
       "119               325           239               0                93   \n",
       "120               325           141               1                57   \n",
       "121               143            94               0                69   \n",
       "122               116            81               1                60   \n",
       "123               146            56               2                70   \n",
       "124               138            49               1                77   \n",
       "125               131            52               0                83   \n",
       "126               117             0               1                81   \n",
       "127                76             0               0                57   \n",
       "128                 7             0               0                 4   \n",
       "129                 3             0               0                 5   \n",
       "130                14             0               0                10   \n",
       "\n",
       "     store_and_fwd_flag_val  ...  passenger_count_val  tip_amount_val  \\\n",
       "0                         2  ...                    4               0   \n",
       "1                      8584  ...                    0               0   \n",
       "2                     10322  ...                    0               0   \n",
       "3                     12891  ...                    0               0   \n",
       "4                     13316  ...                    0               0   \n",
       "5                     12809  ...                    0               0   \n",
       "6                     12218  ...                    1               0   \n",
       "7                     12385  ...                    0               0   \n",
       "8                     12409  ...                    0               0   \n",
       "9                     13989  ...                    0               0   \n",
       "10                    12698  ...                    0               0   \n",
       "11                    12730  ...                    0               0   \n",
       "12                    13271  ...                    0               0   \n",
       "13                    12217  ...                    0               3   \n",
       "14                    13762  ...                    0               0   \n",
       "15                    13441  ...                    0               0   \n",
       "16                    13899  ...                    1               0   \n",
       "17                    13278  ...                    0               0   \n",
       "18                    13099  ...                    0               0   \n",
       "19                        0  ...                    1               0   \n",
       "20                        0  ...                    0               0   \n",
       "21                        0  ...                    0               0   \n",
       "22                        0  ...                   14               0   \n",
       "23                        0  ...                   39               0   \n",
       "24                        0  ...                  155               0   \n",
       "25                        0  ...                  253               0   \n",
       "26                        0  ...                  318               0   \n",
       "27                        0  ...                  390               0   \n",
       "28                        0  ...                  451               0   \n",
       "29                        0  ...                  482               0   \n",
       "..                      ...  ...                  ...             ...   \n",
       "101                       0  ...                    0               0   \n",
       "102                       0  ...                    1               0   \n",
       "103                       0  ...                    0               0   \n",
       "104                       0  ...                   18               0   \n",
       "105                       0  ...                   82               0   \n",
       "106                       0  ...                   91               0   \n",
       "107                       0  ...                  104               0   \n",
       "108                       0  ...                   89               0   \n",
       "109                       0  ...                  109               0   \n",
       "110                       0  ...                  137               0   \n",
       "111                       0  ...                  146               0   \n",
       "112                       0  ...                  138               0   \n",
       "113                       0  ...                  129               0   \n",
       "114                       0  ...                  101               0   \n",
       "115                       0  ...                  122               0   \n",
       "116                       0  ...                  149               0   \n",
       "117                       0  ...                  195               0   \n",
       "118                       0  ...                  213               0   \n",
       "119                       0  ...                  199               0   \n",
       "120                       0  ...                  222               1   \n",
       "121                       0  ...                  236               1   \n",
       "122                       0  ...                  279               0   \n",
       "123                       0  ...                  282               0   \n",
       "124                       0  ...                  291               0   \n",
       "125                       0  ...                  271               0   \n",
       "126                       0  ...                  233               1   \n",
       "127                       0  ...                  224               0   \n",
       "128                       0  ...                   17               0   \n",
       "129                       0  ...                   19               0   \n",
       "130                       0  ...                   26               1   \n",
       "\n",
       "     fare_amount_val  total_amount_val  improvement_surcharge_val  extra_val  \\\n",
       "0                  0                 0                          0          0   \n",
       "1                  0                 1                          0          0   \n",
       "2                  0                 1                          0          0   \n",
       "3                  0                 2                          0          0   \n",
       "4                  0                 3                          0          0   \n",
       "5                  0                 0                          0          0   \n",
       "6                  0                 3                          0          0   \n",
       "7                  0                 4                          0          0   \n",
       "8                  0                 1                          0          2   \n",
       "9                  0                 6                          0          0   \n",
       "10                 0               982                          0          0   \n",
       "11                 0               750                          0          0   \n",
       "12                 0               303                          0          0   \n",
       "13                19                19                          0          8   \n",
       "14                25                25                          0         17   \n",
       "15                 0                 2                          0          1   \n",
       "16                 0                 3                          0          0   \n",
       "17                 0                 1                          0          0   \n",
       "18                 0                 1                          0          0   \n",
       "19                 0                 0                          0          0   \n",
       "20                 0                 0                          0          0   \n",
       "21                 0                 0                          0          0   \n",
       "22                 0                 0                          0          0   \n",
       "23                 0                 0                          0          0   \n",
       "24                 0                 0                          0          0   \n",
       "25                 0                 0                          0          0   \n",
       "26                 0                 0                          0          0   \n",
       "27                 0                 0                          0          0   \n",
       "28                 0                 0                          0          0   \n",
       "29                 0                 0                          0          0   \n",
       "..               ...               ...                        ...        ...   \n",
       "101               13                55                         13         99   \n",
       "102                7                47                          7         79   \n",
       "103               10                43                         10         88   \n",
       "104                6                28                          6         77   \n",
       "105                7                24                          7         86   \n",
       "106                6                35                          6         76   \n",
       "107               18                48                         18         86   \n",
       "108                9                33                          9         57   \n",
       "109               11                28                         10         61   \n",
       "110                8                28                          7         63   \n",
       "111               10                46                          9         79   \n",
       "112               10                53                         10         80   \n",
       "113                8                36                          8         87   \n",
       "114                8                31                          8         65   \n",
       "115               16                35                         16         68   \n",
       "116               20                49                         20         76   \n",
       "117               17                43                         17         88   \n",
       "118                9                48                          9         66   \n",
       "119               16                42                         16         79   \n",
       "120               12                47                         12         53   \n",
       "121               24              4486                         24       4655   \n",
       "122               24              5198                         24       5407   \n",
       "123               23              5057                         23       5267   \n",
       "124               29              4988                         29       5217   \n",
       "125               27              4410                         27       4614   \n",
       "126               29              3867                         29       4133   \n",
       "127               47              3789                         47       3996   \n",
       "128                2               156                          2        151   \n",
       "129                2               215                          2        210   \n",
       "130                6               351                          6        344   \n",
       "\n",
       "     mta_tax_val  tolls_amount_val  congestion_surcharge_val  row_val  \n",
       "0              0                 0                         0    28187  \n",
       "1              0                 0                         0    26763  \n",
       "2              0                 0                         0    28782  \n",
       "3              0                 0                         0    28599  \n",
       "4              0                 0                         0    29606  \n",
       "5              0                 0                         0    28375  \n",
       "6              0                 0                         0    27260  \n",
       "7              0                 0                         0    27377  \n",
       "8              0                 0                         0    27974  \n",
       "9              0                 0                         0    31216  \n",
       "10             0                 0                         0    28560  \n",
       "11             0                 0                         0    29175  \n",
       "12           468                 0                         0    29739  \n",
       "13            17                 0                         0    22291  \n",
       "14            15                 1                         0    25768  \n",
       "15             0                 0                         0    30298  \n",
       "16             0                 0                         0    30969  \n",
       "17             0                 0                         0    29658  \n",
       "18             0                 0                         0    29321  \n",
       "19             0                 0                         0    25056  \n",
       "20             0                 0                         0    31085  \n",
       "21             0                 0                         0    28401  \n",
       "22             0                 0                         0    27826  \n",
       "23             0                 0                         0    27638  \n",
       "24             0                 0                         0    26932  \n",
       "25             0                 0                         0    28411  \n",
       "26             0                 0                         0    32150  \n",
       "27             0                 0                         0    29443  \n",
       "28             0                 0                         0    31115  \n",
       "29             0                 0                         0    30201  \n",
       "..           ...               ...                       ...      ...  \n",
       "101           12                 0                         0      551  \n",
       "102            8                 0                         0      496  \n",
       "103           10                 0                         0      490  \n",
       "104            9                 0                         0      480  \n",
       "105            6                 0                         0      558  \n",
       "106            6                 0                         0      606  \n",
       "107           18                 0                         0      611  \n",
       "108            9                 0                         0      540  \n",
       "109           11                 0                         0      501  \n",
       "110            7                 0                         0      583  \n",
       "111            8                 0                         0      518  \n",
       "112           11                 0                         0      508  \n",
       "113            8                 0                         0      510  \n",
       "114            8                 0                         0      475  \n",
       "115           15                 1                         0      426  \n",
       "116           17                 0                         0      460  \n",
       "117           16                 0                         0      567  \n",
       "118            8                 0                         0      546  \n",
       "119           16                 1                         0      579  \n",
       "120           11                 1                         0      553  \n",
       "121           22                 1                        10     4711  \n",
       "122           24                 1                        19     5413  \n",
       "123           22                 0                        12     5297  \n",
       "124           28                 0                        24     5269  \n",
       "125           28                 0                        19     4654  \n",
       "126           29                 1                        18     4124  \n",
       "127           44                 1                        37     3990  \n",
       "128            2                 1                         1      166  \n",
       "129            2                 0                         1      236  \n",
       "130            6                 0                         5      398  \n",
       "\n",
       "[131 rows x 21 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirtyDetailedYellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file',\n",
       " 'number of lines',\n",
       " 'tpep_dropoff_datetime_val',\n",
       " 'tpep_pickup_datetime_val',\n",
       " 'dolocationid_val',\n",
       " 'pulocationid_val',\n",
       " 'vendorid_val',\n",
       " 'ratecodeid_val',\n",
       " 'payment_type_val',\n",
       " 'store_and_fwd_flag_val',\n",
       " 'trip_distance_val',\n",
       " 'passenger_count_val',\n",
       " 'tip_amount_val',\n",
       " 'fare_amount_val',\n",
       " 'total_amount_val',\n",
       " 'improvement_surcharge_val',\n",
       " 'extra_val',\n",
       " 'mta_tax_val',\n",
       " 'tolls_amount_val',\n",
       " 'congestion_surcharge_val',\n",
       " 'row_val']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirtyDetailedYellow.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>number of lines</th>\n",
       "      <th>dolocationid_val</th>\n",
       "      <th>vendorid_val</th>\n",
       "      <th>ratecodeid_val</th>\n",
       "      <th>payment_type_val</th>\n",
       "      <th>store_and_fwd_flag_val</th>\n",
       "      <th>passenger_count_val</th>\n",
       "      <th>mta_tax_val</th>\n",
       "      <th>congestion_surcharge_val</th>\n",
       "      <th>row_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>28187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28187</td>\n",
       "      <td>0</td>\n",
       "      <td>28185</td>\n",
       "      <td>0</td>\n",
       "      <td>28187</td>\n",
       "      <td>28187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>26763</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26763</td>\n",
       "      <td>0</td>\n",
       "      <td>18179</td>\n",
       "      <td>0</td>\n",
       "      <td>26763</td>\n",
       "      <td>26763</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>28782</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28782</td>\n",
       "      <td>0</td>\n",
       "      <td>18460</td>\n",
       "      <td>0</td>\n",
       "      <td>28782</td>\n",
       "      <td>28782</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>28599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28599</td>\n",
       "      <td>0</td>\n",
       "      <td>15708</td>\n",
       "      <td>0</td>\n",
       "      <td>28599</td>\n",
       "      <td>28599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>29606</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29606</td>\n",
       "      <td>0</td>\n",
       "      <td>16290</td>\n",
       "      <td>0</td>\n",
       "      <td>29606</td>\n",
       "      <td>29606</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>28375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28375</td>\n",
       "      <td>0</td>\n",
       "      <td>15566</td>\n",
       "      <td>0</td>\n",
       "      <td>28375</td>\n",
       "      <td>28375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>27260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27260</td>\n",
       "      <td>0</td>\n",
       "      <td>15042</td>\n",
       "      <td>0</td>\n",
       "      <td>27260</td>\n",
       "      <td>27260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>27377</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27377</td>\n",
       "      <td>0</td>\n",
       "      <td>14992</td>\n",
       "      <td>0</td>\n",
       "      <td>27377</td>\n",
       "      <td>27377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>27974</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27974</td>\n",
       "      <td>0</td>\n",
       "      <td>15565</td>\n",
       "      <td>0</td>\n",
       "      <td>27974</td>\n",
       "      <td>27974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>31216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31216</td>\n",
       "      <td>0</td>\n",
       "      <td>17227</td>\n",
       "      <td>0</td>\n",
       "      <td>25280</td>\n",
       "      <td>31216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>28560</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28560</td>\n",
       "      <td>0</td>\n",
       "      <td>15862</td>\n",
       "      <td>0</td>\n",
       "      <td>2204</td>\n",
       "      <td>28560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>29175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29175</td>\n",
       "      <td>0</td>\n",
       "      <td>16445</td>\n",
       "      <td>0</td>\n",
       "      <td>1666</td>\n",
       "      <td>29175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>29739</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16468</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>22291</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10074</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>25768</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25768</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>30298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>30969</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17070</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30969</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>29658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16380</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29658</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>29321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29321</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>25056</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25056</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>31085</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14554</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31085</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>28401</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14064</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>27826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14381</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27826</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>27638</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14419</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27638</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>26932</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13416</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26932</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>28411</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14034</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28411</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>32150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>29443</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13698</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29443</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>31115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15166</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>30201</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14843</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30201</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>19302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19302</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>17168</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>16831</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>17886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17886</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>19532</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19532</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>18564</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>19008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>17510</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17510</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>16969</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16969</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>18852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18852</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>18606</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18606</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>18443</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18443</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>17421</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>15686</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>15684</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>16062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16062</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>17638</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17638</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>16270</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>16326</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16326</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>15324</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>14023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>15637</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>14847</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>15108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>13869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>12810</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>148</td>\n",
       "      <td>148</td>\n",
       "      <td>148</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>12598</td>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>476</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>696</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>/home/epb123/output/integrated Yellow/yellow_t...</td>\n",
       "      <td>1099</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  file  number of lines  \\\n",
       "0    /home/epb123/output/integrated Yellow/yellow_t...            28187   \n",
       "1    /home/epb123/output/integrated Yellow/yellow_t...            26763   \n",
       "2    /home/epb123/output/integrated Yellow/yellow_t...            28782   \n",
       "3    /home/epb123/output/integrated Yellow/yellow_t...            28599   \n",
       "4    /home/epb123/output/integrated Yellow/yellow_t...            29606   \n",
       "5    /home/epb123/output/integrated Yellow/yellow_t...            28375   \n",
       "6    /home/epb123/output/integrated Yellow/yellow_t...            27260   \n",
       "7    /home/epb123/output/integrated Yellow/yellow_t...            27377   \n",
       "8    /home/epb123/output/integrated Yellow/yellow_t...            27974   \n",
       "9    /home/epb123/output/integrated Yellow/yellow_t...            31216   \n",
       "10   /home/epb123/output/integrated Yellow/yellow_t...            28560   \n",
       "11   /home/epb123/output/integrated Yellow/yellow_t...            29175   \n",
       "12   /home/epb123/output/integrated Yellow/yellow_t...            29739   \n",
       "13   /home/epb123/output/integrated Yellow/yellow_t...            22291   \n",
       "14   /home/epb123/output/integrated Yellow/yellow_t...            25768   \n",
       "15   /home/epb123/output/integrated Yellow/yellow_t...            30298   \n",
       "16   /home/epb123/output/integrated Yellow/yellow_t...            30969   \n",
       "17   /home/epb123/output/integrated Yellow/yellow_t...            29658   \n",
       "18   /home/epb123/output/integrated Yellow/yellow_t...            29321   \n",
       "19   /home/epb123/output/integrated Yellow/yellow_t...            25056   \n",
       "20   /home/epb123/output/integrated Yellow/yellow_t...            31085   \n",
       "21   /home/epb123/output/integrated Yellow/yellow_t...            28401   \n",
       "22   /home/epb123/output/integrated Yellow/yellow_t...            27826   \n",
       "23   /home/epb123/output/integrated Yellow/yellow_t...            27638   \n",
       "24   /home/epb123/output/integrated Yellow/yellow_t...            26932   \n",
       "25   /home/epb123/output/integrated Yellow/yellow_t...            28411   \n",
       "26   /home/epb123/output/integrated Yellow/yellow_t...            32150   \n",
       "27   /home/epb123/output/integrated Yellow/yellow_t...            29443   \n",
       "28   /home/epb123/output/integrated Yellow/yellow_t...            31115   \n",
       "29   /home/epb123/output/integrated Yellow/yellow_t...            30201   \n",
       "..                                                 ...              ...   \n",
       "101  /home/epb123/output/integrated Yellow/yellow_t...            19302   \n",
       "102  /home/epb123/output/integrated Yellow/yellow_t...            17168   \n",
       "103  /home/epb123/output/integrated Yellow/yellow_t...            16831   \n",
       "104  /home/epb123/output/integrated Yellow/yellow_t...            17886   \n",
       "105  /home/epb123/output/integrated Yellow/yellow_t...            19532   \n",
       "106  /home/epb123/output/integrated Yellow/yellow_t...            18564   \n",
       "107  /home/epb123/output/integrated Yellow/yellow_t...            19008   \n",
       "108  /home/epb123/output/integrated Yellow/yellow_t...            17510   \n",
       "109  /home/epb123/output/integrated Yellow/yellow_t...            16969   \n",
       "110  /home/epb123/output/integrated Yellow/yellow_t...            18852   \n",
       "111  /home/epb123/output/integrated Yellow/yellow_t...            18606   \n",
       "112  /home/epb123/output/integrated Yellow/yellow_t...            18443   \n",
       "113  /home/epb123/output/integrated Yellow/yellow_t...            17421   \n",
       "114  /home/epb123/output/integrated Yellow/yellow_t...            15686   \n",
       "115  /home/epb123/output/integrated Yellow/yellow_t...            15684   \n",
       "116  /home/epb123/output/integrated Yellow/yellow_t...            16062   \n",
       "117  /home/epb123/output/integrated Yellow/yellow_t...            17638   \n",
       "118  /home/epb123/output/integrated Yellow/yellow_t...            16270   \n",
       "119  /home/epb123/output/integrated Yellow/yellow_t...            16326   \n",
       "120  /home/epb123/output/integrated Yellow/yellow_t...            15324   \n",
       "121  /home/epb123/output/integrated Yellow/yellow_t...            14023   \n",
       "122  /home/epb123/output/integrated Yellow/yellow_t...            15637   \n",
       "123  /home/epb123/output/integrated Yellow/yellow_t...            14847   \n",
       "124  /home/epb123/output/integrated Yellow/yellow_t...            15108   \n",
       "125  /home/epb123/output/integrated Yellow/yellow_t...            13869   \n",
       "126  /home/epb123/output/integrated Yellow/yellow_t...            12810   \n",
       "127  /home/epb123/output/integrated Yellow/yellow_t...            12598   \n",
       "128  /home/epb123/output/integrated Yellow/yellow_t...              476   \n",
       "129  /home/epb123/output/integrated Yellow/yellow_t...              696   \n",
       "130  /home/epb123/output/integrated Yellow/yellow_t...             1099   \n",
       "\n",
       "     dolocationid_val  vendorid_val  ratecodeid_val  payment_type_val  \\\n",
       "0                   0             0           28187                 0   \n",
       "1                   0             0           26763                 0   \n",
       "2                   0             0           28782                 0   \n",
       "3                   0             0           28599                 0   \n",
       "4                   0             0           29606                 0   \n",
       "5                   0             0           28375                 0   \n",
       "6                   0             0           27260                 0   \n",
       "7                   0             0           27377                 0   \n",
       "8                   0             0           27974                 0   \n",
       "9                   0             0           31216                 0   \n",
       "10                  0             0           28560                 0   \n",
       "11                  0             0           29175                 0   \n",
       "12                  0             0               0                 0   \n",
       "13                  0             0               0                 0   \n",
       "14                 11             0               0                 0   \n",
       "15                  0             0               0                 0   \n",
       "16                  0             0               0                 0   \n",
       "17                  0             0               0                 0   \n",
       "18                  0             0               0                 0   \n",
       "19                  1             0               0                 0   \n",
       "20                  0             0               0                 0   \n",
       "21                  0             0               0                 0   \n",
       "22                  0             0               0                 0   \n",
       "23                  0             0               0                 0   \n",
       "24                  0             0               0                 0   \n",
       "25                  1             0               0                 0   \n",
       "26                  0             0               0                 0   \n",
       "27                  0             0               0                 0   \n",
       "28                  0             0               0                 0   \n",
       "29                  0             0               0                 0   \n",
       "..                ...           ...             ...               ...   \n",
       "101                 0             0               0                 0   \n",
       "102                 0             0               0                 0   \n",
       "103                 0             0               0                 0   \n",
       "104                 0             0               0                 0   \n",
       "105                 0             0               0                 0   \n",
       "106                 0             0               0                 0   \n",
       "107                 0             0               0                 0   \n",
       "108                 0             0               0                 0   \n",
       "109                 0             0               0                 0   \n",
       "110                 0             0               0                 0   \n",
       "111                 0             0               0                 0   \n",
       "112                 0             0               0                 0   \n",
       "113                 0             0               0                 0   \n",
       "114                 0             0               0                 0   \n",
       "115                 0             0               0                 0   \n",
       "116                 0             0               0                 0   \n",
       "117                 0             0               0                 0   \n",
       "118                 0             0               0                 0   \n",
       "119                 0             0               0                 0   \n",
       "120                 0             0               0                 0   \n",
       "121                 0             0               0                 0   \n",
       "122                 0             0               0                 0   \n",
       "123                 0             0               0                 0   \n",
       "124                 0             0               0                 0   \n",
       "125                 0             0               0                 0   \n",
       "126                 0           148             148               148   \n",
       "127                 0           107             107               107   \n",
       "128                 0            33              33                33   \n",
       "129                 0           117             117               117   \n",
       "130                 0           100             100               100   \n",
       "\n",
       "     store_and_fwd_flag_val  passenger_count_val  mta_tax_val  \\\n",
       "0                     28185                    0        28187   \n",
       "1                     18179                    0        26763   \n",
       "2                     18460                    0        28782   \n",
       "3                     15708                    0        28599   \n",
       "4                     16290                    0        29606   \n",
       "5                     15566                    0        28375   \n",
       "6                     15042                    0        27260   \n",
       "7                     14992                    0        27377   \n",
       "8                     15565                    0        27974   \n",
       "9                     17227                    0        25280   \n",
       "10                    15862                    0         2204   \n",
       "11                    16445                    0         1666   \n",
       "12                    16468                    0            0   \n",
       "13                    10074                    0            0   \n",
       "14                    12006                    0            0   \n",
       "15                    16857                    0            0   \n",
       "16                    17070                    0            0   \n",
       "17                    16380                    0            0   \n",
       "18                    16222                    0            0   \n",
       "19                    14136                    0            0   \n",
       "20                    14554                    0            0   \n",
       "21                    14064                    0            0   \n",
       "22                    14381                    0            0   \n",
       "23                    14419                    0            0   \n",
       "24                    13416                    0            0   \n",
       "25                    14034                    0            0   \n",
       "26                    15826                    0            0   \n",
       "27                    13698                    0            0   \n",
       "28                    15166                    0            0   \n",
       "29                    14843                    0            0   \n",
       "..                      ...                  ...          ...   \n",
       "101                       0                    0            0   \n",
       "102                       0                    0            0   \n",
       "103                       0                    0            0   \n",
       "104                       0                    0            0   \n",
       "105                       0                    0            0   \n",
       "106                       0                    0            0   \n",
       "107                       0                    0            0   \n",
       "108                       0                    0            0   \n",
       "109                       0                    0            0   \n",
       "110                       0                    0            0   \n",
       "111                       0                    0            0   \n",
       "112                       0                    0            0   \n",
       "113                       0                    0            0   \n",
       "114                       0                    0            0   \n",
       "115                       0                    0            0   \n",
       "116                       0                    0            0   \n",
       "117                       0                    0            0   \n",
       "118                       0                    0            0   \n",
       "119                       0                    0            0   \n",
       "120                       0                    0            0   \n",
       "121                       0                    0            0   \n",
       "122                       0                    0            0   \n",
       "123                       0                    0            0   \n",
       "124                       0                    0            0   \n",
       "125                       0                    0            0   \n",
       "126                     148                  148            0   \n",
       "127                     107                  107            0   \n",
       "128                      33                   33            0   \n",
       "129                     117                  117            0   \n",
       "130                     100                  100            0   \n",
       "\n",
       "     congestion_surcharge_val  row_val  \n",
       "0                       28187        0  \n",
       "1                       26763        0  \n",
       "2                       28782        0  \n",
       "3                       28599        0  \n",
       "4                       29606        0  \n",
       "5                       28375        0  \n",
       "6                       27260        0  \n",
       "7                       27377        0  \n",
       "8                       27974        0  \n",
       "9                       31216        0  \n",
       "10                      28560        0  \n",
       "11                      29175        0  \n",
       "12                      29739        0  \n",
       "13                      22291        0  \n",
       "14                      25768        0  \n",
       "15                      30298        0  \n",
       "16                      30969        0  \n",
       "17                      29658        0  \n",
       "18                      29321        0  \n",
       "19                      25056        0  \n",
       "20                      31085        0  \n",
       "21                      28401        0  \n",
       "22                      27826        0  \n",
       "23                      27638        0  \n",
       "24                      26932        0  \n",
       "25                      28411        0  \n",
       "26                      32150        0  \n",
       "27                      29443        0  \n",
       "28                      31115        0  \n",
       "29                      30201        0  \n",
       "..                        ...      ...  \n",
       "101                     19302        0  \n",
       "102                     17168        0  \n",
       "103                     16831        0  \n",
       "104                     17886        0  \n",
       "105                     19532        0  \n",
       "106                     18564        0  \n",
       "107                     19008        0  \n",
       "108                     17510        0  \n",
       "109                     16969        0  \n",
       "110                     18852        0  \n",
       "111                     18606        0  \n",
       "112                     18443        0  \n",
       "113                     17421        0  \n",
       "114                     15686        0  \n",
       "115                     15684        0  \n",
       "116                     16062        0  \n",
       "117                     17638        0  \n",
       "118                     16270        0  \n",
       "119                     16326        0  \n",
       "120                      9735        0  \n",
       "121                         0        0  \n",
       "122                         0        0  \n",
       "123                         0        0  \n",
       "124                         0        0  \n",
       "125                         0        0  \n",
       "126                         0      142  \n",
       "127                         0      106  \n",
       "128                         0       32  \n",
       "129                         0      109  \n",
       "130                         0       92  \n",
       "\n",
       "[131 rows x 11 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missingYellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file',\n",
       " 'number of lines',\n",
       " 'dolocationid_val',\n",
       " 'vendorid_val',\n",
       " 'ratecodeid_val',\n",
       " 'payment_type_val',\n",
       " 'store_and_fwd_flag_val',\n",
       " 'passenger_count_val',\n",
       " 'mta_tax_val',\n",
       " 'congestion_surcharge_val',\n",
       " 'row_val']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missingYellow.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears like a good number of those total_amount errors are arising from beyond 2019 - since the \"extra\" column also shows a significant uptick there, I suspect the two are related.  \n",
    "\n",
    "The results of a brief look directly in the records seems to support this - it looks like at some point after 2019, congestion_surcharge might be getting picked up by extra (as well as some other oddities in extra) as well as by itself, throwing off our filter. We will investigate further soon, but the fact that congestion_surcharges appear to a) start being incorrect and b) stop being missing around the same time adds weight to this hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also here appear to have a number of missing dolocationid_val clustered in one file. Let's investigate later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the dirty dataframe separately to avoid unfortunate errors\n",
    "# We can update our cleaned values as we go\n",
    "\n",
    "recleanedYELLOW = dirtyYELLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of our core variables, what is broken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2130560 rows containing dirty values and...\n",
      "481 rows containing no dirty values but missing values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where our dropped records came from\n",
    "print(\"We have \"+str(dirtyYELLOW.filter(dirtyYELLOW.row_val == 1).count())+\" rows containing dirty values and...\")\n",
    "print(str(dirtyYELLOW.filter(dirtyYELLOW.row_val == 2).count())+\" rows containing no dirty values but missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the summary, we can see that out of what is important, we still have quite a good set:  \n",
    "- pickup_datetime: clean, not missing\n",
    "- dropoff_datetime: clean, not missing\n",
    "- pulocationid: dirty, not missing\n",
    "- dolocationid: dirty, missing\n",
    "- trip_distance: dirty, not missing\n",
    "- tip_amount: dirty, not missing\n",
    "- total_amount: dirty, not missing\n",
    "- payment_type: dirty, missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first dive deeper into those missing values of dolocationid...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 25 missing dropoff values\n"
     ]
    }
   ],
   "source": [
    "print(\"We have \"+str(dirtyYELLOW.filter(dirtyYELLOW.dolocationid_val == 2).count())+\" missing dropoff values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there were so few records, we may as well pull them out and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dolocationid_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dolocationid_val\n",
       "14                11\n",
       "19                 1\n",
       "25                 1\n",
       "40                 1\n",
       "41                 1\n",
       "47                 4\n",
       "48                 1\n",
       "54                 2\n",
       "57                 1\n",
       "58                 2"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missingYellow[['dolocationid_val']].loc[(missingYellow[['dolocationid_val']].\\\n",
    "                                         loc[:, missingYellow[['dolocationid_val']].dtypes != object] != 0).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2010-03.csv\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2010-08.csv\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2011-02.csv\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2012-05.csv\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2012-06.csv\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2012-12.csv\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-01.csv\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-07.csv\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-10.csv\n",
      "opened /home/epb123/output/integrated Yellow/yellow_tripdata_2013-11.csv\n",
      "+------------+---------------+------------------+----------+--------------------+---------------------+\n",
      "|dolocationid|   pulocationid|      total_amount|tip_amount|tpep_pickup_datetime|tpep_dropoff_datetime|\n",
      "+------------+---------------+------------------+----------+--------------------+---------------------+\n",
      "|        null|            239|                 0|       0.5| 2010-03-02 09:24:43|  2010-03-02 09:51:31|\n",
      "|        null|            142|                 0|       0.5| 2010-03-01 19:13:32|  2010-03-01 19:22:30|\n",
      "|        null|            162|                 0|       0.5| 2010-03-02 09:23:29|  2010-03-02 09:53:46|\n",
      "|        null|            170|                 0|       0.5| 2010-03-26 09:51:45|  2010-03-26 10:02:45|\n",
      "|        null|            186|                 0|       0.5| 2010-03-05 15:43:41|  2010-03-05 15:59:59|\n",
      "|        null|             79|                 0|       0.5| 2010-03-29 15:44:44|  2010-03-29 16:00:28|\n",
      "|        null|            107|                 0|       0.5| 2010-03-02 09:50:12|  2010-03-02 09:58:02|\n",
      "|        null|failed to match|                 0|       0.5| 2010-03-02 09:38:21|  2010-03-02 09:52:37|\n",
      "|        null|            163|                 0|       0.5| 2010-03-16 15:53:41|  2010-03-16 16:00:35|\n",
      "|        null|            230|                 0|       0.5| 2010-03-02 09:35:23|  2010-03-02 09:52:07|\n",
      "|        null|            148|                 0|       0.5| 2010-03-11 15:50:19|  2010-03-11 15:55:08|\n",
      "|        null|            145|3.6000000000000001|         0| 2010-08-26 10:34:49|  2010-08-26 10:36:27|\n",
      "|        null|failed to match|                 9|         0| 2011-02-18 01:29:34|  2011-02-24 08:11:00|\n",
      "|        null|            262|                14|         0| 2012-05-30 08:28:29|  2012-05-30 08:42:23|\n",
      "|        null|            263|              15.1|         0| 2012-06-14 22:20:21|  2012-06-14 22:42:50|\n",
      "|        null|              4|                12|         0| 2012-12-11 13:02:30|  2012-12-11 13:14:53|\n",
      "|        null|            237|                16|         0| 2012-12-11 13:03:36|  2012-12-11 13:20:56|\n",
      "|        null|            161|47.149999999999999|         0| 2012-12-11 12:27:25|  2012-12-11 12:58:11|\n",
      "|        null|             48|             16.25|         0| 2012-12-11 11:56:32|  2012-12-11 12:14:35|\n",
      "|        null|            113|6.8700000000000001|         0| 2013-01-23 20:00:12|  2013-01-24 15:24:40|\n",
      "+------------+---------------+------------------+----------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The code is slightly ugly, but it functions for our purposes\n",
    "doMiss = create_df(missingYellow.at[14,'file'])\n",
    "doMiss = doMiss.unionByName(create_df(missingYellow.at[19,'file']))\n",
    "doMiss = doMiss.unionByName(create_df(missingYellow.at[25,'file']))\n",
    "doMiss = doMiss.unionByName(create_df(missingYellow.at[40,'file']))\n",
    "doMiss = doMiss.unionByName(create_df(missingYellow.at[41,'file']))\n",
    "doMiss = doMiss.unionByName(create_df(missingYellow.at[47,'file']))\n",
    "doMiss = doMiss.unionByName(create_df(missingYellow.at[48,'file']))\n",
    "doMiss = doMiss.unionByName(create_df(missingYellow.at[54,'file']))\n",
    "doMiss = doMiss.unionByName(create_df(missingYellow.at[57,'file']))\n",
    "doMiss = doMiss.unionByName(create_df(missingYellow.at[58,'file']))\n",
    "\n",
    "doMiss.filter(col('dolocationid').isNull()).select(['dolocationid','pulocationid','total_amount','tip_amount',\\\n",
    "                                                    'tpep_pickup_datetime','tpep_dropoff_datetime']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as though a large part of the records with a missing dolocationid also have a 0 total_amount and a 0.5 tip_amount. I would suspect data corruption or some repeated incorrect entries as the cause of this error - we should remove the records as their inclusion would likely be inaccurate.  \n",
    "\n",
    "For the other records, it looks as if the dolocationid was truly null - based on the 2.2 code, this would mean our query point for the shapefile was invalid. Perhaps the dropoff was underground in such a way that the taxi's GPS could not store the record correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 records are dropped as a result of this action.\n"
     ]
    }
   ],
   "source": [
    "print(str(recleanedYELLOW.filter((col('dolocationid').isNull()) \\\n",
    "                                         & (col('total_amount') == 0) & (col('tip_amount') == 0.5)).count()) \\\n",
    "         + \" records are dropped as a result of this action.\")\n",
    "recleanedYELLOW = recleanedYELLOW.filter((col('dolocationid').isNotNull()) \\\n",
    "                                         | (col('total_amount') != 0) | (col('tip_amount') != 0.5))\n",
    "\n",
    "# We also want to update the dirty database for later use\n",
    "dirtyYELLOW = dirtyYELLOW.filter((col('dolocationid').isNotNull()) \\\n",
    "                                         | (col('total_amount') != 0) | (col('tip_amount') != 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start again on the dirty records with **total_amount**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 36354 dirty values and...\n",
      "0 missing values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where our dropped records came from\n",
    "print(\"We have \"+str(dirtyYELLOW.filter(dirtyYELLOW.total_amount_val == 1).count())+\" dirty values and...\")\n",
    "print(str(dirtyYELLOW.filter(dirtyYELLOW.total_amount_val == 2).count())+\" missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 0 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "# We can then analyse the dirty values - we know the 5% criterion will have caused many records to drop\n",
    "# Let us now check to see if the values are incorrect strings\n",
    "# We know the total_amounts here are not null, thus we can do...\n",
    "\n",
    "dirtyYELLOWanalysis = dirtyYELLOW.filter(dirtyYELLOW.total_amount_val == 1)\n",
    "print(\"we have \" + str(dirtyYELLOWanalysis.select(\"total_amount\",sql.col(\"total_amount\").cast(\"float\").isNotNull().alias(\"strings\")).\\\n",
    "        filter(col('strings') == 'false').count()) + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we appear to have 693 negative total_amount entries\n"
     ]
    }
   ],
   "source": [
    "# We can then continue with negatives\n",
    "\n",
    "dirtyYELLOWanalysis = dirtyYELLOWanalysis.withColumn('total_amount',col('total_amount').cast(FloatType()))\n",
    "dirtyYELLOWanalysis = dirtyYELLOWanalysis.withColumn('negative_total', sql.when(col('total_amount') < 0.0, 1).otherwise(0))\n",
    "print(\"we appear to have \" + str(dirtyYELLOWanalysis.filter(col('negative_total') == 1).count()) + \" negative total_amount entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative values can be converted into positive ones, under the assumption that they are mistakenly entered. If we then remove duplicates after the conversion, we can cover ourselves in case the negative values are some sort of accounting reversal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 3 lucky taxi drivers\n",
      "+--------------------+------------+\n",
      "|            filename|total_amount|\n",
      "+--------------------+------------+\n",
      "|yellow_tripdata_2...|     9001.01|\n",
      "|yellow_tripdata_2...|    200006.3|\n",
      "|yellow_tripdata_2...|      1239.3|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us now check to see if any of the records tripped the 1000 upper bound\n",
    "\n",
    "print(\"there are \"+str(dirtyYELLOWanalysis.filter(dirtyYELLOWanalysis.total_amount > 1000).count())+\" lucky taxi drivers\")\n",
    "dirtyYELLOWanalysis.filter(dirtyYELLOWanalysis.total_amount > 1000).select([col('filename'),col('total_amount')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the lucky taxi drivers, the record with a 1,200 fare could be a visit by the same YouTuber as we found in Green, but the other two are highly unlikely. We will thus set those values to Null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All other conditions were checked, so we must have approx. 36k total_amount records which are more than 5% away from the sum of their components. We will follow the same philosophy as last time, but bearing in mind the possible error in the extra column, and the quantity of broken records, we should run the validation once more, but with a further condition checking the relationship with the congestion_surcharge removed - if, as I suspect, it was occasionally being double counted, this would greatly reduce the number of flagged values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receipt updating testing function stored as UDF\n",
      "3560 irreconcilable totals removed\n"
     ]
    }
   ],
   "source": [
    "# Let's make the updates to the recleanedYELLOW, which we will later inject back into the dataset.\n",
    "\n",
    "# We have no strings to update - we can cast the column as a float\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('total_amount',col('total_amount').cast(FloatType()))\n",
    "# So first update values as absolute - probably faster to abs() the whole column than rely on conditionals\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('total_amount', sql.abs(col('total_amount')))\n",
    "# We reject the discovered outliers with values 200,006.3 and 9001.01 - let us remove them\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('total_amount', sql.when(((col('total_amount') == 200006.3)\\\n",
    "                                            | (col('total_amount') == 9001.01)),\\\n",
    "                                            None).otherwise(col('total_amount')))\n",
    "# Let us set the remainder of records = Null, after running our additional test\n",
    "\n",
    "def YELLOWtotalupdate(total,cong_s,extra,fare,improv,tax,tip,toll):\n",
    "# Function to transform total_amount to null if conditions not held\n",
    "# Function returns Null if it is invalid, total if it is valid\n",
    "    \n",
    "    #no need for this as we know it's not missing\n",
    "#     if total == None or total == \"\" or total == 'nan':\n",
    "#         return 2\n",
    "#     try:\n",
    "#         total = float(total)\n",
    "#     except:\n",
    "#         return 1 \n",
    "    # No need for this as already done\n",
    "    #difference = abs(total)\n",
    "    difference = total\n",
    "    for i in [cong_s,extra,fare,improv,tax,tip,toll]:\n",
    "        if i == None or i == \"\" or i == 'nan' or i == 'NaN':\n",
    "            i = 0\n",
    "        try:\n",
    "            i = abs(float(i))\n",
    "        # Note we use abs here so the fact that we haven't checked tip yet does not affect it\n",
    "            difference = difference - i\n",
    "        except:\n",
    "        # We do the same for float values - we simply want to ignore incorrect strings here\n",
    "            pass\n",
    "        \n",
    "    if abs(difference) > total*0.05: #and total >= 0 and total <= 1000:\n",
    "    # if the difference is greater than 5% of total, it is incorrect\n",
    "    # before returning None, test to see that the congestion_surcharge has no impact    \n",
    "        \n",
    "        difference = total\n",
    "        for i in [extra,fare,improv,tax,tip,toll]:\n",
    "            if i == None or i == \"\" or i == 'nan' or i == 'NaN':\n",
    "                i = 0\n",
    "            try:\n",
    "                i = abs(float(i))\n",
    "                difference = difference - i\n",
    "            except:\n",
    "                pass  \n",
    "        \n",
    "        if abs(difference) > total*0.05: #and total >= 0 and total <= 1000:\n",
    "            return None\n",
    "    \n",
    "    return total\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udfYELLOWtotalbroken = udf(YELLOWtotalupdate, FloatType())\n",
    "\n",
    "print(\"Receipt updating testing function stored as UDF\")\n",
    "\n",
    "def add_YELLOW_total_amount_update(dfrval):\n",
    "# Adds the extra validation as new dataframe columns, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfrval = dfrval.withColumn('total_amount', udfYELLOWtotalbroken('total_amount','congestion_surcharge','extra',\\\n",
    "    'fare_amount','improvement_surcharge','mta_tax','tip_amount','tolls_amount'))\n",
    "\n",
    "    return dfrval\n",
    "\n",
    "recleanedYELLOW = add_YELLOW_total_amount_update(recleanedYELLOW)\n",
    "\n",
    "# we only need to count once - we did not previously have missing values\n",
    "print(str(recleanedYELLOW.filter(col('total_amount').isNull()).count()) + \" irreconcilable totals removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 36,365 flagged total_amount values, we only end up removing 3,560 of them. Assuming congestion_surcharge was double counted has made a huge impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then move onto **tip_amount**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 8 dirty values and...\n",
      "0 missing values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where any dropped records came from\n",
    "print(\"We have \"+str(dirtyYELLOW.filter(dirtyYELLOW.tip_amount_val == 1).count())+\" dirty values and...\")\n",
    "print(str(dirtyYELLOW.filter(dirtyYELLOW.tip_amount_val == 2).count())+\" missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 0 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "# Let us now check to see if the values are incorrect strings\n",
    "# We know the tip_amounts here are not null, thus we can do...\n",
    "\n",
    "dirtyYELLOWanalysis = dirtyYELLOW.filter(dirtyYELLOW.tip_amount_val == 1)\n",
    "print(\"we have \" + str(dirtyYELLOWanalysis.select(\"tip_amount\",sql.col(\"tip_amount\").cast(\"float\").isNotNull().alias(\"strings\")).\\\n",
    "        filter(col('strings') == 'false').count()) + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we appear to have 8 negative tip_amount entries\n"
     ]
    }
   ],
   "source": [
    "# We can then continue with negatives\n",
    "\n",
    "dirtyYELLOWanalysis = dirtyYELLOWanalysis.withColumn('tip_amount',col('tip_amount').cast(FloatType()))\n",
    "dirtyYELLOWanalysis = dirtyYELLOWanalysis.withColumn('negative_tip', sql.when(col('tip_amount') < 0.0, 1).otherwise(0))\n",
    "print(\"we appear to have \" + str(dirtyYELLOWanalysis.filter(col('negative_tip') == 1).count()) + \" negative tip_amount entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we had 3 in Green, we have 8 in Yellow - we can repair these and put them back into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make the updates to the recleanedYELLOW, which we will later inject back into the dataset.\n",
    "\n",
    "# We have no strings to update - we can cast the column as a float\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('tip_amount',col('tip_amount').cast(FloatType()))\n",
    "# So first update values as absolute - probably faster to abs() the whole column than rely on conditionals\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('tip_amount', sql.abs(col('tip_amount')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start on **payment_type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2072517 dirty values and...\n",
      "505 missing values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where our dropped records came from\n",
    "print(\"We have \"+str(dirtyYELLOW.filter(dirtyYELLOW.payment_type_val == 1).count())+\" dirty values and...\")\n",
    "print(str(dirtyYELLOW.filter(dirtyYELLOW.payment_type_val == 2).count())+\" missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found the cause of our number of dirty records... We assume the payment_type started being stored in number format only after a certain date. Let's, however, start with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can impute missing values - we have just fixed the tip amount  \n",
    "- If there is a Null value but tip exists, we store as card  \n",
    "- If there is a Null value but tip == 0, we store as cash  \n",
    "\n",
    "We assume the tip would not exist with a void trip or a no charge trip. We also make the assumption that no tip strongly correlates with a cash trip (in a country where restaurant tips are 20% standard, we believe this is reasonable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 115 trips with a tip but no payment type stored\n",
      "we have 390 trips with no tip and no payment type stored\n"
     ]
    }
   ],
   "source": [
    "# We can impute missing values - we have just fixed the tip amount\n",
    "# If there is a Null value but tip exists, we store as card\n",
    "# If there is a Null value but tip == 0, we store as cash\n",
    "# We assume the tip would not exist with a void trip or a no charge trip\n",
    "dirtyYELLOWanalysis = dirtyYELLOW\n",
    "print(\"we have \" + str(dirtyYELLOWanalysis.filter((col(\"payment_type\").isNull()) & (col(\"tip_amount\") != 0)).count())\\\n",
    "                       + \" trips with a tip but no payment type stored\")\n",
    "print(\"we have \" + str(dirtyYELLOWanalysis.filter((col(\"payment_type\").isNull()) & (col(\"tip_amount\") == 0)).count())\\\n",
    "                       + \" trips with no tip and no payment type stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "recleanedYELLOW = recleanedYELLOW.withColumn('payment_type',sql.when((col(\"payment_type\").isNull())\\\n",
    "                                                                   & (col(\"tip_amount\") != 0), '1')\\\n",
    "                                           .otherwise(col('payment_type')))\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('payment_type',sql.when((col(\"payment_type\").isNull())\\\n",
    "                                                                   & (col(\"tip_amount\") == 0), '2')\\\n",
    "                                           .otherwise(col('payment_type')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 0 remaining trips with a tip but no payment type stored\n",
      "we have 0 remaining trips with no tip and no payment type stored\n"
     ]
    }
   ],
   "source": [
    "print(\"we have \" + str(recleanedYELLOW.filter((col(\"payment_type\").isNull()) & (col(\"tip_amount\") != 0)).count())\\\n",
    "                       + \" remaining trips with a tip but no payment type stored\")\n",
    "print(\"we have \" + str(recleanedYELLOW.filter((col(\"payment_type\").isNull()) & (col(\"tip_amount\") == 0)).count())\\\n",
    "                       + \" remaining trips with no tip and no payment type stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 2067593 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "dirtyYELLOWanalysis = dirtyYELLOW.filter(dirtyYELLOW.payment_type_val == 1)\n",
    "\n",
    "# We can test invalid payment types\n",
    "# Let us first check to see if the values are incorrect strings\n",
    "# We know the payment_types here are not null, thus we can do...\n",
    "\n",
    "print(\"we have \" + str(dirtyYELLOWanalysis.filter(col(\"payment_type\").cast(\"int\").isNull()).count())\\\n",
    "                       + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tax testing function stored as UDF\n"
     ]
    }
   ],
   "source": [
    "def paymentStringFix(pmt):\n",
    "# Function returns 0 if the figure is valid, 1 if it is invalid, 2 if it is None\n",
    "# We have no more null values    \n",
    "\n",
    "    pmt = pmt.lower()\n",
    "    \n",
    "# 1) Credit card\n",
    "    if pmt[0:2] == 'cr' or pmt[0:3] == 'car':\n",
    "        return '1'\n",
    "# 2) Cash \n",
    "    if pmt[0:2] == 'cs' or pmt[0:3] == 'cas':\n",
    "        return '2'\n",
    "# 3) No charge\n",
    "    if pmt[0] == 'n':\n",
    "        return '3'\n",
    "# 4) Dispute\n",
    "    if pmt[0] == 'd':\n",
    "        return '4'\n",
    "# 5) Unknown\n",
    "    if pmt[0] == 'u':\n",
    "        return '5'\n",
    "# 6) Voided trip\n",
    "    if pmt[0] == 'v':\n",
    "        return '6'\n",
    "\n",
    "    # if it is none of the above, just return the original string\n",
    "    return pmt\n",
    "    \n",
    "# Define function as a UDF for pyspark usage\n",
    "udfpayment_string_fix = udf(paymentStringFix, StringType())\n",
    "\n",
    "print(\"payment testing function stored as UDF\")\n",
    "\n",
    "def add_payment_string_fix(dfTXval):\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfTXval = dfTXval.withColumn('payment_type', udfpayment_string_fix('payment_type'))\n",
    "\n",
    "    return dfTXval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "recleanedYELLOW = add_payment_string_fix(recleanedYELLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 0 incorrectly stored strings remaining\n"
     ]
    }
   ],
   "source": [
    "print(\"we have \" + str(recleanedYELLOW.filter(col(\"payment_type\").cast(\"int\").isNull()).count())\\\n",
    "                       + \" incorrectly stored strings remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the strings are dealt with, we can test valid entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 0 values above 6\n",
      "We have 0 values below 1\n"
     ]
    }
   ],
   "source": [
    "print(\"We have \" + str(dirtyYELLOWanalysis.filter(col(\"payment_type\").cast(\"int\") > 6).count())\\\n",
    "                       + \" values above 6\")\n",
    "print(\"We have \" + str(dirtyYELLOWanalysis.filter(col(\"payment_type\").cast(\"int\") < 1).count())\\\n",
    "                       + \" values below 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check how many trips were either 3 or 6:  \n",
    "1) Credit card  \n",
    "2) Cash  \n",
    "3) No charge  \n",
    "4) Dispute  \n",
    "5) Unknown  \n",
    "6) Voided trip  \n",
    "\n",
    "Now that we have mostly repaired payment type, We have flagged trips in 3 or 6 for removal because they are invalid - we do not want to analyse free trips or trips which did not exist, particularly with queries based on amounts. Trips in unknown, we cannot classify, and just because someone decided to dispute their fare did not mean by itself that it was not valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will need to drop 4917 trips with no charge.\n",
      "We will need to drop 0 voided trips.\n"
     ]
    }
   ],
   "source": [
    "print(\"We will need to drop \"+str(dirtyYELLOWanalysis.filter(col('payment_type') == 3).count())\\\n",
    "      + \" trips with no charge.\")\n",
    "print(\"We will need to drop \"+str(dirtyYELLOWanalysis.filter(col('payment_type') == 6).count())\\\n",
    "      + \" voided trips.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "recleanedYELLOW = recleanedYELLOW.filter(col('payment_type') != 3)\n",
    "recleanedYELLOW = recleanedYELLOW.filter(col('payment_type') != 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 0 trips with no charge.\n",
      "We now have 0 voided trips.\n"
     ]
    }
   ],
   "source": [
    "print(\"We now have \"+str(recleanedYELLOW.filter(col('payment_type') == 3).count())\\\n",
    "      + \" trips with no charge.\")\n",
    "print(\"We now have \"+str(recleanedYELLOW.filter(col('payment_type') == 6).count())\\\n",
    "      + \" voided trips.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We also need to clean 444 cash trips with a recorded tip\n"
     ]
    }
   ],
   "source": [
    "print(\"We also need to clean \" + str(recleanedYELLOW.filter((col(\"payment_type\")== 2) & (col(\"tip_amount\") != 0)).count())\\\n",
    "                       + \" cash trips with a recorded tip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we should change the payment_type to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "recleanedYELLOW = recleanedYELLOW.withColumn('payment_type',sql.when(\\\n",
    "                                                                     (col(\"payment_type\")== 2) &\\\n",
    "                                                                     (col(\"tip_amount\") != 0),\\\n",
    "                                                                     1).otherwise(col('payment_type')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 0 cash trips with a recorded tip\n"
     ]
    }
   ],
   "source": [
    "print(\"We now have \" + str(recleanedYELLOW.filter((col(\"payment_type\")== 2) & (col(\"tip_amount\") != 0)).count())\\\n",
    "                       + \" cash trips with a recorded tip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have been dropping rows in our cleaned file, so let's refresh dirtyYELLOW\n",
    "dirtyYELLOW = recleanedYELLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then move onto **trip_distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 19367 dirty values and...\n",
      "0 missing values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where our dropped records came from\n",
    "print(\"We have \"+str(dirtyYELLOW.filter(dirtyYELLOW.trip_distance_val == 1).count())+\" dirty values and...\")\n",
    "print(str(dirtyYELLOW.filter(dirtyYELLOW.trip_distance_val == 2).count())+\" missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 0 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "# Let us now check to see if the values are incorrect strings\n",
    "# We know the trip_distances here are not null, thus we can do...\n",
    "\n",
    "dirtyYELLOWanalysis = dirtyYELLOW.filter(dirtyYELLOW.trip_distance_val == 1)\n",
    "print(\"we have \" + str(dirtyYELLOWanalysis.select(\"trip_distance\",sql.col(\"trip_distance\").cast(\"float\").isNotNull().alias(\"strings\")).\\\n",
    "        filter(col('strings') == 'false').count()) + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we appear to have 3 negative trip_distance entries\n"
     ]
    }
   ],
   "source": [
    "# We can then continue with negatives\n",
    "\n",
    "dirtyYELLOWanalysis = dirtyYELLOWanalysis.withColumn('trip_distance',col('trip_distance').cast(FloatType()))\n",
    "dirtyYELLOWanalysis = dirtyYELLOWanalysis.withColumn('negative_trip', sql.when(col('trip_distance') < 0.0, 1).otherwise(0))\n",
    "print(\"we appear to have \" + str(dirtyYELLOWanalysis.filter(col('negative_trip') == 1).count()) + \" negative trip_distance entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 19360 trips to nowhere\n"
     ]
    }
   ],
   "source": [
    "print(\"there are \"+str(dirtyYELLOWanalysis.filter(dirtyYELLOWanalysis.trip_distance == 0).count())+\" trips to nowhere\")\n",
    "#dirtyYELLOWanalysis.filter(dirtyYELLOWanalysis.trip_distance == 0).select([col('filename'),col('trip_distance')]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 4 trips to somewhere far far away\n",
      "+--------------------+-------------+------------+\n",
      "|            filename|trip_distance|total_amount|\n",
      "+--------------------+-------------+------------+\n",
      "|yellow_tripdata_2...|        494.4|       74.21|\n",
      "|yellow_tripdata_2...|        500.0|        23.8|\n",
      "|yellow_tripdata_2...|     300833.1|        70.0|\n",
      "|yellow_tripdata_2...|        396.3|      411.06|\n",
      "+--------------------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Last is to check trips with distance >= 300\n",
    "print(\"there are \"+str(dirtyYELLOWanalysis.filter(sql.abs(dirtyYELLOWanalysis.trip_distance) >= 300.0).count())+\" trips to somewhere far far away\")\n",
    "dirtyYELLOWanalysis.filter(sql.abs(dirtyYELLOWanalysis.trip_distance) >= 300).select([col('filename'),col('trip_distance'),col('total_amount')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three of those could possibly be feasable, and their presence could in fact match with some of the total_amount anomalies we discovered, the record with a distance of >300k miles suggests a driver who never turned off his meter rather than a correct record. Out of the others, the 396.3 miles record could have been a driver going from New York to Buffalo - a trip of approximately 370 miles, and if the total amount matches the 1,239.3 value we saw earlier, I will keep it.  \n",
    "\n",
    "\"Plus 50 cents per 1/5 mile when traveling above 12mph or per 60 seconds in slow traffic or when the vehicle is stopped.\"\n",
    "Is what the current NYC fare rate is - a total amount of only 400 does not align with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now follow a similar pattern as green - we will correct negative values and then use tips and totals to filter the distance values (a trip with distance 0 but a tip certainly did not have the correct distance value), following with a check of the long trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 12419 trips with 0 distances and a nonzero tip or an amount > $5\n",
      "we have 0 expensive trips with 0 distances and a nonzero tip or an amount > $5 remaining\n"
     ]
    }
   ],
   "source": [
    "# Let's make the updates to the recleanedYELLOW, which we will later inject back into the dataset.\n",
    "\n",
    "# let's first handle that negative value\n",
    "# Abs-ing the entire column is a little bit inelegant, but will suffice\n",
    "\n",
    "# We have no strings to update - we can cast the column as a float\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('trip_distance',col('trip_distance').cast(FloatType()))\n",
    "# So first update values as absolute - probably faster to abs() the whole column than rely on conditionals\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('trip_distance', sql.abs(col('trip_distance')))\n",
    "\n",
    "# Count the dirty records - use the pre-filtered dataset here\n",
    "print(\"we have \"+str(recleanedYELLOW.filter(( col('trip_distance') == 0.0)\\\n",
    "            & ( (col('total_amount') >= 5.0) | (col(\"tip_amount\") > 0.0))).count())+\\\n",
    "            \" trips with 0 distances and a nonzero tip or an amount > $5\")\n",
    "\n",
    "# Next let's apply our filtering criteria to those trip_distances\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('trip_distance', sql.when( ( col('trip_distance') == 0.0)\\\n",
    "                                            & ( (col('total_amount') >= 5.0) | (col(\"tip_amount\") > 0.0)),\\\n",
    "                                            None).otherwise(col('trip_distance')))\n",
    "\n",
    "print(\"we have \"+str(recleanedYELLOW.filter(( col('trip_distance') == 0.0)\\\n",
    "            & ( (col('total_amount') >= 5.0) | (col(\"tip_amount\") > 0.0))).count())+\\\n",
    "            \" expensive trips with 0 distances and a nonzero tip or an amount > $5 remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a little bit disappointing, but we must remove these large distance values\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('trip_distance', sql.when((col('trip_distance') == 396.3) \\\n",
    "                                            | ( col('trip_distance') == 494.4) \\\n",
    "                                            | ( col('trip_distance') == 500) \\\n",
    "                                            | ( col('trip_distance') == 300833.1),\\\n",
    "                                            None).otherwise(col('trip_distance')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can analyse the **dolocationid** and **pulocationid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 69851 dirty dropoff values and...\n",
      "14 missing dropoff values, of which 11 have been removed.\n",
      "We have 66229 dirty pickup values and...\n",
      "0 missing pickup values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where our dropped records came from\n",
    "print(\"We have \"+str(dirtyYELLOW.filter(dirtyYELLOW.dolocationid_val == 1).count())+\" dirty dropoff values and...\")\n",
    "print(str(dirtyYELLOW.filter(dirtyYELLOW.dolocationid_val == 2).count())+\" missing dropoff values, of which 11 have been removed.\")\n",
    "print(\"We have \"+str(dirtyYELLOW.filter(dirtyYELLOW.pulocationid_val == 1).count())+\" dirty pickup values and...\")\n",
    "print(str(dirtyYELLOW.filter(dirtyYELLOW.pulocationid_val == 2).count())+\" missing pickup values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again - we have run a filter in 2.2 - Null dropoff values likely were invalid points in the conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then start with **pulocationid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 56548 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "# Let us now check to see if the values are incorrect strings\n",
    "# We know the locationids here are not null, thus we can do...\n",
    "\n",
    "dirtyYELLOWanalysis = dirtyYELLOW.filter(col('pulocationid_val') == 1)\n",
    "\n",
    "#CASTING AS INT HERE!!!!!!\n",
    "print(\"we have \" + str(dirtyYELLOWanalysis.select(\"pulocationid\",sql.col(\"pulocationid\").cast(\"int\").isNotNull().alias(\"strings\")).\\\n",
    "        filter(col('strings') == 'false').count()) + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These would result from a \"failed to match\" failure coming out of the 2.2 locationid calculations. Basically, if the coordinates were corrupt - i.e. something was stored at 5 deg. longitude and 36 deg latitude, the shapefile would not identify the location as being in New York. We can set these equal to Null to avoid any later confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 56548 match failures\n",
      "we have re-valued 56548 match failures\n"
     ]
    }
   ],
   "source": [
    "# Testing the theory...\n",
    "print(\"we have \"+str(recleanedYELLOW.filter(col('pulocationid') == \"failed to match\").count())+\" match failures\")\n",
    "\n",
    "# Next let's apply our filtering criteria to those locationids\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('pulocationid', sql.when(col('pulocationid') == \"failed to match\",\\\n",
    "                                            None).otherwise(col('pulocationid')))\n",
    "\n",
    "print(\"we have re-valued \"+str(recleanedYELLOW.filter(col('pulocationid').isNull()).count())+\" match failures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have run the locationid testing code in 2.2, and the resulting errors must therefore come from values 264, 265, and above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 9681 values above our threshold\n"
     ]
    }
   ],
   "source": [
    "print(\"We have \"+str(recleanedYELLOW.filter(col('pulocationid') > 263).count())+\" values above our threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 values remain after cleaning\n"
     ]
    }
   ],
   "source": [
    "recleanedYELLOW = recleanedYELLOW.withColumn('pulocationid', sql.when(col('pulocationid') > 263,\\\n",
    "                                            None).otherwise(col('pulocationid')))\n",
    "print(str(recleanedYELLOW.filter(col('pulocationid') > 263).count())+\" values remain after cleaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for **dolocationid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 59548 incorrectly stored strings\n"
     ]
    }
   ],
   "source": [
    "# Let us now check to see if the values are incorrect strings\n",
    "# We know the locationids here are not null, thus we can do...\n",
    "\n",
    "dirtyYELLOWanalysis = dirtyYELLOW.filter(col('dolocationid_val') == 1)\n",
    "\n",
    "#CASTING AS INT HERE!!!!!!\n",
    "print(\"we have \" + str(dirtyYELLOWanalysis.filter(col(\"dolocationid\").cast(\"int\").isNull()).count()) + \" incorrectly stored strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 59548 match failures\n"
     ]
    }
   ],
   "source": [
    "# Testing the theory...\n",
    "print(\"we have \"+str(recleanedYELLOW.filter(col('dolocationid') == \"failed to match\").count())+\" match failures\")\n",
    "\n",
    "# Next let's apply our filtering criteria to those locationids\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('dolocationid', sql.when(col('dolocationid') == \"failed to match\",\\\n",
    "                                            None).otherwise(col('dolocationid')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have re-valued 59548 match failures\n"
     ]
    }
   ],
   "source": [
    "print(\"we have re-valued \"+str(recleanedYELLOW.filter(col('dolocationid').isNull()).count()-14)+\" match failures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10303 values above our threshold\n"
     ]
    }
   ],
   "source": [
    "print(\"We have \"+str(recleanedYELLOW.filter(col('dolocationid') > 263).count())+\" values above our threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 values remain above the threshold\n"
     ]
    }
   ],
   "source": [
    "recleanedYELLOW = recleanedYELLOW.withColumn('dolocationid', sql.when(col('dolocationid') > 263,\\\n",
    "                                            None).otherwise(col('dolocationid')))\n",
    "\n",
    "print(str(recleanedYELLOW.filter(col('dolocationid') > 263).count())+\" values remain above the threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not forget to examine the incorrect **tpep_pickup/dropoff_datetime** columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 8757 dirty dropoff values and...\n",
      "0 missing dropoff values\n",
      "We have 8757 dirty pickup values and...\n",
      "0 missing pickup values\n"
     ]
    }
   ],
   "source": [
    "# Let's find out where our dropped records came from\n",
    "print(\"We have \"+str(dirtyYELLOW.filter(dirtyYELLOW.tpep_dropoff_datetime_val == 1).count())+\" dirty dropoff values and...\")\n",
    "print(str(dirtyYELLOW.filter(dirtyYELLOW.tpep_dropoff_datetime_val == 2).count())+\" missing dropoff values\")\n",
    "print(\"We have \"+str(dirtyYELLOW.filter(dirtyYELLOW.tpep_pickup_datetime_val == 1).count())+\" dirty pickup values and...\")\n",
    "print(str(dirtyYELLOW.filter(dirtyYELLOW.tpep_pickup_datetime_val == 2).count())+\" missing pickup values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the prior issues with trip_distance, we can be reasonably confident that there will be further mismatches. Any trip with an average speed >75mph in central NYC will be an incorrect record - while the several long journeys we have found would allow us to assume someone might average 55 (speed limit in NY) or so on the highway, an average above 75 when so much time would be lost moving through NYC would be almost impossible. We will therefore first correct for any trips demonstrating this error - they will almost certainly be incorrect, and so we will drop the records. \n",
    "\n",
    "Along with this, we can correct for invalid formats (by setting to Null), and afterwards we can examine dropoffs before pickups (where we will repair by switching pickuptime and dropoff time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dirtyYELLOW\n",
    "del dirtyYELLOWanalysis\n",
    "\n",
    "# we no longer use these, and we need the memory. If I were re-writing this code, I would avoid their usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "recleanedYELLOW = recleanedYELLOW.withColumn('dropdt', udfCOLORdatetime_drop_flag('tpep_pickup_datetime',\\\n",
    "                                                                                'tpep_dropoff_datetime',\\\n",
    "                                                                                'trip_distance'))\n",
    "# Drop columns when datetime flag is True!\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('tpep_dropoff_datetime',sql.when(col('dropdt') == True, None).\\\n",
    "                                           otherwise(col('tpep_dropoff_datetime')))\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('tpep_pickup_datetime',sql.when(col('dropdt') == True, None).\\\n",
    "                                           otherwise(col('tpep_pickup_datetime')))\n",
    "\n",
    "recleanedYELLOW = recleanedYELLOW.drop('dropdt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the current version of the cleaned file, we now have... 8254 fewer datetime values and 8254 fewer pickup values.\n"
     ]
    }
   ],
   "source": [
    "# We can do this because we have no pre-existing Nulls\n",
    "\n",
    "print(\"In the current version of the cleaned file, we now have... \"+\\\n",
    "     str(recleanedYELLOW.filter(recleanedYELLOW.tpep_dropoff_datetime.isNull()).count())+\\\n",
    "     \" fewer datetime values and \"+\\\n",
    "     str(recleanedYELLOW.filter(recleanedYELLOW.tpep_pickup_datetime.isNull()).count())+\\\n",
    "     \" fewer pickup values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will now repair 505 backwards values\n"
     ]
    }
   ],
   "source": [
    "# Make the switch Flag dummy\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('tsFlag',udfCOLORdatetime_switch('tpep_pickup_datetime','tpep_dropoff_datetime'))\n",
    "print(\"We will now repair \"+str(recleanedYELLOW.filter(col('tsFlag') == 1).count())+\" backwards values\")\n",
    "\n",
    "# Put dropoff values into a temporary column\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('dropoff_temp',col('tpep_dropoff_datetime'))\n",
    "\n",
    "# Move pickup values into dropoff when switch flag is 1\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('tpep_dropoff_datetime',sql.when(col('tsFlag') == 1,col('tpep_pickup_datetime')).\\\n",
    "                                           otherwise(col('tpep_dropoff_datetime')))\n",
    "# Move dropoff values from temporary column into pickup when switch flag is 1\n",
    "recleanedYELLOW = recleanedYELLOW.withColumn('tpep_pickup_datetime',sql.when(col('tsFlag') == 1,col('dropoff_temp')).\\\n",
    "                                           otherwise(col('tpep_pickup_datetime')))\n",
    "\n",
    "# Drop temporary columns\n",
    "recleanedYELLOW = recleanedYELLOW.drop(*['tsFlag','dropoff_temp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We appear to have cleaned two more records than expected - as our criteria here are firm, we believe this could be the result of rounding or the prior repair to the trip distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we still have further errors in other variables, our main set and the set relevant for the query has been fully analysed. We have replaced incorrect and irreparable values with Null, and repaired those which we could. We can now re-merge our recleanedYELLOW dataset with the other data.  \n",
    "\n",
    "Let's start by checking how many complete (for 2.4) records we can extract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have managed to repair 2030231 complete records\n"
     ]
    }
   ],
   "source": [
    "# We've set irreparable values as Null, so...\n",
    "print(\"We have managed to repair \"+str(recleanedYELLOW.filter(\\\n",
    "                                                             (col('tpep_pickup_datetime').isNotNull()) &\\\n",
    "                                                             (col('tpep_dropoff_datetime').isNotNull()) &\\\n",
    "                                                             (col('dolocationid').isNotNull()) &\\\n",
    "                                                             (col('pulocationid').isNotNull()) &\\\n",
    "                                                             (col('trip_distance').isNotNull()) &\\\n",
    "                                                             (col('total_amount').isNotNull()) &\\\n",
    "                                                             (col('tip_amount').isNotNull()) &\\\n",
    "                                                             (col('payment_type').isNotNull())\n",
    "                                                             ).count())+\" complete records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this is approximately 31% of all of the dirty records we discovered, to have 40k complete and usable additions is a strong result. The question is then what we do with the remaining incomplete records. Given that this represents ~2% of the raw dataset, it would be nice to include them, but again we would have a constantly changing overall database size across queries. Our results would therefore be inconsistent.  \n",
    "\n",
    "We thus proceed by returning these fully cleaned values to their respective datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us filter for the values we want\n",
    "everdirtyYELLOW = recleanedYELLOW.filter(\\\n",
    "                                     (col('tpep_pickup_datetime').isNull()) |\\\n",
    "                                     (col('tpep_dropoff_datetime').isNull()) |\\\n",
    "                                     (col('dolocationid').isNull()) |\\\n",
    "                                     (col('pulocationid').isNull()) |\\\n",
    "                                     (col('trip_distance').isNull()) |\\\n",
    "                                     (col('total_amount').isNull()) |\\\n",
    "                                     (col('tip_amount').isNull()) |\\\n",
    "                                     (col('payment_type').isNull())\n",
    "                                     )\n",
    "\n",
    "recleanedYELLOW = recleanedYELLOW.filter(\\\n",
    "                                     (col('tpep_pickup_datetime').isNotNull()) &\\\n",
    "                                     (col('tpep_dropoff_datetime').isNotNull()) &\\\n",
    "                                     (col('dolocationid').isNotNull()) &\\\n",
    "                                     (col('pulocationid').isNotNull()) &\\\n",
    "                                     (col('trip_distance').isNotNull()) &\\\n",
    "                                     (col('total_amount').isNotNull()) &\\\n",
    "                                     (col('tip_amount').isNotNull()) &\\\n",
    "                                     (col('payment_type').isNotNull())\n",
    "                                     )\n",
    "\n",
    "# Before re-integration, we must remember that we have re-cast a lot of columns as floats and the rest of the data is StringType\n",
    "everdirtyYELLOW = everdirtyYELLOW.select([col(i).cast(\"string\") for i in everdirtyYELLOW.columns])\n",
    "recleanedYELLOW = recleanedYELLOW.select([col(i).cast(\"string\") for i in recleanedYELLOW.columns])\n",
    "\n",
    "# We can then drop the validation columns\n",
    "recleanedYELLOW = recleanedYELLOW.drop(*['pulocationid_val',\\\n",
    "                                   'dolocationid_val',\\\n",
    "                                   'tpep_pickup_datetime_val',\\\n",
    "                                   'tpep_dropoff_datetime_val',\\\n",
    "                                   'vendorid_val',\\\n",
    "                                   'improvement_surcharge_val',\\\n",
    "                                   'mta_tax_val',\\\n",
    "                                   'extra_val',\\\n",
    "                                   'fare_amount_val',\\\n",
    "                                   'tip_amount_val',\\\n",
    "                                   'tolls_amount_val',\\\n",
    "                                   'total_amount_val',\\\n",
    "                                   'store_and_fwd_flag_val',\\\n",
    "                                   'trip_distance_val',\\\n",
    "                                   'passenger_count_val',\\\n",
    "                                   'trip_type_val',\\\n",
    "                                   'payment_type_val',\\\n",
    "                                   'ratecodeid_val',\\\n",
    "                                   'ehail_fee_val',\\\n",
    "                                   'congestion_surcharge_val',\\\n",
    "                                   'row_val'])\n",
    "\n",
    "# leaving us with just the filename variable as something which differentiates the schema from our clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YELLOW_combine(cleanedupYELLOWDF,alwaysdirtyYELLOWDF):\n",
    "    \n",
    "    reAdded = 0\n",
    "    repair_filepaths = [xxx.filename for xxx in recleanedYELLOW.select('filename').distinct().collect()]\n",
    "\n",
    "    for i in range(0,len(repair_filepaths)):\n",
    "        repair_filepaths[i] = clean_YELLOW_directory+\"/all CSV/\"+repair_filepaths[i]+\"clean\"\n",
    "    \n",
    "    repair_filepaths.sort()\n",
    "    \n",
    "    for i in repair_filepaths:\n",
    "        tempFilterDF = recleanedYELLOW.filter(col('filename') == i.replace(clean_YELLOW_directory,\"\").\\\n",
    "                                                                      replace('/all CSV/',\"\").\\\n",
    "                                                                      replace('csvclean','csv'))\n",
    "            \n",
    "        dfi = create_df(i)\n",
    "        pre_existing = dfi.count()\n",
    "\n",
    "        if pre_existing == 0:\n",
    "            dfi = tempFilterDF    \n",
    "        else:\n",
    "            dfi = dfi.unionByName(tempFilterDF.drop('filename'))\n",
    "        #we do not drop_duplicates here, as the operation is incredibly expensive for no benefit\n",
    "\n",
    "        addedDFI = dfi.count()-pre_existing\n",
    "        print(str(addedDFI)+\" additional records added\")\n",
    "        reAdded = reAdded + addedDFI\n",
    "        save_df(dfi,ntpath.basename(i), clean_YELLOW_directory)\n",
    "\n",
    "    print(str(reAdded)+\" records added across all files\")\n",
    "    \n",
    "    if reAdded == 0:\n",
    "        print(\"WARNING: REPAIRED RECORDS COULD NOT BE RE-ADDED\")\n",
    "    \n",
    "    # We have the dirty records saved on a monthly basis already anyway\n",
    "    # We will therefore save everdirtyYELLOW alone\n",
    "    # We will then have saved files per month with caught dirty records and one master file with unrecoverable records\n",
    "    # This will record unrecoverable values, but will not take hours to run\n",
    "    # We do not coalesce() here as the file is large (and we will not be reading it again outside of spark)\n",
    "    \n",
    "    alwaysdirtyYELLOWDF.write.options(header = True).mode(\"overwrite\").\\\n",
    "         csv(os.path.join(dirty_YELLOW_directory, \"irreparable_YELLOW_records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2009-01.csvclean\n",
      "27136 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2009-02.csvclean\n",
      "25879 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2009-03.csvclean\n",
      "27855 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2009-04.csvclean\n",
      "27669 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2009-05.csvclean\n",
      "28612 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2009-06.csvclean\n",
      "27601 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2009-07.csvclean\n",
      "26510 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2009-08.csvclean\n",
      "26578 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2009-09.csvclean\n",
      "27182 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2009-10.csvclean\n",
      "30263 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2009-11.csvclean\n",
      "26632 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2009-12.csvclean\n",
      "27572 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2010-01.csvclean\n",
      "28536 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2010-02.csvclean\n",
      "21548 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2010-03.csvclean\n",
      "24870 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2010-04.csvclean\n",
      "29161 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2010-05.csvclean\n",
      "29794 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2010-06.csvclean\n",
      "28541 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2010-07.csvclean\n",
      "28161 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2010-08.csvclean\n",
      "24070 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2010-09.csvclean\n",
      "29975 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2010-10.csvclean\n",
      "27434 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2010-11.csvclean\n",
      "26812 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2010-12.csvclean\n",
      "26745 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2011-01.csvclean\n",
      "26039 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2011-02.csvclean\n",
      "26883 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2011-03.csvclean\n",
      "30492 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2011-04.csvclean\n",
      "28197 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2011-05.csvclean\n",
      "29965 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2011-06.csvclean\n",
      "29247 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2011-07.csvclean\n",
      "28497 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2011-08.csvclean\n",
      "25584 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2011-09.csvclean\n",
      "28045 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2011-10.csvclean\n",
      "30320 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2011-11.csvclean\n",
      "28044 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2011-12.csvclean\n",
      "28811 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2012-01.csvclean\n",
      "28956 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2012-02.csvclean\n",
      "29000 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2012-03.csvclean\n",
      "31212 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2012-04.csvclean\n",
      "29852 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2012-05.csvclean\n",
      "29931 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2012-06.csvclean\n",
      "29064 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2012-07.csvclean\n",
      "27638 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2012-08.csvclean\n",
      "27645 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2012-09.csvclean\n",
      "27974 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2012-10.csvclean\n",
      "28093 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2012-11.csvclean\n",
      "26736 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2012-12.csvclean\n",
      "28522 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2013-01.csvclean\n",
      "28713 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2013-02.csvclean\n",
      "27263 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2013-03.csvclean\n",
      "30651 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2013-04.csvclean\n",
      "29290 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2013-05.csvclean\n",
      "28444 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2013-06.csvclean\n",
      "27941 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2013-07.csvclean\n",
      "26989 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2013-08.csvclean\n",
      "24552 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2013-09.csvclean\n",
      "27660 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2013-10.csvclean\n",
      "29425 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2013-11.csvclean\n",
      "28014 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2013-12.csvclean\n",
      "27219 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2014-01.csvclean\n",
      "26884 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2014-02.csvclean\n",
      "25477 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2014-03.csvclean\n",
      "30060 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2014-04.csvclean\n",
      "28383 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2014-05.csvclean\n",
      "28548 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2014-06.csvclean\n",
      "26632 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2014-07.csvclean\n",
      "25223 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2014-08.csvclean\n",
      "24480 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2014-09.csvclean\n",
      "25967 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2014-10.csvclean\n",
      "27727 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2014-11.csvclean\n",
      "25686 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2014-12.csvclean\n",
      "25301 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2015-01.csvclean\n",
      "40 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2015-02.csvclean\n",
      "34 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2015-03.csvclean\n",
      "51 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2015-04.csvclean\n",
      "33 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2015-05.csvclean\n",
      "34 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2015-06.csvclean\n",
      "34 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2015-07.csvclean\n",
      "27 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2015-08.csvclean\n",
      "35 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2015-09.csvclean\n",
      "46 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2015-10.csvclean\n",
      "29 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2015-11.csvclean\n",
      "41 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2015-12.csvclean\n",
      "33 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2016-01.csvclean\n",
      "24 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2016-02.csvclean\n",
      "27 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2016-03.csvclean\n",
      "34 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2016-04.csvclean\n",
      "32 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2016-05.csvclean\n",
      "33 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2016-06.csvclean\n",
      "33 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2016-07.csvclean\n",
      "26 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2016-08.csvclean\n",
      "27 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2016-09.csvclean\n",
      "33 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2016-10.csvclean\n",
      "27 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2016-11.csvclean\n",
      "37 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2016-12.csvclean\n",
      "28 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2017-01.csvclean\n",
      "31 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2017-02.csvclean\n",
      "27 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2017-03.csvclean\n",
      "27 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2017-04.csvclean\n",
      "33 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2017-05.csvclean\n",
      "31 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2017-06.csvclean\n",
      "31 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2017-07.csvclean\n",
      "29 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2017-08.csvclean\n",
      "37 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2017-09.csvclean\n",
      "30 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2017-10.csvclean\n",
      "22 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2017-11.csvclean\n",
      "38 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2017-12.csvclean\n",
      "40 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2018-01.csvclean\n",
      "28 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2018-02.csvclean\n",
      "30 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2018-03.csvclean\n",
      "29 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2018-04.csvclean\n",
      "22 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2018-05.csvclean\n",
      "26 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2018-06.csvclean\n",
      "23 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2018-07.csvclean\n",
      "27 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2018-08.csvclean\n",
      "36 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2018-09.csvclean\n",
      "36 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2018-10.csvclean\n",
      "47 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2018-11.csvclean\n",
      "39 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2018-12.csvclean\n",
      "45 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2019-01.csvclean\n",
      "33 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2019-02.csvclean\n",
      "4382 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2019-03.csvclean\n",
      "5097 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2019-04.csvclean\n",
      "4948 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2019-05.csvclean\n",
      "4882 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2019-06.csvclean\n",
      "4291 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2020-01.csvclean\n",
      "3910 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2020-02.csvclean\n",
      "3807 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2020-04.csvclean\n",
      "174 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2020-05.csvclean\n",
      "306 additional records added\n",
      "opened /home/epb123/output/cleaned YELLOW/all CSV/yellow_tripdata_2020-06.csvclean\n",
      "427 additional records added\n",
      "2030231 records added across all files\n"
     ]
    }
   ],
   "source": [
    "YELLOW_combine(recleanedYELLOW, everdirtyYELLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/output/cleaned YELLOW/all CSV\n"
     ]
    }
   ],
   "source": [
    "# Clean the PySpark outputs from before\n",
    "YELLOW_cleanCSV_folderPath = cleanup(clean_YELLOW_directory)\n",
    "with open(v_direc + \"YELLOW_cleanCSV_folderPath\",'wb') as cleanedYELLOWCSVdirec:\n",
    "    pickle.dump(YELLOW_cleanCSV_folderPath,cleanedYELLOWCSVdirec)\n",
    "\n",
    "# YELLOW_dirtyCSV_folderPath = cleanup(dirty_YELLOW_directory)\n",
    "# with open(v_direc + \"YELLOW_dirtyCSV_folderPath\",'wb') as dirtyYELLOWCSVdirec:\n",
    "#     pickle.dump(YELLOW_dirtyCSV_folderPath,dirtyYELLOWCSVdirec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run time of our 2.3 implimentation is abysmal, however this seems to be a direct result of the numerous record keeping approaches we keep in order to discover dirty data. The code could be streamlined significantly by simply removing all counts and keeping only the filter items. Interestingly, the native pyspark filtering functions and UDFs seem to perform more similarly than expected - when manipulating our Yellow datasets, they are both slow. \n",
    "\n",
    "Finally, had we known filtering for payment types would catch over two million dirty Yellow records, we may not have chosen to use a single dataframe to clean our dirty records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning complete - onto 2.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
