{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction  \n",
    "### The following code was tested on:  \n",
    "Python 3+ running on...  \n",
    "  \n",
    "Microsoft Surface Go:\n",
    "- CPU - Intel Pentium 4415Y @ 1.60 GHz, 2 cores, 4 threads  \n",
    "- RAM - 8 GB\n",
    "\n",
    "ULB Virtual Machine on Surface Go with:\n",
    "- CPU - 2 cores dedicated  \n",
    "- RAM - 4096 MB dedicated\n",
    "\n",
    "ULB Cluster - user epb123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampled Dataset exploration, meta-data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ntpath\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import:  \n",
    "- Pandas for its dataframes  \n",
    "- Numpy for its data analysis packages  \n",
    "- os to access system files  \n",
    "- ntpath to assist with filenames\n",
    "- pickle to access variables across notebooks\n",
    "\n",
    "For future use, any user running the code can directly input their data storage directory for convenience.  \n",
    "We are trying to run this on the ULB cluster, but this helped with local testing and development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(newFolderName, newFolderPath):\n",
    "# Function takes a folder name and a path and creates a new folder/directory\n",
    "# Function returns folder path\n",
    "    \n",
    "    fullFolderPath = os.path.join(newFolderPath, newFolderName)\n",
    "    \n",
    "    # Folder might already have been created\n",
    "    try:\n",
    "        os.mkdir(fullFolderPath)\n",
    "        print(\"new folder created at \"+ fullFolderPath)\n",
    "    except FileExistsError:\n",
    "        print(\"folder already exists at \" + fullFolderPath)\n",
    "    \n",
    "    return fullFolderPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, storing and loading variables with %store does not work on the ULB cluster. We therefore use the pickle package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/variables\n",
      "folder already exists at /home/epb123/output\n"
     ]
    }
   ],
   "source": [
    "# Set this = True if you are running the code on the ULB cluster\n",
    "cluster = True\n",
    "\n",
    "# Add your home directory as a raw string - must be less than 256 characters!!!\n",
    "# NOTE: RUN YOUR NOTEBOOKS FROM THE SAME FOLDER FOR THIS ALL TO WORK (PREFERABLY THE BASE DIRECTORY)\n",
    "if cluster == True:\n",
    "    base_directory = r\"/home/epb123/\"\n",
    "elif cluster == False:\n",
    "    base_directory = r\"/media/sf_Distributed/Project cleaner/\"\n",
    "\n",
    "#making some folders to keep things clean\n",
    "v_direc = create_folder('variables', base_directory) + '/'\n",
    "save_directory = create_folder('output', base_directory) + '/'\n",
    "\n",
    "with open(\"v_direc\",'wb') as variabledirectory:\n",
    "    pickle.dump(v_direc,variabledirectory)   \n",
    "with open(v_direc + \"save_directory\",'wb') as savedirectory:\n",
    "    pickle.dump(save_directory,savedirectory)   \n",
    "with open(v_direc + \"base_directory\",'wb') as basedirectory:\n",
    "    pickle.dump(base_directory,basedirectory) \n",
    "\n",
    "# Add your data storage filepath below as a raw string - must be less than 256 characters!!!\n",
    "if cluster == True:\n",
    "    data_folder = r\"/home/epb199/data/\"\n",
    "elif cluster == False:\n",
    "    data_folder = r\"/media/sf_Distributed/Project cleaner/tlc_0.2perc/\"\n",
    "\n",
    "#%store data_folder\n",
    "with open(v_direc + \"data_folder\",'wb') as datafolder:\n",
    "    pickle.dump(data_folder,datafolder)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics about the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute basic statistics about the number of files in this sub-dataset, their size, and the number of records (lines) in each file. For length and number of records, give the min, max, mean, 25, 50, 75, 90 percentiles values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for later use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we first needed some function for taking all of the file paths from the centralised data storage location and classifying them into types. We relied on the filenames for this classification (assuming they were correct), with the only point of note here being that we need to start with FHVHV or the if statement capturing FHV filenames will misclassify FHVHV files (without a bit of extra code). It then returns a convenient set of four lists with each set of filepaths for us to iterate over later. These filepaths are ordered oldest to newest with the simple .sort() as TlC was kind enough to follow an easily sortable and consistent file naming schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(data_directory):\n",
    "# Code to retrieve all path names from central data download and classify into type\n",
    "# Returns tuple containing four lists: fhv filepaths, fhvhv filepaths, green filepaths, and yellow filepaths, in that order\n",
    "# Each list will be sorted in ascending order - earliest records will therefore appear first\n",
    "\n",
    "        # Making sure we are working in the directory the data is stored in\n",
    "        original_directory = os.getcwd()\n",
    "        os.chdir(data_directory)\n",
    "        \n",
    "        # Getting filepaths & filenames from directory, sorting by date (ASSUMING CORRECT FILE NAMING), splitting by type\n",
    "        raw_filenames = os.listdir(data_directory)\n",
    "        raw_filenames.sort()\n",
    "        \n",
    "        fhv_filepaths = []\n",
    "        fhvhv_filepaths = []\n",
    "        green_filepaths = []\n",
    "        yellow_filepaths = []\n",
    "        \n",
    "        # Looping through all filenames, categorising by type, and generating filepaths\n",
    "        for i in raw_filenames:\n",
    "            if \".csv\" in i:\n",
    "                if \"fhvhv\" in i:\n",
    "                    # Have to start with fhvhv, or else fhvhv files will be captured by fhv statement below\n",
    "                    fhvhv_filepaths.append(os.path.abspath(i))\n",
    "                elif \"fhv\" in i:\n",
    "                    fhv_filepaths.append(os.path.abspath(i))\n",
    "                elif \"green\" in i:\n",
    "                    green_filepaths.append(os.path.abspath(i))\n",
    "                elif \"yellow\" in i:\n",
    "                    yellow_filepaths.append(os.path.abspath(i))\n",
    "                else:\n",
    "                    print(\"not found\" + i)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        # Change directory back to original one before returning results\n",
    "        os.chdir(original_directory)\n",
    "        \n",
    "        return fhv_filepaths, fhvhv_filepaths, green_filepaths, yellow_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we needed a way to quickly count the number of rows in the files. Ours seem to be relatively small (only up to ~5MB for some of the Yellow CSVs, so a simple iteration would probably have been good enough but when running on the full 100% dataset, it would have been expensive to open each file and count each row. This operation could have been done in Spark, but as the rest of 2.1 did not appear to need it, we preferred to stay with regular Python methods.  \n",
    "\n",
    "What we ended up doing was to impliment an \"unbuffered counter\", which was proposed by a user called Michael Bacon on StackOverflow. Essentially what it does is that it skips the step where Python loads data into a buffer for  use while the file completes loading. While that can be useful if we want to do a lot with the files, here we do not - we only want to run the row count operation and exit. Python will, in this function, read the raw file as kept on system storage (with the SSD on our laptops it is fine, but with a hard disk this may change), count the lines, and then move on, without losing time moving the whole file into a new area of memory. Given the expense of just loading the full files as spark dataframes later, we were very happy with the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unbuffered_counter(filename):\n",
    "# Quickly counting rows in any large file by using raw unbuffered data, code credit to Michael Bacon on StackOverflow\n",
    "# Returns number of new lines in the file\n",
    "# Could also be done with a Spark RDD, but this method is sufficiently fast, accounting for time to start up Spark\n",
    "    \n",
    "    file = open(filename, 'rb')\n",
    "    records = 0\n",
    "    buffer_size = 1024 * 1024\n",
    "    read_file = file.raw.read\n",
    "    buffer = read_file(buffer_size)\n",
    "    while buffer:\n",
    "        records += buffer.count(b'\\n')\n",
    "        buffer = read_file(buffer_size)\n",
    "    file.close()\n",
    "    \n",
    "    # First row is the header!\n",
    "    records = records - 1\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We needed a function to take a list of numbers and calculate basic statistics. This will be run eight times, so it is here for convenience. Numpy is the obvious package for these calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_stats(file_datalist):\n",
    "# Calculates basic stats for a given list\n",
    "# Returns a list containing relevant stats\n",
    "\n",
    "    stats = []\n",
    "    \n",
    "    stats.append(np.min(file_datalist))\n",
    "    stats.append(np.max(file_datalist))\n",
    "    stats.append(np.mean(file_datalist))\n",
    "    \n",
    "    stats.append(np.percentile(file_datalist, 25))\n",
    "    stats.append(np.percentile(file_datalist, 50))\n",
    "    stats.append(np.percentile(file_datalist, 75))\n",
    "    stats.append(np.percentile(file_datalist, 90))\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a function to take a list of filepaths and calculate those summary statistics using the function immediately above. Here, we apply os.stat()'s st_size for the file size and the unbuffered counter from before. We return the outputs as a single list as we later make a dataframe to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_sizelength(subdirectory):\n",
    "# Function to take a subdirectory, calculate summary stats of filesizes and lengths for files in subdirectory\n",
    "# Returns a list with all statistics in order size, length, with: min, max, mean, 25%, 50%, 75%, 90%\n",
    "\n",
    "    allstats = []\n",
    "    filesizes = []\n",
    "    filelengths = []\n",
    "\n",
    "    for i in subdirectory:\n",
    "        # os.stat().st_size returns file size in bytes\n",
    "        filesizes.append(os.stat(i).st_size)\n",
    "        filelengths.append(unbuffered_counter(i))\n",
    "\n",
    "    # Calling data_stats to calculate summary statistics\n",
    "    allstats.extend(data_stats(filesizes))\n",
    "    allstats.extend(data_stats(filelengths))\n",
    "    \n",
    "    return allstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating summary statistics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the analysis by just implementing the functions we have just developed. We also store the filepaths for use in 2.2 and beyond. We present the results of the simple calculation as a dataframe as that makes the most sense for this type of data, adding the function returns to the dataframe as individual items with [:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Below are summary statistics for the TLC Trip Record sub-datasets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>files</th>\n",
       "      <th>min size</th>\n",
       "      <th>max size</th>\n",
       "      <th>mean size</th>\n",
       "      <th>25% size</th>\n",
       "      <th>50% size</th>\n",
       "      <th>75% size</th>\n",
       "      <th>90% size</th>\n",
       "      <th>min records</th>\n",
       "      <th>max records</th>\n",
       "      <th>mean records</th>\n",
       "      <th>25% records</th>\n",
       "      <th>50% records</th>\n",
       "      <th>75% records</th>\n",
       "      <th>90% records</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fhv</th>\n",
       "      <td>64.0</td>\n",
       "      <td>52060.0</td>\n",
       "      <td>3339455.0</td>\n",
       "      <td>1.147258e+06</td>\n",
       "      <td>218111.25</td>\n",
       "      <td>646615.0</td>\n",
       "      <td>2257156.50</td>\n",
       "      <td>3020144.5</td>\n",
       "      <td>959.0</td>\n",
       "      <td>47672.0</td>\n",
       "      <td>21712.62500</td>\n",
       "      <td>6030.50</td>\n",
       "      <td>21692.0</td>\n",
       "      <td>33844.75</td>\n",
       "      <td>43708.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fhvhv</th>\n",
       "      <td>10.0</td>\n",
       "      <td>535789.0</td>\n",
       "      <td>2978931.0</td>\n",
       "      <td>2.007751e+06</td>\n",
       "      <td>1121047.25</td>\n",
       "      <td>2542280.0</td>\n",
       "      <td>2687857.25</td>\n",
       "      <td>2804332.8</td>\n",
       "      <td>8625.0</td>\n",
       "      <td>47703.0</td>\n",
       "      <td>32181.90000</td>\n",
       "      <td>18022.00</td>\n",
       "      <td>40714.0</td>\n",
       "      <td>43056.25</td>\n",
       "      <td>44922.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>76.0</td>\n",
       "      <td>2512.0</td>\n",
       "      <td>570765.0</td>\n",
       "      <td>2.624373e+05</td>\n",
       "      <td>121494.75</td>\n",
       "      <td>190194.5</td>\n",
       "      <td>456955.00</td>\n",
       "      <td>499751.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3546.0</td>\n",
       "      <td>2026.50000</td>\n",
       "      <td>1358.75</td>\n",
       "      <td>2072.5</td>\n",
       "      <td>2892.25</td>\n",
       "      <td>3126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yellow</th>\n",
       "      <td>131.0</td>\n",
       "      <td>43103.0</td>\n",
       "      <td>5959352.0</td>\n",
       "      <td>3.750760e+06</td>\n",
       "      <td>1756967.00</td>\n",
       "      <td>4442047.0</td>\n",
       "      <td>5123591.50</td>\n",
       "      <td>5491438.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>32300.0</td>\n",
       "      <td>24203.51145</td>\n",
       "      <td>19989.00</td>\n",
       "      <td>26294.0</td>\n",
       "      <td>29080.00</td>\n",
       "      <td>30202.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        files  min size   max size     mean size    25% size   50% size  \\\n",
       "fhv      64.0   52060.0  3339455.0  1.147258e+06   218111.25   646615.0   \n",
       "fhvhv    10.0  535789.0  2978931.0  2.007751e+06  1121047.25  2542280.0   \n",
       "green    76.0    2512.0   570765.0  2.624373e+05   121494.75   190194.5   \n",
       "yellow  131.0   43103.0  5959352.0  3.750760e+06  1756967.00  4442047.0   \n",
       "\n",
       "          75% size   90% size  min records  max records  mean records  \\\n",
       "fhv     2257156.50  3020144.5        959.0      47672.0   21712.62500   \n",
       "fhvhv   2687857.25  2804332.8       8625.0      47703.0   32181.90000   \n",
       "green    456955.00   499751.0         15.0       3546.0    2026.50000   \n",
       "yellow  5123591.50  5491438.0        476.0      32300.0   24203.51145   \n",
       "\n",
       "        25% records  50% records  75% records  90% records  \n",
       "fhv         6030.50      21692.0     33844.75      43708.9  \n",
       "fhvhv      18022.00      40714.0     43056.25      44922.0  \n",
       "green       1358.75       2072.5      2892.25       3126.0  \n",
       "yellow     19989.00      26294.0     29080.00      30202.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths = get_paths(data_folder)\n",
    "#%store filepaths\n",
    "with open(v_direc + \"filepaths\",'wb') as file_paths:\n",
    "    pickle.dump(filepaths,file_paths)\n",
    "    \n",
    "# filepaths[0] is fhv, [1] is fhvhv, [2] is green, [3] is yellow\n",
    "\n",
    "# Creating lists to store sub-dataset statistics, and adding the number of files as the first item\n",
    "fhv_info = [len(filepaths[0])]\n",
    "fhvhv_info = [len(filepaths[1])]\n",
    "green_info = [len(filepaths[2])]\n",
    "yellow_info = [len(filepaths[3])]\n",
    "\n",
    "# Adding filesize and length summary statistics in each sub-dataset, using file_sizelength routine\n",
    "fhv_info.extend(file_sizelength(filepaths[0]))\n",
    "fhvhv_info.extend(file_sizelength(filepaths[1]))\n",
    "green_info.extend(file_sizelength(filepaths[2]))\n",
    "yellow_info.extend(file_sizelength(filepaths[3]))\n",
    "\n",
    "# Creating a dataframe to present results\n",
    "datastats_df = pd.DataFrame(columns = ['files', 'min size', 'max size', 'mean size', '25% size', '50% size', '75% size', '90% size', 'min records', 'max records', 'mean records', '25% records', '50% records', '75% records', '90% records'])\n",
    "datastats_df.loc['fhv'] = fhv_info[:]\n",
    "datastats_df.loc['fhvhv'] = fhvhv_info[:]\n",
    "datastats_df.loc['green'] = green_info[:]\n",
    "datastats_df.loc['yellow'] = yellow_info[:]\n",
    "\n",
    "print(\"\\nBelow are summary statistics for the TLC Trip Record sub-datasets:\")\n",
    "datastats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the schema evolution.\n",
    "\n",
    "Over time, the relational schema associated to each type of trip data (yellow, green, fhv, hvfhv) has changed. Let us analyze the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for later use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three functions just return the year, month, and date from a filepath. Since TLC was good enough to be consistent in their file naming, we can happily take advantage of this to make our lives a bit easier when identifying record groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pathyear(givenpath):\n",
    "# Function to get the year from a given filepath\n",
    "# Returns the year as an integer\n",
    "    return int(givenpath[-11:-7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pathmonth(givenpath):\n",
    "# Function to get the month from a given filepath\n",
    "# Returns the month as an integer\n",
    "    return int(givenpath[-6:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pathdate(givenpath):\n",
    "# Function to get the date from a given filepath\n",
    "# Returns the date as a tuple in (year, month) form\n",
    "        return (int(givenpath[-11:-7]), int(givenpath[-6:-4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially the catch-all function for checking the schema. As we want to minimise the number of loops we go through for efficiency's sake, we track all schema issues together.  \n",
    "\n",
    "In analysing the schema changes, we make use of Python's unordered set properties - we effectively go through once and catch the major issues requiring human intervention (changed variable names, missing variables, etc.) using sets and then afterwards, we use the original list format just to track order changes.  \n",
    "\n",
    "We use dataframes for the returned column addition/subtraction and re-ordering analysis, and a list of lists for the common schemas as this will be useful in 2.2.\n",
    "\n",
    "One minor thing to note is that we make use of ntpath.basename() here for very easy file identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_analysis(sub_data):\n",
    "# Function to analyse changes in data column labels, taking filepath list as inputs\n",
    "# Function returns a tuple containing:\n",
    "# Return 1. dataframe containing columns added to files when comparing vs. previous year, and columns dropped from files when comparing vs. subsequent year\n",
    "# Return 2. list of lists, where each sub-list contains the dates as a tuple (year, month) which have a common schema\n",
    "# Return 3. dataframe of variables where the index changed from one year to another\n",
    "    # Create variables to capture changes - column changes as a set and a list of dates with the same schema\n",
    "    col_changes = {}\n",
    "    same_schema = []\n",
    "    order_changesDf = pd.DataFrame(columns = ['file 0', 'file 1', 'index 0', 'index 1'])\n",
    "    \n",
    "    # Variable to store dates with the current schema\n",
    "    current_schema = []\n",
    "\n",
    "    # Checking if column names are the same - iterates over all files in sub-dataset\n",
    "    for j in range (len(sub_data)):        \n",
    "        # Storing first row of the csv and ensuring all headers are lowercase without leading and trailing white spaces\n",
    "        df_j= pd.read_csv(sub_data[j],nrows=0)\n",
    "        l1 = [item.lower().strip() for item in list(df_j.columns)]\n",
    "        \n",
    "        jdate = get_pathdate(sub_data[j])\n",
    "        \n",
    "        if j == 0:\n",
    "        # Storing first csv's columns as l0\n",
    "            l0 = l1[:]\n",
    "            current_schema.append(sub_data[j])  \n",
    "        \n",
    "        elif set(l1) != set(l0):\n",
    "        # Comparing the column names in set format, as order should not matter here\n",
    "            l0_name = ntpath.basename(sub_data[j-1])\n",
    "            l1_name = ntpath.basename(sub_data[j])\n",
    "            \n",
    "            # Elements in l0 but not in l1 => dropped elements\n",
    "            col_changes['dropped from '+l0_name] = set(l0) - set(l1)\n",
    "            # Elements in l1 and not in l0 => added elements\n",
    "            col_changes['added to '+l1_name] = set(l1) - set(l0)\n",
    "            # Reset schema tracker\n",
    "            same_schema.append(current_schema[:])\n",
    "            current_schema.clear()\n",
    "            current_schema.append(sub_data[j])                      \n",
    "            \n",
    "            # Tracking column changes\n",
    "            for k in range(len(l0)):\n",
    "                try:\n",
    "                    if l0[k] != l1[k]:\n",
    "                        for j in range(len(l1)):\n",
    "                            if l0[k] == l1[j]:\n",
    "                                order_changesDf.loc[l0[k]+', y:'+str(jdate[0])+\", m:\"+str(jdate[1])] = [l0_name, l1_name, k, j]\n",
    "                except IndexError:\n",
    "                    for j in range(len(l1)):\n",
    "                        if l0[k] == l1[j]:\n",
    "                            order_changesDf.loc[l0[k]+', y:'+str(jdate[0])+\", m:\"+str(jdate[1])] = [l0_name, l1_name, k, j]\n",
    "                    \n",
    "            # Reset l0 for next loop\n",
    "            l0 = l1\n",
    "        \n",
    "        elif set(l1) == set(l0):\n",
    "            current_schema.append(sub_data[j])\n",
    "            # Append the current_schema if we have reached the last record\n",
    "            if j == len(sub_data) - 1:\n",
    "                same_schema.append(current_schema[:])\n",
    "\n",
    "            # Tracking column changes\n",
    "            # We can use the first if statement here (l0 != l1) to possibly skip the further loops\n",
    "            # We did not do this before because obviously, l0 would always == l1 if the sets aren't equal\n",
    "            if l0 != l1:\n",
    "                for k in range(len(l0)):\n",
    "                    try:\n",
    "                        if l0[k] != l1[k]:\n",
    "                            for j in range(len(l1)):\n",
    "                                if l0[k] == l1[j]:\n",
    "                                    order_changesDf.loc[l0[k]+', y:'+str(jdate[0])+\", m:\"+str(jdate[1])] = [l0_name, l1_name, k, j]\n",
    "                    except IndexError:\n",
    "                        for j in range(len(l1)):\n",
    "                            if l0[k] == l1[j]:\n",
    "                                order_changesDf.loc[l0[k]+', y:'+str(jdate[0])+\", m:\"+str(jdate[1])] = [l0_name, l1_name, k, j]\n",
    "            \n",
    "            # Reset l0 for next loop\n",
    "            l0 = l1\n",
    "        \n",
    "        else:\n",
    "            print(\"error reading columns\")\n",
    "                \n",
    "    return (pd.DataFrame.from_dict(col_changes, orient='index'), same_schema, order_changesDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to check that the data itself is correct later on in 2.3, so all we care about for now are the headers for each data set - we can take the variable names by just reading the first row of each CSV (which also saves lots of read time). In this step, we also set all variable names to lowercase and strip all extra leading and/or trailing spaces. In the process of data integration in 2.2, we take the liberty of ensuring that all variable names follow this lowercase & no spaces format, so doing this here allows us to focus on more major schema issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_schema(address):   \n",
    "# Function takes a csv from the given address\n",
    "# Returns a list of stripped and lowercase column names\n",
    "    \n",
    "    headers = [item.lower().strip() for item in list(pd.read_csv(address,nrows=0).columns)]\n",
    "    \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We weren't quite sure why, but the sample download appeared to be missing several years' worth of data. We assumed this was somehow a result of the sampling process, but here is a function to track the gaps. There is nothing particularly interesting in the construction - it just calculates an expected next year from the given path and checks to see if it is consistent with what we actually observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_analysis(sub_data):\n",
    "# Function to extract date of every file in sub-dataset and highlight missing months\n",
    "# Function returns list containing missing months in tuple (year, month) form\n",
    "    \n",
    "    # Generate a list of all dates\n",
    "    # sub_data[i][-11:-7] is the year, sub_data[i][-6:-4] is the month, with the naming convention as used\n",
    "    gap_years = []\n",
    "    for i in range(len(sub_data)):\n",
    "        current_year = get_pathyear(sub_data[i])\n",
    "        current_month = get_pathmonth(sub_data[i])\n",
    "        \n",
    "        # Compare previous expectation to current one\n",
    "        if i == 0:\n",
    "            pass\n",
    "        elif current_month != expected_month or current_year != expected_year:\n",
    "            # Add all missing months to gap_years list\n",
    "            while current_month != expected_month or current_year != expected_year:\n",
    "                gap_years.append((expected_year, expected_month))\n",
    "                if expected_month < 12:\n",
    "                    expected_month = expected_month + 1\n",
    "                else:\n",
    "                    expected_year = expected_year + 1\n",
    "                    expected_month = 1\n",
    "        \n",
    "        # Calculate what we expect the next date should be\n",
    "        if current_month < 12:\n",
    "            expected_year = current_year\n",
    "            expected_month = current_month + 1\n",
    "        else:\n",
    "            expected_year = current_year + 1\n",
    "            expected_month = 1\n",
    "    \n",
    "    return(gap_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema evolution code\n",
    "\n",
    "The below code runs the previously defined functions for each index of the filepaths[] list. They also store what is useful in 2.2 to memory to avoid re-calculation.  \n",
    "\n",
    "Note: you should un-comment out the line: print(XYZ_schema[1]) if you want to see the full list of lists containing file paths with a common schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of schema changes for fhv cab data files\n",
    "\n",
    "Analyze the schema changes for the FHV cab data files. Write down your conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_missing = date_analysis(filepaths[0])\n",
    "fhv_schema = column_analysis(filepaths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following months are missing from this sub-dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2019, 12), (2020, 2)]\n"
     ]
    }
   ],
   "source": [
    "print(fhv_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following files, the schema is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(fhv_schema[1])\n",
    "fhv_sameSchema = fhv_schema[1]\n",
    "#%store fhv_sameSchema\n",
    "with open(v_direc + \"fhv_sameSchema\",'wb') as fhvsameSchema:\n",
    "    pickle.dump(fhv_sameSchema,fhvsameSchema)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In or after the following files, there was the following schema change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dropped from fhv_tripdata_2016-12.csv</th>\n",
       "      <td>pickup_date</td>\n",
       "      <td>locationid</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>added to fhv_tripdata_2017-01.csv</th>\n",
       "      <td>dropoff_datetime</td>\n",
       "      <td>pulocationid</td>\n",
       "      <td>dolocationid</td>\n",
       "      <td>pickup_datetime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropped from fhv_tripdata_2017-06.csv</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>added to fhv_tripdata_2017-07.csv</th>\n",
       "      <td>sr_flag</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropped from fhv_tripdata_2017-12.csv</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>added to fhv_tripdata_2018-01.csv</th>\n",
       "      <td>dispatching_base_number</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropped from fhv_tripdata_2018-12.csv</th>\n",
       "      <td>dispatching_base_number</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>added to fhv_tripdata_2019-01.csv</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             0             1  \\\n",
       "dropped from fhv_tripdata_2016-12.csv              pickup_date    locationid   \n",
       "added to fhv_tripdata_2017-01.csv             dropoff_datetime  pulocationid   \n",
       "dropped from fhv_tripdata_2017-06.csv                     None          None   \n",
       "added to fhv_tripdata_2017-07.csv                      sr_flag          None   \n",
       "dropped from fhv_tripdata_2017-12.csv                     None          None   \n",
       "added to fhv_tripdata_2018-01.csv      dispatching_base_number          None   \n",
       "dropped from fhv_tripdata_2018-12.csv  dispatching_base_number          None   \n",
       "added to fhv_tripdata_2019-01.csv                         None          None   \n",
       "\n",
       "                                                  2                3  \n",
       "dropped from fhv_tripdata_2016-12.csv          None             None  \n",
       "added to fhv_tripdata_2017-01.csv      dolocationid  pickup_datetime  \n",
       "dropped from fhv_tripdata_2017-06.csv          None             None  \n",
       "added to fhv_tripdata_2017-07.csv              None             None  \n",
       "dropped from fhv_tripdata_2017-12.csv          None             None  \n",
       "added to fhv_tripdata_2018-01.csv              None             None  \n",
       "dropped from fhv_tripdata_2018-12.csv          None             None  \n",
       "added to fhv_tripdata_2019-01.csv              None             None  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fhv_schema[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final schema was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag']\n"
     ]
    }
   ],
   "source": [
    "print(file_schema(filepaths[0][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations on schema changes  \n",
    "\n",
    "2018-12 and earlier:  \n",
    "- dispatching_base_number = dispatching_base_num\n",
    "- note: both columns existed simultaneously\n",
    "\n",
    "2016-12 and earlier:\n",
    "- pickup_date = pickup_datetime\n",
    "- locationid = pulocationid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index of the following variables has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file 0</th>\n",
       "      <th>file 1</th>\n",
       "      <th>index 0</th>\n",
       "      <th>index 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dispatching_base_num, y:2018, m:1</th>\n",
       "      <td>fhv_tripdata_2017-12.csv</td>\n",
       "      <td>fhv_tripdata_2018-01.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_datetime, y:2018, m:1</th>\n",
       "      <td>fhv_tripdata_2017-12.csv</td>\n",
       "      <td>fhv_tripdata_2018-01.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_datetime, y:2018, m:1</th>\n",
       "      <td>fhv_tripdata_2017-12.csv</td>\n",
       "      <td>fhv_tripdata_2018-01.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pulocationid, y:2018, m:1</th>\n",
       "      <td>fhv_tripdata_2017-12.csv</td>\n",
       "      <td>fhv_tripdata_2018-01.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dolocationid, y:2018, m:1</th>\n",
       "      <td>fhv_tripdata_2017-12.csv</td>\n",
       "      <td>fhv_tripdata_2018-01.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sr_flag, y:2018, m:1</th>\n",
       "      <td>fhv_tripdata_2017-12.csv</td>\n",
       "      <td>fhv_tripdata_2018-01.csv</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_datetime, y:2019, m:1</th>\n",
       "      <td>fhv_tripdata_2018-12.csv</td>\n",
       "      <td>fhv_tripdata_2019-01.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_datetime, y:2019, m:1</th>\n",
       "      <td>fhv_tripdata_2018-12.csv</td>\n",
       "      <td>fhv_tripdata_2019-01.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pulocationid, y:2019, m:1</th>\n",
       "      <td>fhv_tripdata_2018-12.csv</td>\n",
       "      <td>fhv_tripdata_2019-01.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dolocationid, y:2019, m:1</th>\n",
       "      <td>fhv_tripdata_2018-12.csv</td>\n",
       "      <td>fhv_tripdata_2019-01.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sr_flag, y:2019, m:1</th>\n",
       "      <td>fhv_tripdata_2018-12.csv</td>\n",
       "      <td>fhv_tripdata_2019-01.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dispatching_base_num, y:2019, m:1</th>\n",
       "      <td>fhv_tripdata_2018-12.csv</td>\n",
       "      <td>fhv_tripdata_2019-01.csv</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     file 0  \\\n",
       "dispatching_base_num, y:2018, m:1  fhv_tripdata_2017-12.csv   \n",
       "pickup_datetime, y:2018, m:1       fhv_tripdata_2017-12.csv   \n",
       "dropoff_datetime, y:2018, m:1      fhv_tripdata_2017-12.csv   \n",
       "pulocationid, y:2018, m:1          fhv_tripdata_2017-12.csv   \n",
       "dolocationid, y:2018, m:1          fhv_tripdata_2017-12.csv   \n",
       "sr_flag, y:2018, m:1               fhv_tripdata_2017-12.csv   \n",
       "pickup_datetime, y:2019, m:1       fhv_tripdata_2018-12.csv   \n",
       "dropoff_datetime, y:2019, m:1      fhv_tripdata_2018-12.csv   \n",
       "pulocationid, y:2019, m:1          fhv_tripdata_2018-12.csv   \n",
       "dolocationid, y:2019, m:1          fhv_tripdata_2018-12.csv   \n",
       "sr_flag, y:2019, m:1               fhv_tripdata_2018-12.csv   \n",
       "dispatching_base_num, y:2019, m:1  fhv_tripdata_2018-12.csv   \n",
       "\n",
       "                                                     file 1 index 0 index 1  \n",
       "dispatching_base_num, y:2018, m:1  fhv_tripdata_2018-01.csv       0       6  \n",
       "pickup_datetime, y:2018, m:1       fhv_tripdata_2018-01.csv       1       0  \n",
       "dropoff_datetime, y:2018, m:1      fhv_tripdata_2018-01.csv       2       1  \n",
       "pulocationid, y:2018, m:1          fhv_tripdata_2018-01.csv       3       2  \n",
       "dolocationid, y:2018, m:1          fhv_tripdata_2018-01.csv       4       3  \n",
       "sr_flag, y:2018, m:1               fhv_tripdata_2018-01.csv       5       4  \n",
       "pickup_datetime, y:2019, m:1       fhv_tripdata_2019-01.csv       0       1  \n",
       "dropoff_datetime, y:2019, m:1      fhv_tripdata_2019-01.csv       1       2  \n",
       "pulocationid, y:2019, m:1          fhv_tripdata_2019-01.csv       2       3  \n",
       "dolocationid, y:2019, m:1          fhv_tripdata_2019-01.csv       3       4  \n",
       "sr_flag, y:2019, m:1               fhv_tripdata_2019-01.csv       4       5  \n",
       "dispatching_base_num, y:2019, m:1  fhv_tripdata_2019-01.csv       6       0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fhv_schema[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of schema changes for fhvhv data files\n",
    "\n",
    "Analyze the schema changes for the FHVHV cab data files. Write down your conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhvhv_missing = date_analysis(filepaths[1])\n",
    "fhvhv_schema = column_analysis(filepaths[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following months are missing from this sub-dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2019, 7), (2019, 8), (2019, 9), (2019, 10), (2019, 11), (2019, 12), (2020, 2)]\n"
     ]
    }
   ],
   "source": [
    "print(fhvhv_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following files, the schema is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(fhvhv_schema[1])\n",
    "fhvhv_sameSchema = fhvhv_schema[1]\n",
    "#%store fhvhv_sameSchema\n",
    "with open(v_direc + \"fhvhv_sameSchema\",'wb') as fhvhvsameSchema:\n",
    "    pickle.dump(fhvhv_sameSchema,fhvhvsameSchema)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In or after the following files, there was the following schema change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fhvhv_schema[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final schema was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hvfhs_license_num', 'dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag']\n"
     ]
    }
   ],
   "source": [
    "print(file_schema(filepaths[1][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema for the FHVHV set does not appear to have ever changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index of the following variables has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file 0</th>\n",
       "      <th>file 1</th>\n",
       "      <th>index 0</th>\n",
       "      <th>index 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [file 0, file 1, index 0, index 1]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fhvhv_schema[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of schema changes for green cab data files\n",
    "\n",
    "Analyze the schema changes for the green taxi data files. Write down your conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_missing = date_analysis(filepaths[2])\n",
    "green_schema = column_analysis(filepaths[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following months are missing from this sub-dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2019, 7), (2019, 8), (2019, 9), (2019, 10), (2019, 11), (2019, 12), (2020, 3)]\n"
     ]
    }
   ],
   "source": [
    "print(green_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following files, the schema is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(green_schema[1])\n",
    "green_sameSchema = green_schema[1]\n",
    "#%store green_sameSchema\n",
    "with open(v_direc + \"green_sameSchema\",'wb') as greensameSchema:\n",
    "    pickle.dump(green_sameSchema,greensameSchema)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In or after the following files, there was the following schema change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dropped from green_tripdata_2014-12.csv</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>added to green_tripdata_2015-01.csv</th>\n",
       "      <td>improvement_surcharge</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropped from green_tripdata_2016-06.csv</th>\n",
       "      <td>pickup_longitude</td>\n",
       "      <td>dropoff_latitude</td>\n",
       "      <td>pickup_latitude</td>\n",
       "      <td>dropoff_longitude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>added to green_tripdata_2016-07.csv</th>\n",
       "      <td>pulocationid</td>\n",
       "      <td>dolocationid</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropped from green_tripdata_2018-12.csv</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>added to green_tripdata_2019-01.csv</th>\n",
       "      <td>congestion_surcharge</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             0  \\\n",
       "dropped from green_tripdata_2014-12.csv                   None   \n",
       "added to green_tripdata_2015-01.csv      improvement_surcharge   \n",
       "dropped from green_tripdata_2016-06.csv       pickup_longitude   \n",
       "added to green_tripdata_2016-07.csv               pulocationid   \n",
       "dropped from green_tripdata_2018-12.csv                   None   \n",
       "added to green_tripdata_2019-01.csv       congestion_surcharge   \n",
       "\n",
       "                                                        1                2  \\\n",
       "dropped from green_tripdata_2014-12.csv              None             None   \n",
       "added to green_tripdata_2015-01.csv                  None             None   \n",
       "dropped from green_tripdata_2016-06.csv  dropoff_latitude  pickup_latitude   \n",
       "added to green_tripdata_2016-07.csv          dolocationid             None   \n",
       "dropped from green_tripdata_2018-12.csv              None             None   \n",
       "added to green_tripdata_2019-01.csv                  None             None   \n",
       "\n",
       "                                                         3  \n",
       "dropped from green_tripdata_2014-12.csv               None  \n",
       "added to green_tripdata_2015-01.csv                   None  \n",
       "dropped from green_tripdata_2016-06.csv  dropoff_longitude  \n",
       "added to green_tripdata_2016-07.csv                   None  \n",
       "dropped from green_tripdata_2018-12.csv               None  \n",
       "added to green_tripdata_2019-01.csv                   None  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_schema[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final schema was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vendorid', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'ratecodeid', 'pulocationid', 'dolocationid', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge']\n"
     ]
    }
   ],
   "source": [
    "print(file_schema(filepaths[2][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations on schema changes  \n",
    "\n",
    "2016-06 and earlier:\n",
    "- dolocationid and pulocationid can be computed from dropoff_longitude | dropoff_latitude and pickup_longitude | pickup_latitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index of the following variables has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file 0</th>\n",
       "      <th>file 1</th>\n",
       "      <th>index 0</th>\n",
       "      <th>index 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_amount, y:2015, m:1</th>\n",
       "      <td>green_tripdata_2014-12.csv</td>\n",
       "      <td>green_tripdata_2015-01.csv</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>payment_type, y:2015, m:1</th>\n",
       "      <td>green_tripdata_2014-12.csv</td>\n",
       "      <td>green_tripdata_2015-01.csv</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_type, y:2015, m:1</th>\n",
       "      <td>green_tripdata_2014-12.csv</td>\n",
       "      <td>green_tripdata_2015-01.csv</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passenger_count, y:2016, m:7</th>\n",
       "      <td>green_tripdata_2016-06.csv</td>\n",
       "      <td>green_tripdata_2016-07.csv</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_distance, y:2016, m:7</th>\n",
       "      <td>green_tripdata_2016-06.csv</td>\n",
       "      <td>green_tripdata_2016-07.csv</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fare_amount, y:2016, m:7</th>\n",
       "      <td>green_tripdata_2016-06.csv</td>\n",
       "      <td>green_tripdata_2016-07.csv</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extra, y:2016, m:7</th>\n",
       "      <td>green_tripdata_2016-06.csv</td>\n",
       "      <td>green_tripdata_2016-07.csv</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mta_tax, y:2016, m:7</th>\n",
       "      <td>green_tripdata_2016-06.csv</td>\n",
       "      <td>green_tripdata_2016-07.csv</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tip_amount, y:2016, m:7</th>\n",
       "      <td>green_tripdata_2016-06.csv</td>\n",
       "      <td>green_tripdata_2016-07.csv</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tolls_amount, y:2016, m:7</th>\n",
       "      <td>green_tripdata_2016-06.csv</td>\n",
       "      <td>green_tripdata_2016-07.csv</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ehail_fee, y:2016, m:7</th>\n",
       "      <td>green_tripdata_2016-06.csv</td>\n",
       "      <td>green_tripdata_2016-07.csv</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>improvement_surcharge, y:2016, m:7</th>\n",
       "      <td>green_tripdata_2016-06.csv</td>\n",
       "      <td>green_tripdata_2016-07.csv</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_amount, y:2016, m:7</th>\n",
       "      <td>green_tripdata_2016-06.csv</td>\n",
       "      <td>green_tripdata_2016-07.csv</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>payment_type, y:2016, m:7</th>\n",
       "      <td>green_tripdata_2016-06.csv</td>\n",
       "      <td>green_tripdata_2016-07.csv</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_type, y:2016, m:7</th>\n",
       "      <td>green_tripdata_2016-06.csv</td>\n",
       "      <td>green_tripdata_2016-07.csv</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        file 0  \\\n",
       "total_amount, y:2015, m:1           green_tripdata_2014-12.csv   \n",
       "payment_type, y:2015, m:1           green_tripdata_2014-12.csv   \n",
       "trip_type, y:2015, m:1              green_tripdata_2014-12.csv   \n",
       "passenger_count, y:2016, m:7        green_tripdata_2016-06.csv   \n",
       "trip_distance, y:2016, m:7          green_tripdata_2016-06.csv   \n",
       "fare_amount, y:2016, m:7            green_tripdata_2016-06.csv   \n",
       "extra, y:2016, m:7                  green_tripdata_2016-06.csv   \n",
       "mta_tax, y:2016, m:7                green_tripdata_2016-06.csv   \n",
       "tip_amount, y:2016, m:7             green_tripdata_2016-06.csv   \n",
       "tolls_amount, y:2016, m:7           green_tripdata_2016-06.csv   \n",
       "ehail_fee, y:2016, m:7              green_tripdata_2016-06.csv   \n",
       "improvement_surcharge, y:2016, m:7  green_tripdata_2016-06.csv   \n",
       "total_amount, y:2016, m:7           green_tripdata_2016-06.csv   \n",
       "payment_type, y:2016, m:7           green_tripdata_2016-06.csv   \n",
       "trip_type, y:2016, m:7              green_tripdata_2016-06.csv   \n",
       "\n",
       "                                                        file 1 index 0 index 1  \n",
       "total_amount, y:2015, m:1           green_tripdata_2015-01.csv      17      18  \n",
       "payment_type, y:2015, m:1           green_tripdata_2015-01.csv      18      19  \n",
       "trip_type, y:2015, m:1              green_tripdata_2015-01.csv      19      20  \n",
       "passenger_count, y:2016, m:7        green_tripdata_2016-07.csv       9       7  \n",
       "trip_distance, y:2016, m:7          green_tripdata_2016-07.csv      10       8  \n",
       "fare_amount, y:2016, m:7            green_tripdata_2016-07.csv      11       9  \n",
       "extra, y:2016, m:7                  green_tripdata_2016-07.csv      12      10  \n",
       "mta_tax, y:2016, m:7                green_tripdata_2016-07.csv      13      11  \n",
       "tip_amount, y:2016, m:7             green_tripdata_2016-07.csv      14      12  \n",
       "tolls_amount, y:2016, m:7           green_tripdata_2016-07.csv      15      13  \n",
       "ehail_fee, y:2016, m:7              green_tripdata_2016-07.csv      16      14  \n",
       "improvement_surcharge, y:2016, m:7  green_tripdata_2016-07.csv      17      15  \n",
       "total_amount, y:2016, m:7           green_tripdata_2016-07.csv      18      16  \n",
       "payment_type, y:2016, m:7           green_tripdata_2016-07.csv      19      17  \n",
       "trip_type, y:2016, m:7              green_tripdata_2016-07.csv      20      18  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_schema[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of schema changes for yellow cab data files\n",
    "\n",
    "Analyze the schema changes for the Yellow taxi data files. Write down your conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_missing = date_analysis(filepaths[3])\n",
    "yellow_schema = column_analysis(filepaths[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following months are missing from this sub-dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2019, 7), (2019, 8), (2019, 9), (2019, 10), (2019, 11), (2019, 12), (2020, 3)]\n"
     ]
    }
   ],
   "source": [
    "print(yellow_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following files, the schema is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(yellow_schema[1])\n",
    "yellow_sameSchema = yellow_schema[1]\n",
    "#%store yellow_sameSchema\n",
    "with open(v_direc + \"yellow_sameSchema\",'wb') as yellowsameSchema:\n",
    "    pickle.dump(yellow_sameSchema,yellowsameSchema)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In or after the following files, there was the following schema change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dropped from yellow_tripdata_2009-12.csv</th>\n",
       "      <td>total_amt</td>\n",
       "      <td>start_lon</td>\n",
       "      <td>tolls_amt</td>\n",
       "      <td>start_lat</td>\n",
       "      <td>tip_amt</td>\n",
       "      <td>trip_dropoff_datetime</td>\n",
       "      <td>end_lat</td>\n",
       "      <td>vendor_name</td>\n",
       "      <td>trip_pickup_datetime</td>\n",
       "      <td>fare_amt</td>\n",
       "      <td>end_lon</td>\n",
       "      <td>store_and_forward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>added to yellow_tripdata_2010-01.csv</th>\n",
       "      <td>vendor_id</td>\n",
       "      <td>tolls_amount</td>\n",
       "      <td>dropoff_latitude</td>\n",
       "      <td>total_amount</td>\n",
       "      <td>store_and_fwd_flag</td>\n",
       "      <td>tip_amount</td>\n",
       "      <td>dropoff_datetime</td>\n",
       "      <td>dropoff_longitude</td>\n",
       "      <td>fare_amount</td>\n",
       "      <td>pickup_longitude</td>\n",
       "      <td>pickup_latitude</td>\n",
       "      <td>pickup_datetime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropped from yellow_tripdata_2014-12.csv</th>\n",
       "      <td>vendor_id</td>\n",
       "      <td>dropoff_datetime</td>\n",
       "      <td>rate_code</td>\n",
       "      <td>surcharge</td>\n",
       "      <td>pickup_datetime</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>added to yellow_tripdata_2015-01.csv</th>\n",
       "      <td>ratecodeid</td>\n",
       "      <td>improvement_surcharge</td>\n",
       "      <td>tpep_dropoff_datetime</td>\n",
       "      <td>tpep_pickup_datetime</td>\n",
       "      <td>extra</td>\n",
       "      <td>vendorid</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropped from yellow_tripdata_2016-06.csv</th>\n",
       "      <td>pickup_longitude</td>\n",
       "      <td>dropoff_latitude</td>\n",
       "      <td>pickup_latitude</td>\n",
       "      <td>dropoff_longitude</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>added to yellow_tripdata_2016-07.csv</th>\n",
       "      <td>pulocationid</td>\n",
       "      <td>dolocationid</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropped from yellow_tripdata_2018-12.csv</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>added to yellow_tripdata_2019-01.csv</th>\n",
       "      <td>congestion_surcharge</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            0   \\\n",
       "dropped from yellow_tripdata_2009-12.csv             total_amt   \n",
       "added to yellow_tripdata_2010-01.csv                 vendor_id   \n",
       "dropped from yellow_tripdata_2014-12.csv             vendor_id   \n",
       "added to yellow_tripdata_2015-01.csv                ratecodeid   \n",
       "dropped from yellow_tripdata_2016-06.csv      pickup_longitude   \n",
       "added to yellow_tripdata_2016-07.csv              pulocationid   \n",
       "dropped from yellow_tripdata_2018-12.csv                  None   \n",
       "added to yellow_tripdata_2019-01.csv      congestion_surcharge   \n",
       "\n",
       "                                                             1   \\\n",
       "dropped from yellow_tripdata_2009-12.csv              start_lon   \n",
       "added to yellow_tripdata_2010-01.csv               tolls_amount   \n",
       "dropped from yellow_tripdata_2014-12.csv       dropoff_datetime   \n",
       "added to yellow_tripdata_2015-01.csv      improvement_surcharge   \n",
       "dropped from yellow_tripdata_2016-06.csv       dropoff_latitude   \n",
       "added to yellow_tripdata_2016-07.csv               dolocationid   \n",
       "dropped from yellow_tripdata_2018-12.csv                   None   \n",
       "added to yellow_tripdata_2019-01.csv                       None   \n",
       "\n",
       "                                                             2   \\\n",
       "dropped from yellow_tripdata_2009-12.csv              tolls_amt   \n",
       "added to yellow_tripdata_2010-01.csv           dropoff_latitude   \n",
       "dropped from yellow_tripdata_2014-12.csv              rate_code   \n",
       "added to yellow_tripdata_2015-01.csv      tpep_dropoff_datetime   \n",
       "dropped from yellow_tripdata_2016-06.csv        pickup_latitude   \n",
       "added to yellow_tripdata_2016-07.csv                       None   \n",
       "dropped from yellow_tripdata_2018-12.csv                   None   \n",
       "added to yellow_tripdata_2019-01.csv                       None   \n",
       "\n",
       "                                                            3   \\\n",
       "dropped from yellow_tripdata_2009-12.csv             start_lat   \n",
       "added to yellow_tripdata_2010-01.csv              total_amount   \n",
       "dropped from yellow_tripdata_2014-12.csv             surcharge   \n",
       "added to yellow_tripdata_2015-01.csv      tpep_pickup_datetime   \n",
       "dropped from yellow_tripdata_2016-06.csv     dropoff_longitude   \n",
       "added to yellow_tripdata_2016-07.csv                      None   \n",
       "dropped from yellow_tripdata_2018-12.csv                  None   \n",
       "added to yellow_tripdata_2019-01.csv                      None   \n",
       "\n",
       "                                                          4   \\\n",
       "dropped from yellow_tripdata_2009-12.csv             tip_amt   \n",
       "added to yellow_tripdata_2010-01.csv      store_and_fwd_flag   \n",
       "dropped from yellow_tripdata_2014-12.csv     pickup_datetime   \n",
       "added to yellow_tripdata_2015-01.csv                   extra   \n",
       "dropped from yellow_tripdata_2016-06.csv                None   \n",
       "added to yellow_tripdata_2016-07.csv                    None   \n",
       "dropped from yellow_tripdata_2018-12.csv                None   \n",
       "added to yellow_tripdata_2019-01.csv                    None   \n",
       "\n",
       "                                                             5   \\\n",
       "dropped from yellow_tripdata_2009-12.csv  trip_dropoff_datetime   \n",
       "added to yellow_tripdata_2010-01.csv                 tip_amount   \n",
       "dropped from yellow_tripdata_2014-12.csv                   None   \n",
       "added to yellow_tripdata_2015-01.csv                   vendorid   \n",
       "dropped from yellow_tripdata_2016-06.csv                   None   \n",
       "added to yellow_tripdata_2016-07.csv                       None   \n",
       "dropped from yellow_tripdata_2018-12.csv                   None   \n",
       "added to yellow_tripdata_2019-01.csv                       None   \n",
       "\n",
       "                                                        6                  7   \\\n",
       "dropped from yellow_tripdata_2009-12.csv           end_lat        vendor_name   \n",
       "added to yellow_tripdata_2010-01.csv      dropoff_datetime  dropoff_longitude   \n",
       "dropped from yellow_tripdata_2014-12.csv              None               None   \n",
       "added to yellow_tripdata_2015-01.csv                  None               None   \n",
       "dropped from yellow_tripdata_2016-06.csv              None               None   \n",
       "added to yellow_tripdata_2016-07.csv                  None               None   \n",
       "dropped from yellow_tripdata_2018-12.csv              None               None   \n",
       "added to yellow_tripdata_2019-01.csv                  None               None   \n",
       "\n",
       "                                                            8   \\\n",
       "dropped from yellow_tripdata_2009-12.csv  trip_pickup_datetime   \n",
       "added to yellow_tripdata_2010-01.csv               fare_amount   \n",
       "dropped from yellow_tripdata_2014-12.csv                  None   \n",
       "added to yellow_tripdata_2015-01.csv                      None   \n",
       "dropped from yellow_tripdata_2016-06.csv                  None   \n",
       "added to yellow_tripdata_2016-07.csv                      None   \n",
       "dropped from yellow_tripdata_2018-12.csv                  None   \n",
       "added to yellow_tripdata_2019-01.csv                      None   \n",
       "\n",
       "                                                        9                10  \\\n",
       "dropped from yellow_tripdata_2009-12.csv          fare_amt          end_lon   \n",
       "added to yellow_tripdata_2010-01.csv      pickup_longitude  pickup_latitude   \n",
       "dropped from yellow_tripdata_2014-12.csv              None             None   \n",
       "added to yellow_tripdata_2015-01.csv                  None             None   \n",
       "dropped from yellow_tripdata_2016-06.csv              None             None   \n",
       "added to yellow_tripdata_2016-07.csv                  None             None   \n",
       "dropped from yellow_tripdata_2018-12.csv              None             None   \n",
       "added to yellow_tripdata_2019-01.csv                  None             None   \n",
       "\n",
       "                                                         11  \n",
       "dropped from yellow_tripdata_2009-12.csv  store_and_forward  \n",
       "added to yellow_tripdata_2010-01.csv        pickup_datetime  \n",
       "dropped from yellow_tripdata_2014-12.csv               None  \n",
       "added to yellow_tripdata_2015-01.csv                   None  \n",
       "dropped from yellow_tripdata_2016-06.csv               None  \n",
       "added to yellow_tripdata_2016-07.csv                   None  \n",
       "dropped from yellow_tripdata_2018-12.csv               None  \n",
       "added to yellow_tripdata_2019-01.csv                   None  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_schema[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final schema was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge']\n"
     ]
    }
   ],
   "source": [
    "print(file_schema(filepaths[3][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations on schema changes  \n",
    "\n",
    "2016-06 and earlier:\n",
    "- dolocationid and pulocationid can be computed from dropoff_longitude | dropoff_latitude and pickup_longitude | pickup_latitude\n",
    "\n",
    "2014-12 and earlier:\n",
    "- surcharge = improvement_surcharge\n",
    "- dropoff_datetime = tpep_dropoff_datetime\n",
    "- vendor_id = vendorid\n",
    "- note: vendor_id to vendorid values changed from CMT and VTS to 1 and 2\n",
    "- rate_code = ratecodeid\n",
    "- pickup_datetime = tpep_pickup_datetime\n",
    "\n",
    "2009-12 and earlier:\n",
    "- total_amt = total_amount\n",
    "- store_and_forward = store_and_fwd_flag\n",
    "- fare_amt = fare_amount\n",
    "- tip_amt = tip_amount\n",
    "- end_lon = dropoff_longitude\n",
    "- trip_pickup_datetime = tpep_pickup_datetime\n",
    "- tolls_amt = tolls_amount\n",
    "- end_lat = dropoff_latitude\n",
    "- trip_dropoff_datetime = tpep_dropoff_datetime\n",
    "- vendor_name = vendor_id = vendorid\n",
    "- start_lon = pickup_longitude\n",
    "- start_lat = pickup_latitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index of the following variables has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file 0</th>\n",
       "      <th>file 1</th>\n",
       "      <th>index 0</th>\n",
       "      <th>index 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_amount, y:2015, m:1</th>\n",
       "      <td>yellow_tripdata_2014-12.csv</td>\n",
       "      <td>yellow_tripdata_2015-01.csv</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratecodeid, y:2016, m:7</th>\n",
       "      <td>yellow_tripdata_2016-06.csv</td>\n",
       "      <td>yellow_tripdata_2016-07.csv</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_and_fwd_flag, y:2016, m:7</th>\n",
       "      <td>yellow_tripdata_2016-06.csv</td>\n",
       "      <td>yellow_tripdata_2016-07.csv</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>payment_type, y:2016, m:7</th>\n",
       "      <td>yellow_tripdata_2016-06.csv</td>\n",
       "      <td>yellow_tripdata_2016-07.csv</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fare_amount, y:2016, m:7</th>\n",
       "      <td>yellow_tripdata_2016-06.csv</td>\n",
       "      <td>yellow_tripdata_2016-07.csv</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extra, y:2016, m:7</th>\n",
       "      <td>yellow_tripdata_2016-06.csv</td>\n",
       "      <td>yellow_tripdata_2016-07.csv</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mta_tax, y:2016, m:7</th>\n",
       "      <td>yellow_tripdata_2016-06.csv</td>\n",
       "      <td>yellow_tripdata_2016-07.csv</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tip_amount, y:2016, m:7</th>\n",
       "      <td>yellow_tripdata_2016-06.csv</td>\n",
       "      <td>yellow_tripdata_2016-07.csv</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tolls_amount, y:2016, m:7</th>\n",
       "      <td>yellow_tripdata_2016-06.csv</td>\n",
       "      <td>yellow_tripdata_2016-07.csv</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>improvement_surcharge, y:2016, m:7</th>\n",
       "      <td>yellow_tripdata_2016-06.csv</td>\n",
       "      <td>yellow_tripdata_2016-07.csv</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_amount, y:2016, m:7</th>\n",
       "      <td>yellow_tripdata_2016-06.csv</td>\n",
       "      <td>yellow_tripdata_2016-07.csv</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         file 0  \\\n",
       "total_amount, y:2015, m:1           yellow_tripdata_2014-12.csv   \n",
       "ratecodeid, y:2016, m:7             yellow_tripdata_2016-06.csv   \n",
       "store_and_fwd_flag, y:2016, m:7     yellow_tripdata_2016-06.csv   \n",
       "payment_type, y:2016, m:7           yellow_tripdata_2016-06.csv   \n",
       "fare_amount, y:2016, m:7            yellow_tripdata_2016-06.csv   \n",
       "extra, y:2016, m:7                  yellow_tripdata_2016-06.csv   \n",
       "mta_tax, y:2016, m:7                yellow_tripdata_2016-06.csv   \n",
       "tip_amount, y:2016, m:7             yellow_tripdata_2016-06.csv   \n",
       "tolls_amount, y:2016, m:7           yellow_tripdata_2016-06.csv   \n",
       "improvement_surcharge, y:2016, m:7  yellow_tripdata_2016-06.csv   \n",
       "total_amount, y:2016, m:7           yellow_tripdata_2016-06.csv   \n",
       "\n",
       "                                                         file 1 index 0  \\\n",
       "total_amount, y:2015, m:1           yellow_tripdata_2015-01.csv      17   \n",
       "ratecodeid, y:2016, m:7             yellow_tripdata_2016-07.csv       7   \n",
       "store_and_fwd_flag, y:2016, m:7     yellow_tripdata_2016-07.csv       8   \n",
       "payment_type, y:2016, m:7           yellow_tripdata_2016-07.csv      11   \n",
       "fare_amount, y:2016, m:7            yellow_tripdata_2016-07.csv      12   \n",
       "extra, y:2016, m:7                  yellow_tripdata_2016-07.csv      13   \n",
       "mta_tax, y:2016, m:7                yellow_tripdata_2016-07.csv      14   \n",
       "tip_amount, y:2016, m:7             yellow_tripdata_2016-07.csv      15   \n",
       "tolls_amount, y:2016, m:7           yellow_tripdata_2016-07.csv      16   \n",
       "improvement_surcharge, y:2016, m:7  yellow_tripdata_2016-07.csv      17   \n",
       "total_amount, y:2016, m:7           yellow_tripdata_2016-07.csv      18   \n",
       "\n",
       "                                   index 1  \n",
       "total_amount, y:2015, m:1               18  \n",
       "ratecodeid, y:2016, m:7                  5  \n",
       "store_and_fwd_flag, y:2016, m:7          6  \n",
       "payment_type, y:2016, m:7                9  \n",
       "fare_amount, y:2016, m:7                10  \n",
       "extra, y:2016, m:7                      11  \n",
       "mta_tax, y:2016, m:7                    12  \n",
       "tip_amount, y:2016, m:7                 13  \n",
       "tolls_amount, y:2016, m:7               14  \n",
       "improvement_surcharge, y:2016, m:7      15  \n",
       "total_amount, y:2016, m:7               16  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_schema[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final observations\n",
    "\n",
    "There are a number of variables which have been re-named (and probably had their format changed) through the names, and column order changes are common. In order to integrate the data, all schemas can be re-ordered to some common format without much loss of efficiency (as the majority of data will require this anyway). The missing years again are worrying, but given that they exist on the TLC website, there should be no issue assuming this is an artifact of the sampling process.  \n",
    "\n",
    "Please run our code for 2.2 on the same machine/instance in order to correctly access the variables which we have stored in this 2.1 implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
