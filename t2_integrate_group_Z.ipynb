{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction  \n",
    "### The following code was tested on:  \n",
    "Python 3+ running on...  \n",
    "  \n",
    "Microsoft Surface Go:\n",
    "- CPU - Intel Pentium 4415Y @ 1.60 GHz, 2 cores, 4 threads  \n",
    "- RAM - 8 GB\n",
    "\n",
    "ULB Virtual Machine on Surface Go with:\n",
    "- CPU - 2 cores dedicated  \n",
    "- RAM - 4096 MB dedicated\n",
    "\n",
    "ULB Cluster - user epb123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integration\n",
    "\n",
    "For each sub-dataset, write (and execute) code that converts a file (using possibly an old schema) into a file that has the new, latest schema version.\n",
    "\n",
    "Your conversion code should not modify the original files, but instead create a new file. \n",
    "\n",
    "Be sure to explain the design behind your conversion functions!\n",
    "\n",
    "The data integration step is highly parallellizable. Therefore, your solution on this part\n",
    "**must** be written in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import various packages to read files as well as to deal with the shapefile of TLC location data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various imports\n",
    "import os \n",
    "import glob\n",
    "import math\n",
    "import pickle\n",
    "import ntpath\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_direc = pickle.load(open(\"v_direc\",'rb'))\n",
    "\n",
    "data_directory = pickle.load(open(v_direc + \"data_folder\",'rb'))\n",
    "save_directory = pickle.load(open(v_direc + \"save_directory\",'rb'))\n",
    "\n",
    "# Set this = True if you are running the code on the ULB cluster\n",
    "cluster = True\n",
    "\n",
    "if cluster == True:\n",
    "    # Please set the directory of your geographic metadata\n",
    "    shapefile_directory = r\"/home/epb123/shapefile\"\n",
    "    old_data_directory = data_directory\n",
    "    data_directory = r\"hdfs:///user/epb199/data/\"\n",
    "elif cluster == False:\n",
    "    shapefile_directory = r\"/media/sf_Distributed/Project cleaner/shapefile\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will start Spark in local mode, using all available CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Start Spark in LOCAL mode\n",
    "# -------------------------------\n",
    "\n",
    "if cluster == False:\n",
    "\n",
    "    #This is needed to start a Spark session from the notebook\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # Stop any previously running spark session\n",
    "    try: \n",
    "        spark\n",
    "        print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "        spark.stop()\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "    # Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"H600Project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    #When dealing with RDDs, we work the sparkContext object. \n",
    "    #See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "    sc=spark.sparkContext\n",
    "\n",
    "    from pyspark.sql.types import *\n",
    "    from pyspark.sql.functions import col,udf,lit,struct\n",
    "    from pyspark.sql import SQLContext\n",
    "    sql_sc = SQLContext(sc)\n",
    "\n",
    "    print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will start Spark on the ULB cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.4\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# Start Spark on the ULB cluster ,with YARN as resource manager\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "if cluster == True:\n",
    "\n",
    "    #This is needed to start a Spark session from the notebook\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # We need to set the following environment variable, so that Spark knows where YARN runs\n",
    "    os.environ['HADOOP_CONF_DIR']=\"/etc/hadoop/conf\"\n",
    "\n",
    "    # Since we are accessing spark through it's python API, we need to make sure that all executor\n",
    "    # instances run the same version of python. \n",
    "    # (and we want Anaconda to be used, so we have access to numpy, pandas, and so forth)\n",
    "    # You will likely need to adjust this path if your run on a different cluster\n",
    "    os.environ['PYSPARK_PYTHON']=\"/usr/local/anaconda3/bin/python\"\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON']=\"/usr/local/anaconda3/bin/python\"\n",
    "\n",
    "    #The following lines are just there to allow this cell to be re-executed multiple times:\n",
    "    #if a spark session was already started, we stop it before starting a new one\n",
    "    #(there can be only one spark context per jupyter notebook)\n",
    "    try: \n",
    "        spark\n",
    "        print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "        spark.stop()\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\") \\\n",
    "        .config(\"spark.executor.instances\",\"4\") \\\n",
    "        .appName(\"H600Project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    #When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "    sc=spark.sparkContext\n",
    "\n",
    "    from pyspark.sql.types import *\n",
    "    from pyspark.sql.functions import col,udf,lit,struct\n",
    "    from pyspark.sql import SQLContext\n",
    "    sql_sc = SQLContext(sc)\n",
    "\n",
    "    print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'public00.hpda.ulb.ac.be'),\n",
       " ('spark.executor.instances', '4'),\n",
       " ('spark.driver.port', '44961'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.driver.appUIAddress', 'http://publiclogin.hpda.ulb.ac.be:4043'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', 'publiclogin.hpda.ulb.ac.be'),\n",
       " ('spark.app.name', 'H600Project'),\n",
       " ('spark.app.id', 'application_1610647739131_0066'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://public00.hpda.ulb.ac.be:8088/proxy/application_1610647739131_0066'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that spark context is working, print its configuration\n",
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various helpful functions for later use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start with a function to create a PySpark dataframe from a file at a given filepath/address. We chose to work with dataframes instead of RDDs in our implementation because of the additional optimisation given to Spark dataframes vs. RDDs when running in Python.  \n",
    "\n",
    "We import the CSVs without inferring any schema - this appeared to be an expensive operation and while the data is still not cleaned, casting everything manually is preferable to having PySpark guess for us.  \n",
    "\n",
    "We also directly change variable names to be lowercase and stripped for simplicity's sake. We use the select() method to do this rather than withColumnRenamed() as withColumnRenamed() appears to have significant performance issues as the number of columns increases.  \n",
    "\n",
    "We had to change tactics and load the dataframes with Pandas first, before converting, due to limitations running on the ULB cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(address):\n",
    "# Takes csv from address given and creats a dataframe\n",
    "# Returns given csv as a dataframe\n",
    "# Schema of this csv will be in string format!!!!!\n",
    "    # Reading a csv from address while taking headers as a schema\n",
    "    # To infer schema type (i.e. int, float), add option inferSchema = True\n",
    "        \n",
    "    # When running in YARN mode, spark expects its input to come from HDFS by default\n",
    "    # Copying files to local Hadoop would prevent a 'Path does not exist' error on the cluster\n",
    "    # but this is not allowed\n",
    "    # We can therefore only run the below in local mode, but there is a workaround through pandas\n",
    "    \n",
    "    # When opening these through pandas, we have to add the \"error_bad_lines = False\" argument\n",
    "    # This is because PySpark seemed to drop rows with an extra column, but Pandas did not\n",
    "    \n",
    "    #Note - opening as Pandas appears to add a recordID in an unnamed first column\n",
    "    \n",
    "    # Code which does not work on cluster:\n",
    "    # Solved by substituting HDFS file location above - as mentioned, Spark requires an HDFS copy of the files \n",
    "    \n",
    "    addressDF = spark.read.format('csv').option(\"header\", True).load(address)\n",
    "\n",
    "    print(\"opened\", address)\n",
    "\n",
    "\n",
    "    # pandas_DF = pd.read_csv(address, warn_bad_lines = True, error_bad_lines = False)\n",
    "    # addressDF = sql_sc.createDataFrame(pandas_DF.astype(str))\n",
    "    # Replacing column names with lowercase and stripped versions\n",
    "    # Note - not using withColumnRenamed() due to lower efficiency as column number increases\n",
    "        # (it is probably fine here - issues do not start until much bigger column numbers, but wanted to learn)\n",
    "    replacements = {i:i.lower().strip() for i in addressDF.columns}\n",
    "    addressDF = addressDF.select([col(j).alias(replacements.get(j, j)) for j in addressDF.columns])\n",
    "    \n",
    "    return addressDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the comments say, this function will be very useful when renaming columns later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_colNames(df, namesOg, newNames):\n",
    "# Takes a dataframe and two lists - original column names and names to replace them (does not have to be all names)\n",
    "# Returns the dataframe with renamed columns\n",
    "# LISTS MUST BE IN THE ORDER OF REPLACEMENT - i.e. namesOg[1] will be replaced by newNames[1]\n",
    "    \n",
    "    replacements = {}\n",
    "    for i in range(len(namesOg)):\n",
    "        replacements[namesOg[i]] = newNames[i]\n",
    "    df = df.select([col(j).alias(replacements.get(j, j)) for j in df.columns])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is just to make it easy to add additional blank columns (for instance to add \"congestion_surcharge\" to early datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_empty_cols(df, newNames):\n",
    "# Takes a dataframe and a list of names for new empty columns\n",
    "# Returns the dataframe with new columns\n",
    "# Not using multiple withColumn() and a for loop, as this is more efficient\n",
    "\n",
    "    cols = [\"*\"] + [lit(None).cast(StringType()).alias(i) for i in newNames]\n",
    "    df = df.select(cols)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't actually use this function aside from testing, but as you can imagine it is very useful for that purpose. I have left it uncommented for usage convenience, but it could easily be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_fromSchema(schema):\n",
    "# Takes given column names as a list and creates an empty dataframe with that schema in string format\n",
    "# Returns dataframe with schema and no values\n",
    "    fields = []\n",
    "    \n",
    "    for i in schema:\n",
    "        fields.append(StructField(i, StringType(), True))\n",
    "    \n",
    "    dfSchema = StructType(fields)\n",
    "    emptyDF = spark.createDataFrame([],dfSchema)\n",
    "    \n",
    "    return emptyDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a function to test further code which has been maintained for it's usefulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_schema(address):   \n",
    "# Function takes a csv from the given address - NOT IN SPARK\n",
    "# Returns a list of stripped and lowercase column names\n",
    "    \n",
    "    headers = [item.lower().strip() for item in list(pd.read_csv(address,nrows=0).columns)]\n",
    "    \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying not to modify the original dataset, and making a folder to store the new files is necessary. We use os.path.join() because it is less error prone than working directly with the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(newFolderName, newFolderPath):\n",
    "# Function takes a folder name and a path and creates a new folder/directory\n",
    "# Function returns folder path\n",
    "    \n",
    "    fullFolderPath = os.path.join(newFolderPath, newFolderName)\n",
    "    \n",
    "    # Folder might already have been created\n",
    "    try:\n",
    "        os.mkdir(fullFolderPath)\n",
    "        print(\"new folder created at \"+ fullFolderPath)\n",
    "    except FileExistsError:\n",
    "        print(\"folder already exists at \" + fullFolderPath)\n",
    "    \n",
    "    return fullFolderPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we need to save the modified dataframes. PySpark by default will write one csv per partition, i.e. if the database has been split over 20 processors, you will end up with 20 csv files. Here we are using coalesce() to avoid that - if the datasets were too big for local memory, we would omit this and continue working with the partitioned CSVs, but our maximum filesize here is ~5mb.  \n",
    "\n",
    "Due to limitations in getting the code to run on the cluster, we had to revert to converting our PySpark dataframes to Pandas and saving through Pandas methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(saveDF, saveName, savePath):\n",
    "# Function takes a dataframe, filename, and filepath and saves the dataframe\n",
    "# csv will be saved inside a folder with the given saveName, along with a text file _SUCCESS\n",
    "# Want to use this as it is quicker than converting to PandasDF\n",
    "\n",
    "# WARNING: WILL OVERWRITE PREVIOUS VERSIONS\n",
    "    \n",
    "    # I have not been given write permissions on the cluster and cannot directly save this through HDFS\n",
    "    # Another workaround is to just move via pandas again\n",
    "    # If working locally, the first line of code below can be uncommented and used instead\n",
    "    \n",
    "# NOTE: coalesce(1) can be used due to small data size - otherwise, must be written as multiple files due to memory restrictions \n",
    "    \n",
    "    # Code which does not work on cluster:\n",
    "    # saveDF.coalesce(1).write.options(header = True).mode(\"overwrite\").csv(os.path.join(savePath, saveName))\n",
    "    saveDF.toPandas().to_csv(os.path.join(savePath, saveName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base code here was taken from the H600 shapefile usage example, but it has been modified for work with a PySpark UDF. Essentially, the easiest and quickest way to update the coordinate data in older schemas is to pass this function to all rows in a given set of columns in our PySpark dataframe, and in order to do that we need to cast it as a UDF.  \n",
    "\n",
    "Of course, UDFs are not optimal for use with PySpark, but given the use of the spatial index this would be very complicated to write with PySpark functions and might end up being slower regardless.  \n",
    "\n",
    "One very interesting point is that the spatial index seems to require generation within the UDF itself. This is unfortunate as it would have been better to generate it once and then broadcast it to the worker nodes. We have instead settled for broadcasting the zones themselves to avoid having to pass at least this to each node with every function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLC shapefile loaded\n",
      "Location conversion function defined as UDF\n"
     ]
    }
   ],
   "source": [
    "# Load the shapefile, this yields a GeoDataFrame that has a row for each zone\n",
    "zones = gpd.read_file(os.path.join(shapefile_directory, \"taxi_zones.shp\"))\n",
    "\n",
    "# Now re-project the coordinates to the CRS EPSG 4326, which is the CRS used in GPS (https://epsg.io/4326)\n",
    "zones = zones.to_crs({'init':'epsg:4326'})\n",
    "\n",
    "# Broadcast zones variable because otherwise it's going to be passed across to spark nodes very often\n",
    "zonesBC = sc.broadcast(zones)\n",
    "\n",
    "# Create an R-tree index on geopanda dataframe would go here and be broadcast, but it does not work when outside of the UDF\n",
    "# My theory is that it does not work because it is a class and not a normal python variable\n",
    "#rtree = zones.sindex\n",
    "#rtreeBC = sc.broadcast(rtree)\n",
    "\n",
    "print(\"TLC shapefile loaded\")\n",
    "\n",
    "def latLong_ID(lat, long):\n",
    "# Function to calculate NYT locationID from a given latitude and longitude\n",
    "# Function returns the locationID (cast as a string for now!)\n",
    "\n",
    "    if lat == \"\":\n",
    "        return None\n",
    "    if long == \"\":\n",
    "        return None\n",
    "    \n",
    "    if lat == None:\n",
    "        return None\n",
    "    if long == None:\n",
    "        return None\n",
    "    \n",
    "    if lat == \"NaN\":\n",
    "        return None\n",
    "    if long == \"NaN\":\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        latfloat = float(lat)\n",
    "    except:\n",
    "        return None\n",
    "    try:\n",
    "        longfloat = float(long)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    query_point = Point(longfloat, latfloat)\n",
    "    \n",
    "    # Unknown why, but on the cluster, yellow 2010-08 has issues with \"RTreeError: Coordinates must not have minimums more than maximums\"\n",
    "    # This only happens on the cluster, but additional error checking can be built in regardless\n",
    "    if query_point.is_valid:\n",
    "        # Use spatial index to avoid looping through all zones\n",
    "        rtree = zonesBC.value.sindex\n",
    "\n",
    "        for i in rtree.intersection(query_point.bounds):\n",
    "            zoneID = zonesBC.value.iloc[i]\n",
    "            if zoneID.geometry.contains(query_point):\n",
    "                return str(zoneID.LocationID)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return str(\"failed to match\")\n",
    "\n",
    "# Define function as a UDF for pyspark usage\n",
    "udflatLong_ID = udf(latLong_ID, StringType())\n",
    "\n",
    "print(\"Location conversion function defined as UDF\")\n",
    "\n",
    "def add_locationID(dfLocMod):\n",
    "# Adds the location ID as a new dataframe column, calculated from other columns in the dataframe\n",
    "# Takes a dataframe\n",
    "# Returns that dataframe with modifications\n",
    "    \n",
    "    dfLocMod = dfLocMod.withColumn('pulocationid', udflatLong_ID('pickup_latitude', 'pickup_longitude'))\n",
    "    dfLocMod = dfLocMod.withColumn('dolocationid', udflatLong_ID('dropoff_latitude', 'dropoff_longitude'))\n",
    "\n",
    "    return dfLocMod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test the location function, this will let you check that it's working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # location test\n",
    "# print(latLong_ID(\"40.677341461181641\",\"-73.943748474121094\"))\n",
    "# print(latLong_ID(\"40.683746337890625\",\"-73.927146911621094\"))\n",
    "# dfxxx = spark.createDataFrame([('40.677341461181641','-73.943748474121094'),('40.683746337890625','-73.927146911621094')],['latitude','longitude'])\n",
    "# dfxxx = dfxxx.withColumn('pulocationid', udflatLong_ID('latitude', 'longitude'))\n",
    "# dfxxx.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we need the filepaths and the files broken down by schema from 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover 2.1 variables from storage\n",
    "# %store -r filepaths\n",
    "filepaths = pickle.load(open(v_direc + \"filepaths\",'rb'))\n",
    "# %store -r fhv_sameSchema\n",
    "fhv_sameSchema = pickle.load(open(v_direc + \"fhv_sameSchema\",'rb'))\n",
    "# %store -r fhvhv_sameSchema\n",
    "fhvhv_sameSchema = pickle.load(open(v_direc + \"fhvhv_sameSchema\",'rb'))\n",
    "# %store -r green_sameSchema\n",
    "green_sameSchema = pickle.load(open(v_direc + \"green_sameSchema\",'rb'))\n",
    "# %store -r yellow_sameSchema\n",
    "yellow_sameSchema = pickle.load(open(v_direc + \"yellow_sameSchema\",'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating the respective schemas\n",
    "\n",
    "Below, we use the variables we just removed from storage to iterate through lists of lists, where each sub-list contains filepaths where the schema is the same. We can then apply one block of functions to each sub-list which brings it into line with the desired schema.  \n",
    "\n",
    "Given the analysis just completed in 2.1, the function blocks here have been created manually by tracking which transformations are required in each area. Since we have completed 2.1 results and analysis, this was simple. Given the structure of the code below, converting it to a function which takes a general file from the dataset as an argument would use the lists of which file belongs in which schema and it would be a matter of matching them. If the schema of this general file was unknown at the time of running 2.1, we could always just re-run 2.1's code - assuming the file belongs to an already existing schema (which appears to be a safe assumption), 2.1 would correctly classify it for use here.\n",
    "\n",
    "We could have also first merged all months inside a particular taxi type or sub-schema set into one dataframe and proceeded from there, which would possibly make better use of spark's parallelisation. However, given that these monthly data sets are a small sample of the real data (so the full dataset would make use of the parallelisation anyway), we proceed on a monthly basis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating FHV schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/output/integrated FHV\n"
     ]
    }
   ],
   "source": [
    "# Create folder for output files:\n",
    "fhv_folderPath = create_folder(\"integrated FHV\", save_directory)\n",
    "#%store fhv_folderPath\n",
    "with open(v_direc + \"fhv_folderPath\",'wb') as fhvfolderPath:\n",
    "    pickle.dump(fhv_folderPath,fhvfolderPath)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened hdfs:///user/epb199/data/fhv_tripdata_2015-01.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2015-02.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2015-03.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2015-04.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2015-05.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2015-06.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2015-07.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2015-08.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2015-09.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2015-10.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2015-11.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2015-12.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2016-01.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2016-02.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2016-03.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2016-04.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2016-05.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2016-06.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2016-07.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2016-08.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2016-09.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2016-10.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2016-11.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2016-12.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2017-01.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2017-02.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2017-03.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2017-04.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2017-05.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2017-06.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2017-07.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2017-08.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2017-09.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2017-10.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2017-11.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2017-12.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2018-01.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2018-02.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2018-03.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2018-04.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2018-05.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2018-06.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2018-07.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2018-08.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2018-09.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2018-10.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2018-11.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2018-12.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2019-01.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2019-02.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2019-03.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2019-04.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2019-05.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2019-06.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2019-07.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2019-08.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2019-09.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2019-10.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2019-11.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2020-01.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2020-03.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2020-04.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2020-05.csv\n",
      "opened hdfs:///user/epb199/data/fhv_tripdata_2020-06.csv\n",
      "FHV schema integration complete\n"
     ]
    }
   ],
   "source": [
    "# Want to match all files to the sorted version of the final schema\n",
    "# With identified schema type, rename columns as necessary then drop columns as necessary, sort all columns to ensure correct ordering\n",
    "\n",
    "#fhv_sameSchema[0] is older\n",
    "    #[0] requires:\n",
    "        #renaming locationid to pulocationid\n",
    "        #renaming pickup_date to pickup_datetime\n",
    "        #addition of dolocationid\n",
    "        #addition of dropoff_datetime\n",
    "        #addition of sr_flag\n",
    "        #sorting of final columns\n",
    "for i in fhv_sameSchema[0]:\n",
    "    if cluster == True:\n",
    "        i = data_directory + ntpath.basename(i)\n",
    "    dfi = create_df(i)\n",
    "    dfi = replace_colNames(dfi, ['locationid', 'pickup_date'], ['pulocationid', 'pickup_datetime'])\n",
    "    dfi = add_empty_cols(dfi, ['dolocationid', 'dropoff_datetime', 'sr_flag'])\n",
    "    dfi = dfi.select(sorted(dfi.columns))\n",
    "    save_df(dfi, ntpath.basename(i), fhv_folderPath)\n",
    "\n",
    "#fhv_sameSchema[1] is the same as [4], but it does not have variable sr_flag\n",
    "    #[1] requires:\n",
    "        #addition of sr_flag\n",
    "        #sorting of final columns\n",
    "for i in fhv_sameSchema[1]:\n",
    "    if cluster == True:\n",
    "        i = data_directory + ntpath.basename(i)\n",
    "    dfi = create_df(i)\n",
    "    # Using withColumn as just one column here\n",
    "    dfi = dfi.withColumn(\"sr_flag\",lit(None).cast(StringType()))\n",
    "    dfi = dfi.select(sorted(dfi.columns))\n",
    "    save_df(dfi, ntpath.basename(i), fhv_folderPath)\n",
    "    \n",
    "#fhv_sameSchema[2] is the same as [4] - it only requires sorting \n",
    "for i in fhv_sameSchema[2]:\n",
    "    if cluster == True:\n",
    "        i = data_directory + ntpath.basename(i)\n",
    "    dfi = create_df(i)\n",
    "    dfi = dfi.select(sorted(dfi.columns))\n",
    "    save_df(dfi, ntpath.basename(i), fhv_folderPath)\n",
    "\n",
    "#fhv_sameSchema[3] is the same as [4], but with dispatching_base_number containing data previously in dispatching_base_num\n",
    "    #[3] requires:\n",
    "        #the column dispatching_base_num to be dropped\n",
    "        #dispatching_base_number to be renamed to dispatching_base_num\n",
    "        #sorting of final columns\n",
    "for i in fhv_sameSchema[3]:\n",
    "    if cluster == True:\n",
    "        i = data_directory + ntpath.basename(i)\n",
    "    dfi = create_df(i)\n",
    "    dfi = dfi.drop(\"dispatching_base_num\")\n",
    "    dfi = replace_colNames(dfi, ['dispatching_base_number'], ['dispatching_base_num'])\n",
    "    dfi = dfi.select(sorted(dfi.columns))\n",
    "    save_df(dfi, ntpath.basename(i), fhv_folderPath)\n",
    "    \n",
    "#fhv_sameSchema[4] is the final schema - it only requires sorting\n",
    "for i in fhv_sameSchema[4]:\n",
    "    if cluster == True:\n",
    "        i = data_directory + ntpath.basename(i)\n",
    "    dfi = create_df(i)\n",
    "    dfi = dfi.select(sorted(dfi.columns))\n",
    "    save_df(dfi, ntpath.basename(i), fhv_folderPath)\n",
    "\n",
    "print(\"FHV schema integration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating FHVHV schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/output/integrated FHVHV\n"
     ]
    }
   ],
   "source": [
    "# Create folder for output files:\n",
    "fhvhv_folderPath = create_folder(\"integrated FHVHV\", save_directory)\n",
    "#%store fhvhv_folderPath\n",
    "with open(v_direc + \"fhvhv_folderPath\",'wb') as fhvhvfolderPath:\n",
    "    pickle.dump(fhvhv_folderPath,fhvhvfolderPath)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened hdfs:///user/epb199/data/fhvhv_tripdata_2019-02.csv\n",
      "opened hdfs:///user/epb199/data/fhvhv_tripdata_2019-03.csv\n",
      "opened hdfs:///user/epb199/data/fhvhv_tripdata_2019-04.csv\n",
      "opened hdfs:///user/epb199/data/fhvhv_tripdata_2019-05.csv\n",
      "opened hdfs:///user/epb199/data/fhvhv_tripdata_2019-06.csv\n",
      "opened hdfs:///user/epb199/data/fhvhv_tripdata_2020-01.csv\n",
      "opened hdfs:///user/epb199/data/fhvhv_tripdata_2020-03.csv\n",
      "opened hdfs:///user/epb199/data/fhvhv_tripdata_2020-04.csv\n",
      "opened hdfs:///user/epb199/data/fhvhv_tripdata_2020-05.csv\n",
      "opened hdfs:///user/epb199/data/fhvhv_tripdata_2020-06.csv\n",
      "FHVHV schema integration complete\n"
     ]
    }
   ],
   "source": [
    "# Want to match all files to the sorted version of the final schema\n",
    "# With identified schema type, rename columns as necessary then drop columns as necessary, sort all columns to ensure correct ordering\n",
    "\n",
    "# FHVHV schema never changed - sort and cast to lowercase and strip to be sure\n",
    "for i in fhvhv_sameSchema[0]:\n",
    "    if cluster == True:\n",
    "        i = data_directory + ntpath.basename(i)\n",
    "    dfi = create_df(i)\n",
    "    dfi = dfi.select(sorted(dfi.columns))\n",
    "    save_df(dfi, ntpath.basename(i), fhvhv_folderPath)\n",
    "\n",
    "print(\"FHVHV schema integration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Green schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/output/integrated Green\n"
     ]
    }
   ],
   "source": [
    "# Create folder for output files:\n",
    "green_folderPath = create_folder(\"integrated Green\", save_directory)\n",
    "#%store green_folderPath\n",
    "with open(v_direc + \"green_folderPath\",'wb') as greenfolderPath:\n",
    "    pickle.dump(green_folderPath,greenfolderPath)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened hdfs:///user/epb199/data/green_tripdata_2013-08.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2013-09.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2013-10.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2013-11.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2013-12.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2014-01.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2014-02.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2014-03.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2014-04.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2014-05.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2014-06.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2014-07.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2014-08.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2014-09.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2014-10.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2014-11.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2014-12.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2015-01.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2015-02.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2015-03.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2015-04.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2015-05.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2015-06.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2015-07.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2015-08.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2015-09.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2015-10.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2015-11.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2015-12.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2016-01.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2016-02.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2016-03.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2016-04.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2016-05.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2016-06.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2016-07.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2016-08.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2016-09.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2016-10.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2016-11.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2016-12.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2017-01.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2017-02.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2017-03.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2017-04.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2017-05.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2017-06.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2017-07.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2017-08.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2017-09.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2017-10.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2017-11.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2017-12.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2018-01.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2018-02.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2018-03.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2018-04.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2018-05.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2018-06.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2018-07.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2018-08.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2018-09.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2018-10.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2018-11.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2018-12.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2019-01.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2019-02.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2019-03.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2019-04.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2019-05.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2019-06.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2020-01.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2020-02.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2020-04.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2020-05.csv\n",
      "opened hdfs:///user/epb199/data/green_tripdata_2020-06.csv\n",
      "Green schema integration complete\n"
     ]
    }
   ],
   "source": [
    "# Want to match all files to the sorted version of the final schema\n",
    "# With identified schema type, rename columns as necessary then drop columns as necessary, sort all columns to ensure correct ordering\n",
    "\n",
    "#green_sameSchema[0]\n",
    "    #[0] requires:\n",
    "        #conversion of location columns\n",
    "        #addition of columns congestion_surcharge, improvement_surcharge\n",
    "        #sorting\n",
    "for i in green_sameSchema[0]:\n",
    "    if cluster == True:\n",
    "        i = data_directory + ntpath.basename(i)\n",
    "    dfi = create_df(i)\n",
    "    dfi = add_locationID(dfi)\n",
    "    dfi = dfi.drop(*[\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"])\n",
    "    dfi = add_empty_cols(dfi,[\"improvement_surcharge\", \"congestion_surcharge\"])\n",
    "    dfi = dfi.select(sorted(dfi.columns))\n",
    "    save_df(dfi, ntpath.basename(i), green_folderPath)\n",
    "\n",
    "#green_sameSchema[1]\n",
    "    #[1] requires:\n",
    "        #conversion of location columns\n",
    "        #addition of column congestion_surcharge\n",
    "        #sorting\n",
    "for i in green_sameSchema[1]:\n",
    "    if cluster == True:\n",
    "        i = data_directory + ntpath.basename(i)\n",
    "    dfi = create_df(i)\n",
    "    dfi = add_locationID(dfi)\n",
    "    dfi = dfi.drop(*[\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"])\n",
    "    dfi = dfi.withColumn(\"congestion_surcharge\",lit(None).cast(StringType()))\n",
    "    dfi = dfi.select(sorted(dfi.columns))\n",
    "    save_df(dfi, ntpath.basename(i), green_folderPath)\n",
    "\n",
    "#green_sameSchema[2]\n",
    "    #[2] requires:\n",
    "        #addition of column congestion_surcharge\n",
    "        #sorting\n",
    "for i in green_sameSchema[2]:\n",
    "    if cluster == True:\n",
    "        i = data_directory + ntpath.basename(i)\n",
    "    dfi = create_df(i)\n",
    "    dfi = dfi.withColumn(\"congestion_surcharge\",lit(None).cast(StringType()))\n",
    "    dfi = dfi.select(sorted(dfi.columns))\n",
    "    save_df(dfi, ntpath.basename(i), green_folderPath)\n",
    "\n",
    "#green_sameSchema[3] is the final schema\n",
    "    #[3] requires:\n",
    "        #sorting\n",
    "for i in green_sameSchema[3]:\n",
    "    if cluster == True:\n",
    "        i = data_directory + ntpath.basename(i)\n",
    "    dfi = create_df(i)\n",
    "    dfi = dfi.select(sorted(dfi.columns))\n",
    "    save_df(dfi, ntpath.basename(i), green_folderPath)\n",
    "    \n",
    "print(\"Green schema integration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Yellow schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists at /home/epb123/output/integrated Yellow\n"
     ]
    }
   ],
   "source": [
    "# Create folder for output files:\n",
    "yellow_folderPath = create_folder(\"integrated Yellow\", save_directory)\n",
    "#%store yellow_folderPath\n",
    "with open(v_direc + \"yellow_folderPath\",'wb') as yellowfolderPath:\n",
    "    pickle.dump(yellow_folderPath,yellowfolderPath)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened hdfs:///user/epb199/data/yellow_tripdata_2009-01.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2009-02.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2009-03.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2009-04.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2009-05.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2009-06.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2009-07.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2009-08.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2009-09.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2009-10.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2009-11.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2009-12.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2010-01.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2010-02.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2010-03.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2010-04.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2010-05.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2010-06.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2010-07.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2010-08.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2010-09.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2010-10.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2010-11.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2010-12.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2011-01.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2011-02.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2011-03.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2011-04.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2011-05.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2011-06.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2011-07.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2011-08.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2011-09.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2011-10.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2011-11.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2011-12.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2012-01.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2012-02.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2012-03.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2012-04.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2012-05.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2012-06.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2012-07.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2012-08.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2012-09.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2012-10.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2012-11.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2012-12.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2013-01.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2013-02.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2013-03.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2013-04.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2013-05.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2013-06.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2013-07.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2013-08.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2013-09.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2013-10.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2013-11.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2013-12.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2014-01.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2014-02.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2014-03.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2014-04.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2014-05.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2014-06.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2014-07.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2014-08.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2014-09.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2014-10.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2014-11.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2014-12.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2015-01.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2015-02.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2015-03.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2015-04.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2015-05.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2015-06.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2015-07.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2015-08.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2015-09.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2015-10.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2015-11.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2015-12.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2016-01.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2016-02.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2016-03.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2016-04.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2016-05.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2016-06.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2016-07.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2016-08.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2016-09.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2016-10.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2016-11.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2016-12.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2017-01.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2017-02.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2017-03.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2017-04.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2017-05.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2017-06.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2017-07.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2017-08.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2017-09.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2017-10.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2017-11.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2017-12.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2018-01.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2018-02.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2018-03.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2018-04.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2018-05.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2018-06.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2018-07.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2018-08.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2018-09.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2018-10.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2018-11.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2018-12.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2019-01.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2019-02.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2019-03.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2019-04.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2019-05.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2019-06.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2020-01.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2020-02.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2020-04.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2020-05.csv\n",
      "opened hdfs:///user/epb199/data/yellow_tripdata_2020-06.csv\n",
      "Yellow schema integration complete\n"
     ]
    }
   ],
   "source": [
    "# Want to match all files to the sorted version of the final schema\n",
    "# With identified schema type, rename columns as necessary then drop columns as necessary, sort all columns to ensure correct ordering\n",
    "\n",
    "#yellow_sameSchema[0]\n",
    "    #[0] requires:\n",
    "        #renaming start_lon to pickup_longitude\n",
    "        #renaming total_amt to total_amount\n",
    "        #renaming tip_amt to tip_amount\n",
    "        #renaming end_lon to dropoff_longitude\n",
    "        #renaming tolls_amt to tolls_amount\n",
    "        #renaming fare_amt to fare_amount\n",
    "        #renaming vendor_name to vendorid\n",
    "        #renaming trip_pickup_datetime to tpep_pickup_datetime\n",
    "        #renaming trip_dropoff_datetime to tpep_dropoff_datetime\n",
    "        #renaming store_and_forward to store_and_fwd_flag\n",
    "        #renaming end_lat to dropoff_latitude\n",
    "        #renaming start_lat to pickup_latitude\n",
    "        #renaming rate_code to ratecodeid\n",
    "        #renaming surcharge to extra (the values appear to match, based on 2.3 analysis)\n",
    "        #conversion of location columns\n",
    "        #addition of column congestion_surcharge\n",
    "        #addition of column improvement_surcharge\n",
    "        #sorting\n",
    "for i in yellow_sameSchema[0]:\n",
    "    if cluster == True:\n",
    "        i = data_directory + ntpath.basename(i)\n",
    "    dfi = create_df(i)\n",
    "    dfi = replace_colNames(dfi,\\\n",
    "                            ['surcharge', 'rate_code', 'start_lat', 'end_lat', 'store_and_forward', 'start_lon', 'total_amt', 'tip_amt', 'end_lon', 'tolls_amt', 'fare_amt', 'vendor_name', 'trip_pickup_datetime', 'trip_dropoff_datetime'],\\\n",
    "                            ['extra', 'ratecodeid', 'pickup_latitude', 'dropoff_latitude', 'store_and_fwd_flag', 'pickup_longitude', 'total_amount', 'tip_amount', 'dropoff_longitude', 'tolls_amount', 'fare_amount', 'vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "    dfi = add_locationID(dfi)   \n",
    "    dfi = dfi.drop(*[\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"])\n",
    "    dfi = add_empty_cols(dfi,[\"improvement_surcharge\", \"congestion_surcharge\"])\n",
    "    dfi = dfi.select(sorted(dfi.columns))\n",
    "    save_df(dfi, ntpath.basename(i), yellow_folderPath)\n",
    "    \n",
    "#yellow_sameSchema[1]\n",
    "    #[1] requires:\n",
    "        #renaming vendor_id to vendorid\n",
    "        #renaming pickup_datetime to tpep_pickup_datetime\n",
    "        #renaming dropoff_datetime to tpep_dropoff_datetime\n",
    "        #renaming rate_code to ratecodeid\n",
    "        #renaming surcharge to extra\n",
    "        #conversion of location columns\n",
    "        #addition of column congestion_surcharge\n",
    "        #addition of column improvement_surcharge\n",
    "        #sorting\n",
    "for i in yellow_sameSchema[1]:\n",
    "    if cluster == True:\n",
    "        i = data_directory + ntpath.basename(i)\n",
    "    dfi = create_df(i)\n",
    "    dfi = replace_colNames(dfi, ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'rate_code', 'surcharge'],\\\n",
    "                           ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'ratecodeid', 'extra'])\n",
    "    dfi = add_locationID(dfi)   \n",
    "    dfi = dfi.drop(*[\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"])\n",
    "    dfi = add_empty_cols(dfi,[\"improvement_surcharge\", \"congestion_surcharge\"])\n",
    "    dfi = dfi.select(sorted(dfi.columns))\n",
    "    save_df(dfi, ntpath.basename(i), yellow_folderPath)\n",
    "\n",
    "#yellow_sameSchema[2]\n",
    "    #[2] requires:\n",
    "        #conversion of location columns\n",
    "        #addition of column congestion_surcharge\n",
    "        #sorting\n",
    "for i in yellow_sameSchema[2]:\n",
    "    if cluster == True:\n",
    "        i = data_directory + ntpath.basename(i)\n",
    "    dfi = create_df(i)\n",
    "    dfi = add_locationID(dfi)\n",
    "    dfi = dfi.drop(*[\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"])\n",
    "    dfi = dfi.withColumn(\"congestion_surcharge\",lit(None).cast(StringType()))\n",
    "    dfi = dfi.select(sorted(dfi.columns))\n",
    "    save_df(dfi, ntpath.basename(i), yellow_folderPath)\n",
    "\n",
    "#yellow_sameSchema[3]\n",
    "    #[3] requires:\n",
    "        #addition of column congestion_surcharge\n",
    "        #sorting\n",
    "for i in yellow_sameSchema[3]:\n",
    "    if cluster == True:\n",
    "        i = data_directory + ntpath.basename(i)\n",
    "    dfi = create_df(i)\n",
    "    dfi = dfi.withColumn(\"congestion_surcharge\",lit(None).cast(StringType()))\n",
    "    dfi = dfi.select(sorted(dfi.columns))\n",
    "    save_df(dfi, ntpath.basename(i), yellow_folderPath)\n",
    "\n",
    "#yellow_sameSchema[4]\n",
    "    #[4] requires:\n",
    "        #sorting\n",
    "for i in yellow_sameSchema[4]:\n",
    "    if cluster == True:\n",
    "        i = data_directory + ntpath.basename(i)\n",
    "    dfi = create_df(i)\n",
    "    dfi = dfi.select(sorted(dfi.columns))\n",
    "    save_df(dfi, ntpath.basename(i), yellow_folderPath)\n",
    "    \n",
    "print(\"Yellow schema integration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "As mentioned previously, the PySpark save option results in a bit of a mess in the file system. The result of the previous integration code is a new folder containing one folder per each integrated file, where the file name is almost gibberish and where there is an additional text file stating \"success\".  \n",
    "\n",
    "This code cleans this up - it renames each file properly and moves it to a common folder.\n",
    "\n",
    "When running on the cluster and opening/saving with pandas methods, this is not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(folderpath):\n",
    "# Function which takes the folderpath for integrated data\n",
    "# Function moves all CSV's to one folder and renames them properly\n",
    "# Function returns the filepath of the new csvs\n",
    "# DO NOT CLEAN MORE THAN ONCE\n",
    "\n",
    "    newPath = create_folder(\"all CSV\", folderpath)\n",
    "    testFilePath = []\n",
    "\n",
    "    for (root,direc,files) in os.walk(folderpath):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                if os.path.basename(root) != os.path.basename(newPath):\n",
    "                    os.rename(os.path.join(root,file),os.path.join(newPath,os.path.basename(root)))\n",
    "                    testFilePath.append(os.path.join(newPath,os.path.basename(root)))\n",
    "\n",
    "    testFilePath.sort()\n",
    "    return testFilePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths cleaned and stored\n"
     ]
    }
   ],
   "source": [
    "# fhv_integratedPaths = cleanup(fhv_folderPath)\n",
    "# fhvhv_integratedPaths = cleanup(fhvhv_folderPath)\n",
    "# green_integratedPaths = cleanup(green_folderPath)\n",
    "# yellow_integratedPaths = cleanup(yellow_folderPath)\n",
    "\n",
    "# elif cluster == True:\n",
    "fhv_integratedPaths = []\n",
    "for i in os.listdir(fhv_folderPath):\n",
    "    if i != \".ipynb_checkpoints\":\n",
    "        fhv_integratedPaths.append(os.path.join(fhv_folderPath,i))\n",
    "fhv_integratedPaths.sort()\n",
    "\n",
    "fhvhv_integratedPaths = []\n",
    "for i in os.listdir(fhvhv_folderPath):\n",
    "    if i != \".ipynb_checkpoints\":\n",
    "        fhvhv_integratedPaths.append(os.path.join(fhvhv_folderPath,i))\n",
    "fhvhv_integratedPaths.sort()\n",
    "\n",
    "green_integratedPaths = []\n",
    "for i in os.listdir(green_folderPath):\n",
    "    if i != \".ipynb_checkpoints\":\n",
    "        green_integratedPaths.append(os.path.join(green_folderPath,i))\n",
    "green_integratedPaths.sort()\n",
    "\n",
    "yellow_integratedPaths = []\n",
    "for i in os.listdir(yellow_folderPath):\n",
    "    if i != \".ipynb_checkpoints\":\n",
    "        yellow_integratedPaths.append(os.path.join(yellow_folderPath,i))\n",
    "yellow_integratedPaths.sort()\n",
    "\n",
    "\n",
    "# fhv_integratedPaths = sorted([os.path.join(fhv_folderPath, fhvfile) for fhvfile in os.listdir(fhv_folderPath)])\n",
    "# fhvhv_integratedPaths = sorted([os.path.join(fhvhv_folderPath, fhvhvfile) for fhvhvfile in os.listdir(fhvhv_folderPath)])\n",
    "# green_integratedPaths = sorted([os.path.join(green_folderPath, greenfile) for greenfile in os.listdir(green_folderPath)])\n",
    "# yellow_integratedPaths = sorted([os.path.join(yellow_folderPath, yellowfile) for yellowfile in os.listdir(yellow_folderPath)])\n",
    "\n",
    "#%store fhv_integratedPaths\n",
    "with open(v_direc + \"fhv_integratedPaths\",'wb') as fhvintegratedPaths:\n",
    "    pickle.dump(fhv_integratedPaths,fhvintegratedPaths)   \n",
    "#%store fhvhv_integratedPaths\n",
    "with open(v_direc + \"fhvhv_integratedPaths\",'wb') as fhvhvintegratedPaths:\n",
    "    pickle.dump(fhvhv_integratedPaths,fhvhvintegratedPaths)  \n",
    "#%store green_integratedPaths\n",
    "with open(v_direc + \"green_integratedPaths\",'wb') as greenintegratedPaths:\n",
    "    pickle.dump(green_integratedPaths,greenintegratedPaths)  \n",
    "#%store yellow_integratedPaths\n",
    "with open(v_direc + \"yellow_integratedPaths\",'wb') as yellowintegratedPaths:\n",
    "    pickle.dump(yellow_integratedPaths,yellowintegratedPaths)\n",
    "\n",
    "print(\"Paths cleaned and stored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Testing code below verifies that the algorithm above has in fact resulted in new files which have identical schemas. It is just a modified version of the 2.1 code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_analysis(sub_data):\n",
    "# Function to analyse changes in data column labels, taking filepath list as inputs\n",
    "# Function returns a tuple containing:\n",
    "# Return 1. dataframe containing columns added to files when comparing vs. previous year, and columns dropped from files when comparing vs. subsequent year\n",
    "# Return 2. list of lists, where each sub-list contains the dates as a tuple (year, month) which have a common schema\n",
    "# Return 3. dataframe of variables where the index changed from one year to another\n",
    "    # Create variables to capture changes - column changes as a set and a list of dates with the same schema\n",
    "    col_changes = {}\n",
    "    same_schema = []\n",
    "    order_changesDf = pd.DataFrame(columns = ['file 0', 'file 1', 'index 0', 'index 1'])\n",
    "    \n",
    "    # Variable to store dates with the current schema\n",
    "    current_schema = []\n",
    "\n",
    "    # Checking if column names are the same - iterates over all files in sub-dataset\n",
    "    for j in range (len(sub_data)):        \n",
    "        # Storing first row of the csv and ensuring all headers are lowercase without leading and trailing white spaces\n",
    "        df_j= pd.read_csv(sub_data[j],nrows=0)\n",
    "        l1 = [item.lower().strip() for item in list(df_j.columns)]\n",
    "        \n",
    "        jdate = get_pathdate(sub_data[j])\n",
    "        \n",
    "        if j == 0:\n",
    "        # Storing first csv's columns as l0\n",
    "            l0 = [item.lower().strip() for item in list(df_j.columns)]\n",
    "            #REPLACE WITH l1[:]???\n",
    "            current_schema.append(sub_data[j])  \n",
    "        \n",
    "        elif set(l1) != set(l0):\n",
    "        # Comparing the column names in set format, as order should not matter here\n",
    "            l0_name = ntpath.basename(sub_data[j-1])\n",
    "            l1_name = ntpath.basename(sub_data[j])\n",
    "            \n",
    "            # Elements in l0 but not in l1 => dropped elements\n",
    "            col_changes['dropped from '+l0_name] = set(l0) - set(l1)\n",
    "            # Elements in l1 and not in l0 => added elements\n",
    "            col_changes['added to '+l1_name] = set(l1) - set(l0)\n",
    "            # Reset schema tracker\n",
    "            same_schema.append(current_schema[:])\n",
    "            current_schema.clear()\n",
    "            current_schema.append(sub_data[j])                      \n",
    "            \n",
    "            # Tracking column changes\n",
    "            for k in range(len(l0)):\n",
    "                try:\n",
    "                    if l0[k] != l1[k]:\n",
    "                        for j in range(len(l1)):\n",
    "                            if l0[k] == l1[j]:\n",
    "                                order_changesDf.loc[l0[k]+', y:'+str(jdate[0])+\", m:\"+str(jdate[1])] = [l0_name, l1_name, k, j]\n",
    "                except IndexError:\n",
    "                    for j in range(len(l1)):\n",
    "                        if l0[k] == l1[j]:\n",
    "                            order_changesDf.loc[l0[k]+', y:'+str(jdate[0])+\", m:\"+str(jdate[1])] = [l0_name, l1_name, k, j]\n",
    "                    \n",
    "            # Reset l0 for next loop\n",
    "            l0 = l1\n",
    "        \n",
    "        elif set(l1) == set(l0):\n",
    "            current_schema.append(sub_data[j])\n",
    "            # Append the current_schema if we have reached the last record\n",
    "            if j == len(sub_data) - 1:\n",
    "                same_schema.append(current_schema[:])\n",
    "\n",
    "            # Tracking column changes\n",
    "            if l0 != l1:\n",
    "                for k in range(len(l0)):\n",
    "                    try:\n",
    "                        if l0[k] != l1[k]:\n",
    "                            for j in range(len(l1)):\n",
    "                                if l0[k] == l1[j]:\n",
    "                                    order_changesDf.loc[l0[k]+', y:'+str(jdate[0])+\", m:\"+str(jdate[1])] = [l0_name, l1_name, k, j]\n",
    "                    except IndexError:\n",
    "                        for j in range(len(l1)):\n",
    "                            if l0[k] == l1[j]:\n",
    "                                order_changesDf.loc[l0[k]+', y:'+str(jdate[0])+\", m:\"+str(jdate[1])] = [l0_name, l1_name, k, j]\n",
    "            \n",
    "            # Reset l0 for next loop\n",
    "            l0 = l1\n",
    "        \n",
    "        else:\n",
    "            print(\"error reading columns\")\n",
    "                \n",
    "    return (pd.DataFrame.from_dict(col_changes, orient='index'), same_schema, order_changesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pathdate(givenpath):\n",
    "# Function to get the date from a given filepath\n",
    "# Returns the date as a tuple in (year, month) form\n",
    "        return (int(givenpath[-11:-7]), int(givenpath[-6:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fhv_test = column_analysis(fhv_integratedPaths)\n",
    "fhv_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    fhv_test[1][1]\n",
    "    print(\"FAILURE\")\n",
    "except:\n",
    "    print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file 0</th>\n",
       "      <th>file 1</th>\n",
       "      <th>index 0</th>\n",
       "      <th>index 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [file 0, file 1, index 0, index 1]\n",
       "Index: []"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fhv_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fhvhv_test = column_analysis(fhvhv_integratedPaths)\n",
    "fhvhv_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    fhvhv_test[1][1]\n",
    "    print(\"FAILURE\")\n",
    "except:\n",
    "    print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file 0</th>\n",
       "      <th>file 1</th>\n",
       "      <th>index 0</th>\n",
       "      <th>index 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [file 0, file 1, index 0, index 1]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fhvhv_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_test = column_analysis(green_integratedPaths)\n",
    "green_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    green_test[1][1]\n",
    "    print(\"FAILURE\")\n",
    "except:\n",
    "    print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file 0</th>\n",
       "      <th>file 1</th>\n",
       "      <th>index 0</th>\n",
       "      <th>index 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [file 0, file 1, index 0, index 1]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_test = column_analysis(yellow_integratedPaths)\n",
    "yellow_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    yellow_test[1][1]\n",
    "    print(\"FAILURE\")\n",
    "except:\n",
    "    print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file 0</th>\n",
       "      <th>file 1</th>\n",
       "      <th>index 0</th>\n",
       "      <th>index 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [file 0, file 1, index 0, index 1]\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_test[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
